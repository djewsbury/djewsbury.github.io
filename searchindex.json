{"categories":[{"title":"blog","uri":"https://djewsbury.github.io/categories/blog/"},{"title":"Misc","uri":"https://djewsbury.github.io/categories/misc/"},{"title":"XLE","uri":"https://djewsbury.github.io/categories/xle/"}],"posts":[{"content":"Dealing with floating point precision Last time I described the straight skeleton algorithm, and write about how precision error can impact calculation. Here, I\u0026rsquo;ll break down the methods and heuristics I\u0026rsquo;m using to try to calculate the skeleton for complex shapes.\nCalculation windows  Instead of process edge event completely independently, let\u0026rsquo;s find a \u0026ldquo;window\u0026rdquo; of near simultaneous events to calculate each step. I found that without this, we could often get \u0026ldquo;missed motorcycles\u0026rdquo; \u0026ndash; essentially a motorcycle crash should have happened, but never did, and the vertex proceeds along past it\u0026rsquo;s crash point. A missed motorcycle like this causes the edges of the wavefront to cross each other. This results in a shape that is unsolvable, and typically the entirely skeleton is ruined.\nNear simultaneous events Imagine that we have two events, A and B, which occur almost at the same calculated time, but B is some tiny amount of time later. A and B have both use some of the same vertices and edges in some way. The naive solution calculates A first, and then scans the resultant shape for the next event that should happen. Commonly, we\u0026rsquo;ll still find event B and proceed, and everything is fine. However, if the time difference between the events is small enough, the precision error related to processing A can be enough so that when we calculate B the second time, we now find that it happens slightly earlier than A (rather than after it).\nNow we have a bit of a dilemma \u0026ndash; do we assume that events that happen minutely before the last processed event are subject to precision error and still need to be processed? If so how big should that window big. How do we handle cases such as the 2 new vertices generated by a motorcycle crash event \u0026ndash; these are generated on the same point and moving away from each other, so can be calculated as crashing into each other minutely in the past? (those vertices are generated on separate wavefronts, but as we\u0026rsquo;ll see that doesn\u0026rsquo;t solve this issue entirely).\nThis is one way in which the calculation window helps, as described a little below.\nChains of collapses In the hex grid case, we often get multiple adjacent edges collapsing at the same time and point. Let\u0026rsquo;s imagine two collapse events, A and B, like this. If we calculate collapse A, we end up generate a vertex at the collapse point. That vertex will happen to be exactly where B is collapsing to (since both edges collapse at the same point). This means we\u0026rsquo;ve generated an infinitesimally short edge. If the length of an edge is less than or near the error of it\u0026rsquo;s vertices, that means we can\u0026rsquo;t accurately calculate the edge normal, which also means we can\u0026rsquo;t calculate the paths for the vertices nor the collapse point of the edge (meaning we might miss a collapse).\nIn the hex grid case, this gets even worse when we have a motorcycle crash happening at the same point and time as a the collapses. Here the motorcycle crash can be calculated to happen before the collapses, between the collapses or after the collapses; each of which can cause different problems.\nMaintaining the calculation window Every iteration we generate a list of the next few events that happen on a wavefront. We\u0026rsquo;ll accept all of the events, up until the gap between successive events exceeds some threshold (which will be very tiny). Now:\n we will process each and every event on this list, regardless of how one event impacts others we will process them mostly in order (but with some small reordering covered below) while processing the window, new vertices generated are considered to be frozen in place (ie, we don\u0026rsquo;t even bother generating vertex velocities until all events are processed)  Frequently, events that occur within this window are going to involve the same vertices and edges. So, when one event is processed that removed edges or vertices, we must adjust the other events in the window to take into account these cases. The simplest example of this is when a motorcycle crash is occurring on an edge that is simultaneously collapsing. If we process the collapse first, we must update the motorcycle crash event to now be crashing with the newly created collapse vertex (see the special case for motorcycle-vs-vertex below).\nThis gets a little more complex with the full range of events that can occur; but this is essential to ensure that every event within the window is actually calculated.\nProcessing collapse chains As mentioned before, when adjacent edges are collapsing simultaneously we should process them together and generate a single collapse vertex from the result. We still have a separate event in the calculation window for each collapse, but when processing a collapse we will look for collapses on adjacent edges that occur at any point in the event window.\nThis is the only time we actually process events out-of-order. In the case where a motorcycle crash involving the same edges is calculated between the collapse events, this would splitting the collapse processing and end up generating a lot of overlapping points. Instead, once we start processing edge collapses, we just make sure we process all adjacent events that occur within the calculation window. Each chain then generates just a single new collapse vertex, rather than one new vertex per collapsed edge.\nSpecial case for motorcycle-vs-vertex Naively, motorcycle crashes are a vertex vs edge operation. However, as mentioned earlier, when motorcycle intersects another vertex very closely, there can be ambiguity about which side of the vertex it is really on, and therefore which edge to use. Furthermore, this risks generated infinitesimal short edges. So we need to introduce a special case for motorcycle-vs-vertex, wherein we imagine the vertices as small circles (that is, with the radius begin their error). This adds some complexity the motorcycle crash processing, but fortunately it doesn\u0026rsquo;t impact much else.\nGenerate velocity from collapse events The calculations for vertex velocity, edge collapse points and motorcycle crash events are all fairly similar \u0026ndash; they all basically boil down to edges moving along their tangents in different ways. However they are subject to precision error in different ways, meaning that we can end up with results that don\u0026rsquo;t agree with each other.\nThis can be an issue with vertices attached to very small edges, or edges with very acute angles between them. In these cases our accuracy is lowest and the likelihood of disagreements is highest.\nIn an ideal world, the collapse points for an edge should be exactly on the path traced by both it\u0026rsquo;s vertices. So, in order to minimize disagreement, we can use the edge collapse point and time directly as a way to calculate the vertex velocity. This raises a problem, though, in that all vertices are connected to two edges \u0026ndash; so which collapse do we use? They should actually both be along the same vertex path, but one might have better precision than the other. I found that just using the soonest event was fine.\nIt also helps to use this calculated velocity in the calculation of motorcycle crashes. It is possible to calculate the motorcycle crash point directly from vertex points themselves (without using this secondarily derived velocity property). However, in order to ensure that collision point is as close to our calculated vertex path as possible, I found it was more reliable to look for crashes along the calculated vertex velocity. This also inherits the special cases for collinearity mentioned below.\nSpecial cases for collinear edges As mentioned earlier, the vertex velocity calculations do not hold up under the presence of collinear edges. Furthermore, we end up with very large precision error in near-collinear cases. We can do our best to try to avoid this type of geometry entering the calculations, but we still need some special cases to avoid extreme cases.\nThere are 2 problems to be wary of:\n edges that make a straight or nearly-straight line, with the vertex somewhere in the middle edges meeting in an extremely acute angle, like the vertex of a V squished horizontally  In case (1), there are infinite valid velocities. We need a special case to ensure we get the velocity that is the most intuitive (ie, just moving with the normal of the combined straight edge). In case (2) we approach an asymptote where the velocity is infinite.\nTo determine \u0026ldquo;near collinearity\u0026rdquo; we need some kind of epsilon/threshold value. This and the calculating window size are probably the main places where we use epsilon values. I found the best approach was to filter out case (1) before running the standard velocity calculation math, but to still calculate case (2) with the standard math. This will allow some very high velocities to enter the calculation occasionally. The calculation window actually helps resolve the worst cases of these, though \u0026ndash; recall the edges with a collapse event at one end and a motorcycle crash event at the other mentioned in the previous page? These cases can result in extreme acute angles; but since both events will fall within the same calculation window, and since we don\u0026rsquo;t update velocities during a calculation window, we can side step the issue.\nCollapses leading to collinearity As mentioned above, sometimes non-adjacent collinear edges can become adjacent as a result of edge collapses between them. Adjacent collinear edges created in this fashion are at least as problematic as if they were part of the input data; so we have to do something about them.\nConsider the hex grid skeleton from last time:\n Here if we look at a part in detail, we can see an example of this\n One option here is to just place the vertex at the collapse location as usual, and calculate it\u0026rsquo;s velocity as the normal of the straight edge that it is on. This adds some extra complexity, however, because as soon as we introduce the possibility of collinear adjacent edges, we need to handle that case in almost all of the calculations. That includes new special cases in both the edge collapse and motorcycle crash detection code.\nAnother option is to just not include that collapse vertex at all. The collapse vertex would follow the path in the dotted line shown, but \u0026ndash; as will become clearer soon \u0026ndash; this vertex doesn\u0026rsquo;t actually have any practical impact on the output result. In fact the output has all of the same properties if it\u0026rsquo;s there or if it\u0026rsquo;s not there.\nSo I elected to simply remove it from the wavefront, which means joining the 2 edges to either side of the collapse into a single edge. We use the same near collinearity detection as mentioned above. This also has a positive side-effect in that very short edges will be detected as collinear to many other edges, which can sometimes mean collapsing down problematic geometry before it causes any real problems.\nThat sounds great; but as it always seems to be with this algorithm, even this scenario introduces it\u0026rsquo;s own special cases. You may notice that removing that dotted line in the above diagram means that two of the grey input edges become connected by one shape in the diagram. This is actually significant, and this is the only case in which is can happen. So we can to record this so we can use it later.\nAvoid changing vertices after they are constructed Whenever we create a new vertex and connect it up via edges, we will need to calculate its velocity and resultant path. The velocity related to the collapse points of the connected edges, which is something we\u0026rsquo;ll need to calculate also. In the edge collapse case, this should be relatively trivial, because a collapse should not change the normal or collapse point of any other edge. But consider the motorcycle crash case, where we introduce edges that are entirely new, and so their collapse points and attached vertex velocities need to be calculated from scratch.\nSo, for each of the new edges, we\u0026rsquo;ll calculate its normal and use this to determine all of the properties we need to calculate. To get this normal, we need the position of the vertex at the same point in time. Since we don\u0026rsquo;t know the velocity of any newly created vertices just yet, we\u0026rsquo;ll set that point in time to be the time of the crash event itself. Then we can calculate the collapse points and from those collapse points we\u0026rsquo;ll find the velocity of the new vertices.\nThis introduces an interesting loop \u0026ndash; we use vertex velocities to calculate edge collapses, and edge collapses to calculate vertex velocities. This runs into the compounding error problems mentioned previously. The newly generated vertices are very likely to be involved with future events, which will mean that their velocities will be required for generating tangents for future edges.\nIt may seem tempting to advance all vertices on the wavefront forward to the time of the event and then find the new positions and recalculate future collapses and motorcycle crashes based on this. In theory, this is valid because the loop created at every subsequent step should still be a valid input loop to the algorithm. However, this will most likely to compound precision error over time and might result in a final shape that is a bit distorted.\nInstead I never recalculate the positions or velocities of vertices after they are first calculated. This means that even if a vertex is involved in many events, so long as it is never removed it will still have the same properties as when it was created (even if it was a vertex from the original input loop).\nAlso note that since we can calculate many events within a calculation window, and don\u0026rsquo;t update the velocities until the end of the window, we can create situations where both vertices of an edge were generated in the same window by different events. This means the vertices will have different initial times, but we won\u0026rsquo;t have a velocity to align them in time. In these cases we treat the vertices as if they were created at exactly the same time and assume that the different in calculated normal would be within the error threshold anyway.\nCalculating epsilons In some cases (such as the near collinearity checks) we need an epsilon value to represent the quantity of error we think is in the calculated results. This determines when we can be confident about a calculation and when it falls into the realm of uncertainty. This is mostly used for the collinearity checks and for calculation window. Given the way that I\u0026rsquo;m using \u0026ldquo;time\u0026rdquo; here, time and space have the same units, so the epsilon values in these cases also have the same units.\nI\u0026rsquo;ve ended up just using a fixed value for this. That may not be perfectly ideal. In theory is should at least scale based on the size of the input shape, I guess. But this may be a case where the simple solution is actually the best.\nIt may be possible to do a little better \u0026ndash; for example, we could calculate the amount of error that the underlying calculations (eg, collapse point determination) can deal with before they generate poor results. Or alternatively we could assign an epsilon value per vertex, and calculate this based on the amount of precision we get from the calculations used to generate that vertex. That might help with the compounding error problem; but it could also make things more complicated.\nSummary It took some time to figure out all of these little details and heuristics, I had to try out quite a few different possible approaches to some of the problems before settling these solutions; but these are what have gotten the best results for me. All of the difficult scenarios described in part one can be handled.\nOften I found that what seemed like the simplest solution to precision errors \u0026ndash; such as \u0026ldquo;magnetting\u0026rdquo; together close vertices \u0026ndash; actually ended up causing more problems. So there\u0026rsquo;s often a kind of conflict between ignoring differences within error thresholds (such as collinearity checks, collapse event reordering, motor vs vertex) and trying to maintain as much precision as possible. It is kind of interesting how impactful precision error is on this algorithm in particular. It plays a big part in many computational geometry algorithms; but here we seem to have a combination of calculations converging on the same values while also having a lot of cases (such as a missed motorcycle or missed collapse) that will instantly ruin the output if the precision we need falls under the error thresholds.\nNext time There\u0026rsquo;s still a little to cover \u0026ndash; next time cover calculating skeletons for different types of shapes (such as silhouettes created in art packages) and some applications in 3d.\n","id":0,"section":"posts","summary":"Dealing with floating point precision Last time I described the straight skeleton algorithm, and write about how precision error can impact calculation. Here, I\u0026rsquo;ll break down the methods and heuristics I\u0026rsquo;m using to try to calculate the skeleton for complex shapes.\nCalculation windows  Instead of process edge event completely independently, let\u0026rsquo;s find a \u0026ldquo;window\u0026rdquo; of near simultaneous events to calculate each step. I found that without this, we could often get \u0026ldquo;missed motorcycles\u0026rdquo; \u0026ndash; essentially a motorcycle crash should have happened, but never did, and the vertex proceeds along past it\u0026rsquo;s crash point.","tags":["StraightSkeleton"],"title":"Straight Skeleton and computing it with finite precision numbers (part 2)","uri":"https://djewsbury.github.io/2021/07/straightskeleton1/","year":"2021"},{"content":"Hello! The \u0026ldquo;straight skeleton\u0026rdquo; is an interesting computation geometry concept that\u0026rsquo;s not super commonly known, but has some interesting properties. I\u0026rsquo;ve just added a implementation to XLE; I\u0026rsquo;m not 100% sure if there are other solid implementations in open source (there may be, but last time I searched, I found implementations in commercial math packages, just not any in the open source domain).\nDefinition Imagine you have an arbitrary polygon (convex, concave, or even with internal holes). For each edge, find the direction that is perpendicular to the edge and pointing inwards into the shape (we\u0026rsquo;ll call this the \u0026ldquo;normal\u0026rdquo; of the edge). Now imagine that edge is moving along it\u0026rsquo;s normal, inwards into the polygon. The polygon will appear to get smaller and smaller, sometimes dividing into two. But in the end, it should eventually collapse into just a set of lines.\nThese lines generated from the collapse, and the paths that the vertices trace out as the polygon shrinks, are called the straight skeleton.\nLet\u0026rsquo;s visualize with some common shapes. A rectangle is a simple case:\n We get a moderately more complex case with a L shaped input:\n Notice that the polygon is shrinking according to it\u0026rsquo;s natural shape. We end up with something that represents the structure of the overall shape. Generally small details have little impact on the final skeleton, but the main structure will be well reflected.\nAlso note that we\u0026rsquo;re not scaling the polygon. Each vertex has a straight line that it\u0026rsquo;s moving along, but this line rarely will intersect the origin. The relative length of the edges changes as time increases.\nEach edge is moving along its normal at the same rate. The line that each vertex moves along is called the vertex path, and is defined by the intersection of two edges as they advance along their tangents. Any given input polygon will have one correct straight skeleton solution, so long as there are no intersecting edge segments (exception for one minor case, which we\u0026rsquo;ll get to in a bit).\nI\u0026rsquo;ll use time related terms (like velocity) to describe what\u0026rsquo;s happening here, because this is just the easiest way to visualize what\u0026rsquo;s happening. Imagine that the polygon is shrinking as we\u0026rsquo;re advancing through time.\nCollapses and motorcycle crashes If we take any 2 adjacent edges, we can calculate a \u0026ldquo;velocity\u0026rdquo; for the vertex between them. The velocity is the amount the vertex will move in a unit of time, and is in the direction of the vertex path (the blue lines above). This can be calculated with a little bit of geometric algebra \u0026ndash; remember that the vertex movement is defined by the movement of the edges on either side of it.\nVertices will continue to move according to their velocity indefinitely until an \u0026ldquo;event\u0026rdquo; happens that changes things. There are only 2 main types of event: one is called an \u0026ldquo;edge collapse\u0026rdquo; and another is called a \u0026ldquo;motorcycle crash\u0026rdquo;.\nSo calculating the straight skeleton is actually pretty simple: we just calculate when the next event will be, advance time and process that event. Then find the next event, and repeat until there are no more events. Believe it or not, this is all we have to do; and it works for any given input polygon (though I\u0026rsquo;ll cover an extension for polygons with holes below).\nCollapses There\u0026rsquo;s a typical example of a collapsing edge in the rectangle case:\n Once an edge reaches zero length, it\u0026rsquo;s considered to be \u0026ldquo;collapsed\u0026rdquo;, and it will be replaced with a single vertex. In the rectangle case, once the collapse is processed, we\u0026rsquo;re finished and don\u0026rsquo;t need to advance any further. But this isn\u0026rsquo;t usually the case. Usually the new vertex will continue according to the same rules as any other vertex: it just traces the path between two edges. Nominally the edges connected to that vertex haven\u0026rsquo;t actually changed direction, but that doesn\u0026rsquo;t mean the velocity of the new vertex is the same the velocities of either of the vertices on the collapsed edge. So, all in all, we\u0026rsquo;ll remove 2 vertices while processing this event and add back 1 new vertex.\nMotorcycle crashes I always imagine the authors that came up with this name were Tron fans.. Perhaps it\u0026rsquo;s true. Anyway, it\u0026rsquo;s actually a pretty descriptive name. Consider the following diagram:\n Here, a vertex and an edge are moving towards each other on a collision course. When they \u0026ldquo;crash\u0026rdquo;, this will actually split the entire wavefront loop into two. We create two new vertices, one in each loop, each moving away from each other and away from the crash.\nIf we zoom up in the middle diagram above:  The extremely observant might have noticed another property here: the vertex paths for each vertex don\u0026rsquo;t have the same length. This means the velocities of each vertex don\u0026rsquo;t have the same magnitude. In fact the velocity of the vertex depends on the angle between its edges (more on that later).\nColinear adjacent edges The math here works for any input polygon, with one exception: if there are adjacent collinear edges. The vertex path is defined by the intersection of adjacent edges; however if those edges are collinear, there are an infinite number of intersections and therefore an infinite number of possible paths. We could have to add extra rules to handle this case properly, but as we\u0026rsquo;ll see in a moment that isn\u0026rsquo;t quite as easy as it sounds. In general, collinearity is kryptonite to the algorithm and we should try to avoid it as much as possible.\nTire meeting the road In theory, these two types of events are all we need to handle in order to calculate the skeleton. We just need some code to find the next event to process and the code to handle processing it. However, of course, this isn\u0026rsquo;t really the end of the story.\nI\u0026rsquo;ve found that computational geometry algorithms like this are amongst the most annoying types of algorithms to implement and debug, and it always tends to be for the same reason. That is, the mathematical operations we\u0026rsquo;re using have only a finite level of precision. It really is kind of shocking how quickly we build up enough error to where we don\u0026rsquo;t know if a given point is on the left or right of a line. And in this algorithm, if we get even a single calculation like that wrong, we can get either a missed motorcycle crash or a edge narrowly missing collapse and expanding in the wrong direction (either of which will ruin the output entirely).\nSo, the real challenge with this algorithm is implementing it for non-quantum computers with finite precision. That means coming up with a set of heuristics that can deal with the error. So, I\u0026rsquo;ll share here the best approaches I\u0026rsquo;ve found.\nHex grids Some input shapes are naturally more prone to numeric precision error that others. While shapes with very soft following edges can be a little tricky, the most difficult types of shapes I\u0026rsquo;ve come across are actually hex grids. Here\u0026rsquo;s a diagram to demonstrate some difficult scenarios that pop up when processing hex grids:\n In many of these cases, we have multiple events happening at once, and sometimes multiple events happening at the same point. That should set off warning bells for precision error. Let\u0026rsquo;s consider, for example, the case marked (A) above.\nHere we have 2 edges coming together, and at one end of the edges we have 2 other edges collapsing and at the other end we have to vertices motorcycling into each other. This is all happening at the same time \u0026ndash; so in theory there are actually 4 events happening simultaneously. Those calculations are all subject to error, though, so let\u0026rsquo;s consider what would happen if we used completely naive math:\n based on error, or random case, we\u0026rsquo;ll decide on one specific event to process first. Let\u0026rsquo;s imagine that event is one of the motorcycle crashes. here, the motorcycle could be crashing directly into a vertex, but we interpret that as a motorcycle vs edge crash, and the edge could be either of the two vertices connected to the other vertex when we process the motorcycle crash, we will end up with 2 vertices connected by an edge, but either on the same point, or extremely close together this creates a big problem because the normal for that edge is extremely subject to error. It could be in almost any direction, meaning the velocities for the vertices is also very prone to error that new edge might even be collapsing \u0026ndash; it could be expanding from it\u0026rsquo;s original tiny length to something much larger furthermore, because the edges in the bottom circle of (A) are also collapsing, the new vertex we\u0026rsquo;ve created as a result of the motorcycle crash is at the end of 2 edges that are collinear or almost collinear. If we attempt to calculate the velocity of a vertex that is at the bottom of a V of almost collinear edges, we\u0026rsquo;ll end up with a value that is asymptotically huge in fact, the velocity can be massive enough that even if the edge collapses happen very briefly after the motorcycle crash, when we attempt to calculate the final position for that vertex, we can end up with something that has escaped the loop and running off to infinity  Consider the two edges marked with (B) above\n these edges are colinear, but not adjacent however, the edges between them will collapse (ultimately becoming 2 collapses at the same time, incidentally) and at that point, they will become adjacent, while still being collinear (recall that edges move along their tangents, so edges will remain colinear if they start collinear) this has the same problems as if we have adjacent collinear edges in the initial input, and will generate infinities and incorrect outputs if not handled specially this raises the question of how to determine if 2 edges are collinear \u0026ndash; because this will definitely require some compensation for precision error  Hex grids also have another added problem in that, since square roots are involved in calculating the input vertices, the inputs polygon actually involves transcendental numbers. Since we\u0026rsquo;re calculating those at finite precision, even the input geometry has built in precision error.\nThese kinds of scenarios, and others, come up over and over again using hex grids. And it\u0026rsquo;s easy to randomly generate input hex grids \u0026ndash; so they make the perfect input data for testing this algorithm.\nif you\u0026rsquo;re curious, here how the above shape ends up:\n Compounding error You may have noticed that edges can be involved in multiple events over the course of the calculation. For example, an edge might be adjacent to a collapse at one point, and later be part of a motorcycle crash. The problem with this is it can cause compounding error in successive events.\nRecall that when we process a motorcycle crash, we generate 2 new vertices, each with edges attached. These new vertices, or the attached edges, can be involved in further motorcycle crashes. However, since we calculated the initial position for that vertex while processing the motorcycle crash, it already has that precision error built in. Any calculations we make for the velocity of the vertex or the tangents and movement of the attached edges also have that error baked in.\nAnd, so, when the vertex or edges are involved in a new event, that event is calculated with the error from the first motorcycle crash, plus the error from this new event. This can happen over and over again \u0026ndash; effectively magnifying the impact of precision error for large polygons.\nFurthermore, all of the geometry math here involves the tangents to edges in some way. To calculate this, we need to know the positions of the vertices for that edge. If both vertices on the edge are part of the initial input loop, this isn\u0026rsquo;t an issue \u0026ndash; we just look at their input positions, and since the edges never rotate, whatever normal we calculate from there will always be valid. However, what if one or two of the vertices were generated as a result of an event?\nTo calculate the edge normal in these cases, we need to know the position of both vertices at an equal time. That requires knowing the vertex path of at least one of the vertices and calculating the movement along that path. That also introduces precision error, which will ultimately impact our calculation of the edge normal.\nNext time This is already getting pretty long, so I\u0026rsquo;ll cut it off here for now. In the next update I\u0026rsquo;ll describe the heuristics I\u0026rsquo;m using, and also show some more complex examples (and maybe talk about some interesting applications of this algorithm).\n","id":1,"section":"posts","summary":"Hello! The \u0026ldquo;straight skeleton\u0026rdquo; is an interesting computation geometry concept that\u0026rsquo;s not super commonly known, but has some interesting properties. I\u0026rsquo;ve just added a implementation to XLE; I\u0026rsquo;m not 100% sure if there are other solid implementations in open source (there may be, but last time I searched, I found implementations in commercial math packages, just not any in the open source domain).\nDefinition Imagine you have an arbitrary polygon (convex, concave, or even with internal holes).","tags":["StraightSkeleton"],"title":"Straight Skeleton and computing it with finite precision numbers","uri":"https://djewsbury.github.io/2021/06/straightskeleton0/","year":"2021"},{"content":"Yikes; it\u0026rsquo;s been so long since I updated this that even the site generator I was using seems to have disappeared! I hit a busy phase with work, and wasn\u0026rsquo;t able to fully finish the spherical harmonics stuff. I probably got through most of the math stuff; but I never quite finished some of the applications and tricks you can do with the technique. It\u0026rsquo;s a pity, perhaps, because that would have been the fun stuff :)\nI think that feeling like I had to pick that series up again after an abscence probably kept me away. But, anyway, I had some fun things I wanted to share, so seems like now is as good a time as any.\nI\u0026rsquo;ve had more time to focus on XLE lately \u0026ndash; I\u0026rsquo;ll talk about where that\u0026rsquo;s going and goals with that project, etc, later. But for now, hit the \u0026ldquo;next page\u0026rdquo; button for an interesting implementation I\u0026rsquo;ve been looking at lately.\n","id":2,"section":"posts","summary":"Yikes; it\u0026rsquo;s been so long since I updated this that even the site generator I was using seems to have disappeared! I hit a busy phase with work, and wasn\u0026rsquo;t able to fully finish the spherical harmonics stuff. I probably got through most of the math stuff; but I never quite finished some of the applications and tricks you can do with the technique. It\u0026rsquo;s a pity, perhaps, because that would have been the fun stuff :)","tags":["blog"],"title":"Quick update before a new post","uri":"https://djewsbury.github.io/2021/06/quickupdate/","year":"2021"},{"content":"This is a continuation of the tutorial on spherical harmonic applications for games. On the last page, we figured out the core math for integrating a basic diffuse BRDF with a spherical environment of incident light.\nEfficient implementation So far, we\u0026rsquo;ve been dealing with spherical harmonics fairly abstractly. But now we can get down to a concrete use and some concrete code. Given our input texture (which effectively describes a sphere of incident light on a point) we want to calculate, for a given normal direction, what is the appropriate diffuse response.\nTo explain that more visually, here\u0026rsquo;s the images from the first page. The first one is the input texture, and the second one is the matching diffuse response (with the same mapping).   (images from sIBL Archive: http://www.hdrlabs.com/sibl/archive.html)\nRecall that we\u0026rsquo;ve expressed the BRDF as zonal harmonic coefficients and the incident light environment as spherical harmonic coefficients. We\u0026rsquo;ve established that we need to use a \u0026ldquo;convolve\u0026rdquo; operation to calculate how the BRDF reflects incident light to excident light (this effectively calculates the integral of the BRDF across the incident sphere). We also worked out that we need to rotate the BRDF in order to align it with the direction of the normal.\nTo refresh, here\u0026rsquo;s the equation for convolution and rotation:  The output of the convolve operation is a set of coefficients, and our final result will be the sum of those coefficients.\nSimplifying Let\u0026rsquo;s define g by rearranging the convolve equation:  Recall that our coefficients for the cosine lobe zonal harmonic are:  Now, taking the coefficients for the cosine lobe zonal harmonic, we can calculate g for each of the bands.\nBand 0 (constant):  Band 1:  Band 2:  As luck would have it, y() gets almost completely factored out!\nShader implementation tricks Notice that the g values mostly take the form g = constant * q(x,y,z), where q(x,y,z) is the non-constant part of the appropriate spherical harmonic (eg, x, yz, etc). The first and seventh equations are a bit special \u0026ndash; the first equation is just a constant value and the seventh equation has 2 terms: a constant part and a z^2 term. We can refactor the equations to combine the constant term with the first equation.\nTo make things a little easier on ourselves, we will pre-multiply the spherical harmonic coefficients by the constant values from the associated g value (taking into account the special case for the seventh one).\nThat leads us to this modified version of the original reconstruction equation:  Here, \u0026ldquo;C\u0026rdquo; is the \u0026ldquo;pre-multiplied\u0026rdquo; coefficient.\nWe can also decide how to vectorize these calculation. We can think of this operation as the dot product of two 9 dimensional vectors; one vector for the pre-multiplied values, one vector containing the evaluations of q(x,y,z). We also have to do this 3 times (once for each color primary \u0026ndash; red, green and blue).\nSome older GPUs had vectorized instructions (or SIMD instructions), which meant that it was more efficient to perform instructions on a full 4D vector, rather than single floats. So there are a few implementations around that use two 4D vectors to do 8 components of the dot product for each color channel (and then add the final constant part).\nModern hardware can keep it\u0026rsquo;s ARUs busy even when just performing single float instructions and as a result, that form of vectorization is less important. Instead, it\u0026rsquo;s often easier for us to just rotate the vectorization around and instead use 8 multiply-add operations with 3 dimensional vectors that just represent the rgb color values. This implementation also doesn\u0026rsquo;t require a dot-product instruction (which, incidentally, was important when implementing this way back on the PS2, where dot products were may difficult due to that hardware\u0026rsquo;s instruction scheduling behaviour).\nFinal shader So our final shader implementation turns out to be very basic. XLE uses the second, more straight-forward kind of implementation. We can pre-multiply the \u0026ldquo;C\u0026rdquo; constant values on the CPU side (typically either on a per-object basis or a per-scene basis).\nfloat3 ResolveSH_Opt(float3 premulCoefficients[9], float3 dir)\r{\rfloat3 result = premulCoefficients[0];\rresult += premulCoefficients[1] * dir.y;\rresult += premulCoefficients[2] * dir.z;\rresult += premulCoefficients[3] * dir.x;\rfloat3 dirSq = dir * dir;\rresult += premulCoefficients[4] * (dir.x * dir.y);\rresult += premulCoefficients[5] * (dir.z * dir.y);\rresult += premulCoefficients[6] * dirSq.z;\rresult += premulCoefficients[7] * (dir.x * dir.z);\rresult += premulCoefficients[8] * (dirSq.x - dirSq.y);\rreturn result;\r}\r Recall that the x, y, z inputs to q(x,y,z) are the normal (and it must actually be unit length) \u0026ndash; this is the parameter dir in the above shader. We can choose to evaluate the spherical harmonic on the vertex shader or on the fragment shader. When using normal maps that will typically mean evaluating in the fragment shader.\nFor lower power hardware, if we have reasonably dense geometry we can evaluate in the vertex shader. This style of lighting is naturally fairly soft and smooth, so linear interpolation across the triangle will often produce reasonable results. But there\u0026rsquo;s one more important detail we haven\u0026rsquo;t discussed: the normal must be in the same coordinate space as the evaluated spherical harmonic coefficients! So does this mean we must transform the normal into world space and do all calculations in that space? That\u0026rsquo;s an option, but as we\u0026rsquo;ll see, there\u0026rsquo;s another way as well.\nRotation One of the key properties of systems like this that use the spherical harmonics is that they are rotationally invariant. If means that if we can rotate our diffuse reflection approximation (and we can), then we will not loose any precision. Imagine an environment with a bright white light in the +Z direction. If we rotate the environment so that light is in another direction (let\u0026rsquo;s say, +X), then we want the shape and brightness of the light to remain exactly the same. \u0026ldquo;Rotationally invariant\u0026rdquo; means that this will be the case regardless of how we rotate the environment. Even though the spherical harmonics appear to use x, y \u0026amp; z differentially, there is no actual bias for any direction (this may be clearer to see in the polar coordinate versions of the harmonics).\nTo rotate a spherical harmonic lighting environment, we must take each band one-at-time. Band 0 is just a constant, so it requires no rotation. In band 1 (which is 3 components), the equation is actually just a permutation of the basic 3x3 rotation matrix operation (which should make intuitive sense, given the directional parts in the band 1 are just x, y, z). For the band 2 and beyond, we need something more involved.\nRotating the band 2 There is a general formula for rotating spherical harmonics coefficients of an arbitrary band. We\u0026rsquo;ll use this to handle band 2.\nIn \u0026ldquo;Spherical Harmonic Lighting: The Gritty Details\u0026rdquo;, Appendix 1, Robin Green describes the basic method we\u0026rsquo;ll be using, which is derived from work by Josepth Ivanic and Klaus Ruedenberg. We have to make some changes to the math, however \u0026ndash; are a couple of import modifications we need to make. First, Robin Green isn\u0026rsquo;t using the Condon-Shortley phase (described in the first part), but we are. Second, there\u0026rsquo;s a small errata in the V matrix.\nWe\u0026rsquo;ll start by defining six 5x5 matrices: u, v, w and U, V, W. The elements of the final rotation matrix, M will be constructed from these matrices in this way:\n The elements of u, v and w are dependent only on l (the band index) m and n. In other words, u, v and w are all constant for band 2. The elements of U, V and W are dependent on the rotation matrix for the previous band (ie, the permutated rotation matrix we used for band 1).\nRobin Green shows the definition of these matrices, but given the notes above and since it is a lot of abstract maths, I won\u0026rsquo;t duplicate it here. Instead I\u0026rsquo;ll give some code.\n(BTW, as a side note, if you look closely at the definitions for these matrices, you\u0026rsquo;ll see cases that can result in a divide by zero. In these cases, the element on the complementary matrix is zero.)\nconst int l=2;\rMat5x5 P[3];\rmemset(\u0026amp;P, 0, sizeof(P)); // (i==-l \u0026amp;\u0026amp; l==l rows not filled in by the below loop)\rfor (int i=-1; i\u0026lt;=1; ++i) {\rfor (int a=-1; a\u0026lt;=1; ++a) {\rfor (int b=-l; b\u0026lt;=l; ++b) {\rfloat\u0026amp; dst = P[i+1][a+l][b+l];\rif (b==l) {\rdst = band1Rotation[i+1][2] * band1Rotation[a+1][ l-1+1]\r- band1Rotation[i+1][0] * band1Rotation[a+1][-l+1+1];\r} else if (b==-l) {\rdst = band1Rotation[i+1][2] * band1Rotation[a+1][-l+1+1]\r+ band1Rotation[i+1][0] * band1Rotation[a+1][ l-1+1];\r} else {\rdst = band1Rotation[i+1][1] * band1Rotation[a+1][b+1];\r}\r}\r}\r}\rMat5x5 uMat, vMat, wMat;\rMat5x5 UMat, VMat, WMat;\rfor (int m=-l; m\u0026lt;=l; ++m) {\rfor (int n=-l; n\u0026lt;=l; ++n) {\rauto kroneckerDeltam0 = (m==0) ? 1 : 0;\rauto absm = abs(m);\rfloat d = (n==-l || n==l) ? (2*l*(2*l-1)) : (l+n)*(l-n);\ruMat[m+l][n+l] = sqrtf(float((l+m)*(l-m))/d);\rvMat[m+l][n+l] = .5f * sqrtf(float((1+kroneckerDeltam0)*(l+absm-1)*(l+absm))/d) * float(1-2*kroneckerDeltam0);\rwMat[m+l][n+l] = -.5f * sqrtf(float((l-absm-1)*(l-absm))/d) * float(1-kroneckerDeltam0);\r// note -- wMat[m+l][n+l] will be zero if (m == l-1) || (m == l) || (m == 0)\r// In these cases, the associated 'W' value is undefined -- so we avoid calculating it in those cases\r// For band 2, this is always the case! Since 'w' is always zero, we also never\r// actually need to calculate 'W'\rauto kroneckerDeltam1 = (m== 1) ? 1 : 0;\rauto kroneckerDeltamneg1 = (m==-1) ? 1 : 0;\rfloat\u0026amp; U = UMat[m+l][n+l];\rfloat\u0026amp; V = VMat[m+l][n+l];\rfloat\u0026amp; W = WMat[m+l][n+l];\rU = P[ 0+1][ m+l][n+l];\rassert((n+l) \u0026gt;= 0 \u0026amp;\u0026amp; (n+l) \u0026lt; 5);\rif (m==0) {\rV = P[ 1+1][ 1+l][n+l] + P[-1+1][ -1+l][n+l];\rW = P[ 1+1][ m+1+l][n+l] + P[-1+1][-m-1+l][n+l]; // redundant, but no danger of an out-of-bound matrix access in this case\r} else if (m\u0026gt;0) {\rV = P[ 1+1][ m-1+l][n+l] * sqrtf(float(1+kroneckerDeltam1))\r- P[-1+1][-m+1+l][n+l] * float(1-kroneckerDeltam1);\rif (wMat[m+l][n+l] != 0.f) {\rassert(isValidA(m+1, l) \u0026amp;\u0026amp; isValidA(-m-1, l));\rW = P[ 1+1][ m+1+l][n+l] + P[-1+1][-m-1+l][n+l];\r} else {\rW = 0.f; // (always the case)\r}\r} else if (m\u0026lt;0) {\rV = P[ 1+1][ m+1+l][n+l] * float(1-kroneckerDeltamneg1)\r+ P[-1+1][-m-1+l][n+l] * sqrtf(float(1+kroneckerDeltamneg1)); // (Google)\rif (wMat[m+l][n+l] != 0.f) {\rW = P[ 1+1][ m-1+l][n+l] - P[-1+1][-m+1+l][n+l];\r} else {\rW = 0.f; // (always the case)\r}\r}\r}\r}\r This code has been thoroughly tested, and is reliable. But since this algorithm seems to have been copied out incorrectly in a number of places, I\u0026rsquo;ll also point out a few other identical implementations:\n in MATLAB code also MATLAB C++ (but beware errors in other parts of this source file)  Evaluation in local space Rotating a spherical harmonic coefficient vector allows us to do the shader evaluation in any coordinate space we choose. In some cases, it may be convenient to do the evaluation in the local coordinate space for an object. This is now possible, we just rotate the spherical harmonic coefficients by the inverse of the rotation part of the local-to-world transform (ie, thereby transforming the spherical harmonic coefficients into the object\u0026rsquo;s local space).\nThis is an interesting advantage of spherical harmonics over cubemaps. Because we\u0026rsquo;re defining our environment mathematically and continuously, they can be a little more malleable.\nPractical rotation The above method is a great reference method for rotating band 2. But it\u0026rsquo;s not very practical in real-world cases. We want something that is at least efficient enough to be done once per object. In an ideal world (as we may see later), we want to be able to do it at a much more granular level. But to do this, we need something far more efficient!\nThere are a number of different approaches to optimizing rotations. Some methods introduce some errors, others are accurate. But we break them down into 3 broad categories:\n factoring arbitrary rotations into rotations around cardinal axes replacing the expensive parts of the algorithm, with cheap approximations algebraically refactoring the full rotation algorithm into a form that is specific to band 2 and well suited to our hardware  I\u0026rsquo;m going to talk about a few of the alternatives, before finally explaining the method I\u0026rsquo;ve been using.\nEfficient rotation through cardinal rotations The rotation matrix method above can calculate a transformation matrix for any arbitrary rotation. But it turns out that for some rotations (particularly rotations around the cardinal axes), many elements of the matrix are zero. Furthermore, in the case of rotations around the Z axis, even the non-zero elements are very easy to calculate.\nIn fact, rotation around the Z axis just follow this template:  This can actually help us generate a full rotation matrix for an arbitrary rotation. Recall that it\u0026rsquo;s possible to decompose any 3D rotation into Euler angle form. In this form, the rotation is described by 3 angles, and each angle is a rotation around a cardinal axis. For example, a rotation in \u0026ldquo;XYZ\u0026rdquo; Euler angles form would be 3 angles, one representing a rotation around the X axis, another for a rotation around the Y axis and another for a rotation around the Z axis.\nThere are many different forms of Euler angles, but the form we\u0026rsquo;re going to use is \u0026ldquo;ZYZ\u0026rdquo; form. In this form, there are 2 rotations around the Z axis; but due to the rotation around the Y axis in the middle, the second Z axis is enough to provide the third degree of freedom we need.\nClearly this is convenient because we can just use the simple template above to build the rotation matrices for the two rotations about the Z direction. After we also build the rotation matrix for the rotation around Y, we will just multiply those 3 rotation matrices together.\nBut how to generate a rotation around the Y axis? Unfortunately, rotations around the Y axis don\u0026rsquo;t come out as cleanly as rotations around the Z axis. But we can use another trick. We can rotate the entire coordinate space to align the Y axis with the Z axis. Then we just rotate around the Z axis again, and rotate coordinate space back.\nAligning Y with Z requires a 90 degree rotation around the X axis (though it may depend on the handiness of your coordinate system). This method actually ends up requiring us to multiply together 5 rotation matrices. 3 of those are rotations around Z, and the remaining 2 are constant matrices.\nThis method is called \u0026ldquo;ZXZXZ\u0026rdquo; (referencing the 3 Z rotations and the two \u0026ldquo;aligning\u0026rdquo; X axis rotations). It was first proposed by Kautz, Sloan and Synder in Fast, Arbitrary BRDF Shading for Low-Frequency Lighting Using Spherical Harmonics (hidden away in the appendix).\nSubstituting approximations The ZXZXZ method give us accurate results, but the 2 extra matrix multiplies added by the \u0026ldquo;XZX\u0026rdquo; step are still frustrating. It would be much nicer if we could just generate an arbitrary rotation around X or Y directly.\nSometimes it can be profitable to simplify a complex equation using a standard approximation technique, such as a Taylor series. This has been proposed as solution to our problem by Jaroslav Krivanek, Jaakko Konttinen, Sumanta Pattanaik and Kadi Bouatouch in \u0026ldquo;Fast Approximation to Spherical Harmonic Rotation\u0026rdquo;.\nTheir goal is to rotate coefficient vectors for higher order bands (eg, 5+ bands) that represent specular response. This makes it view-dependent, which requires rotating once per fragment shader (as opposed to once per object, as in our diffuse methods).\nThey found that their method was accurate for rotations less than 20 degrees for their Phong lobe (but keep in mind this only applies to the Y rotation part, the two Z rotations are still accurate). Obviously that level of accuracy is only useful for very specific solutions.\nThe equations they are attempting to approximate don\u0026rsquo;t seem to be well suited to Taylor series. But I wonder if this approach could be improved by adjusting the parameterization.\nFactoring into zonal harmonics Derek Nowrouzezahrai, Patricio Simari and Eugene Fiume describe a method in \u0026ldquo;Sparse Zonal Harmonic Factorization for Efficient SH Rotation\u0026rdquo; that involves decomposing an SH coefficient vector into a number of zonal harmonics coefficients with fixed rotations. They found that the coefficients for band l of an arbitrary SH could be represented by a sum of at most 2l + 1 oriented zonal harmonic coefficients. This leads to a method for rotation that can be more efficient than ZXZXZ, while still being perfectly accurate.\nIn this method they have some freedom as to pick the orientations for the zonal harmonic coefficients. They describe a technique that attempts to minimize the number of zonal harmonic coefficients required, thereby also reducing the rotation cost. It\u0026rsquo;s not exactly trivial, but it\u0026rsquo;s a very interesting approach.\nYou can still find a demo of this technique being used for realtime global illumination in WebGL!\nStill to come So, on this page we covered a lot of stuff \u0026ndash; we wrote the first little bit of shader code, looked at the algorithm for rotating a spherical harmonics coefficient vector, and started explore some methods for optimizing rotations. But there\u0026rsquo;s still a lot to cover! The methods here for rotating efficiently are interesting, but as we\u0026rsquo;ll see, there are better options for our applications.\nAlso to come \u0026ndash; how to apply spherical harmonics to more than just the diffuse reflections we\u0026rsquo;ve been talking about so far.\n","id":3,"section":"posts","summary":"This is a continuation of the tutorial on spherical harmonic applications for games. On the last page, we figured out the core math for integrating a basic diffuse BRDF with a spherical environment of incident light.\nEfficient implementation So far, we\u0026rsquo;ve been dealing with spherical harmonics fairly abstractly. But now we can get down to a concrete use and some concrete code. Given our input texture (which effectively describes a sphere of incident light on a point) we want to calculate, for a given normal direction, what is the appropriate diffuse response.","tags":["SphericalHarmonics"],"title":"Spherical Harmonics and applications in real time graphics (part 3)","uri":"https://djewsbury.github.io/2017/02/sphericalharmonics2/","year":"2017"},{"content":"More on spherical harmonics is coming, but this is a slight intermission about Vulkan. Yesterday, I got a chance to attend a Vulkan workshop (called Vulkan DevU) in Vancouver, Canada. It was a short conference with talks by some of the Vulkan working group, with a mixture of both advanced and beginner sessions. You can get the slides here.\nUnfortunately, isn\u0026rsquo;t been awhile since I\u0026rsquo;ve touched the Vulkan implementation in XLE, and it wasn\u0026rsquo;t fresh in my mind \u0026ndash; but I got a chance to meet some of the working group members and ask a bunch of random questions. Lately it\u0026rsquo;s been difficult to find enough time to properly focus on low level graphics in XLE; I\u0026rsquo;ve had to prioritize other things.\nMy impressions of Vulkan\u0026rsquo;s strong points were re-affirmed by this conference. Vulkan has a fundamentally open nature and is naturally industry driven \u0026ndash; and working group showed a desire to become even more open; both by inviting contributions and suggesting they would change their NDA structure so they can speak about upcoming drafts sooner in the process.\nVulkan creates a practical balance between compatibility and performance; and the team addressed that directly. They also spoke about their desire to keep Vulkan thin and avoid heuristics \u0026ndash; another great property of the library.\nSo, they gave me every reason to think that Vulkan was in good hands. However, during the conversations we also started to touch upon some of the potential risks of the Vulkan concept. They were upfront about the desire to create a successor to OpenGL, which implies a very broad usage of the API (awesome!) but, in my opinion, there are some possible risks:\nDirectX 12 After what seemed like an uncertain start, DX12 looks like it could be very strong. The fact that it shares many fundamental properties with Vulkan makes the two natural competitors.\nPart of DirectX\u0026rsquo;s strength is that is has for many years worked hand and hand with GPU hardware development. Generally important (game oriented) hardware features need to be exposed by the DX API before they are \u0026ldquo;real\u0026rdquo; \u0026ndash; so Nvidia, AMD \u0026amp; Intel must fight it out to guide the API to best suit whatever hardware feature they want to emphasize. Over the years, both Nvidia and AMD have attempted to lessen their dependence on DX (with GL extensions or Mantle, etc) but it hasn\u0026rsquo;t worked so well. If you want to make a major change (eg, multicore GPUs, bindless, etc), game developers have a tendency to ignore it until it\u0026rsquo;s in DX.\nThe problem for Vulkan is that it risks having to just trot along after DX, or becoming too bound to whichever hardware vendor that feels left out in the cold by the DX council (or, alternatively, mobile oriented hardware vendors that don\u0026rsquo;t have as much skin in DirectX)\nShifting responsibilities onto engine teams Vulkan shifts some of the responsibilities that were previously handled by GPU driver teams onto the game engine team. This is particularly obvious in areas such as memory management, scheduling and coherency\u0026hellip; But it\u0026rsquo;s a general principle, sometimes referred to as the \u0026ldquo;explicit\u0026rdquo; principle. For me, this is Vulkan\u0026rsquo;s greatest attribute; but there are risks associated also.\nFor example, what happens next time there\u0026rsquo;s a big change GPU architecture? What happens if there\u0026rsquo;s a mythical \u0026ldquo;paradigm shift\u0026rdquo;? In the old model, it would just be up to the GPU driver developers write a new driver and bind the old API to the new hardware.\nThe working group have said that major revisions of Vulkan will be compatibility breaks. That would open the door to deep re-architecting of the API after hardware shift. We already see that with DX every 10 years or so. But is that going to be enough to solve this problem?\nFor major hardware changes, I guess time will tell. But what abount minor hardware changes \u0026ndash; those that are not enough to require big API changes, but are significant enough to change the performance profiling of applications. What happens if a certain API usage practice, which was previously a best-practice, suddenly becomes extremely slow on some specific hardware\u0026hellip;?\nTo hazard an example, what happens if the \u0026ldquo;descriptor set\u0026rdquo; model for shader inputs becomes inefficient for new hardware?\nIn the Vulkan model, if we want to keep the \u0026ldquo;explicit\u0026rdquo; principle, then this would require game engine teams going in and adapting their code to suit the new hardware. But there are hundreds more game engine teams than there are GPU driver teams! And game engine teams don\u0026rsquo;t frequently maintain older games for newer hardware innovations. So there is a kind of \u0026ldquo;moral\u0026rdquo; hazard here for the GPU driver team to just go in an modify the explicit API path and replace it with some driver magic that is actually doing something else. But if that happens, we\u0026rsquo;re all screwed in the long term \u0026ndash; because Vulkan is so explicit, it must do exactly what it says it\u0026rsquo;s doing.\nSomething tells me that the \u0026ldquo;explicit\u0026rdquo; principle is both Vulkan\u0026rsquo;s greatest attribute, and also it\u0026rsquo;s greatest along-term test.\nIt\u0026rsquo;s impossible to test for the correctness of a Vulkan application This came up in the context of scheduling; but there\u0026rsquo;s more to it than that. Scheduling in Vulkan is incredibly difficult. People were really shocked by the difficulty of the PS3 (and are still today!), but in some ways it\u0026rsquo;s more difficult in Vulkan. We\u0026rsquo;ve got to specify, down to the exact pipeline stage, exactly when an operation is required to be completed.\nThe problem with this is that most hardware will work fine, even if there are major scheduling errors. But, as Tom said, for every scheduling mistake that can be made, there\u0026rsquo;s at least one GPU that it will cause problems with (otherwise that API feature wouldn\u0026rsquo;t exist!)\nThe difficulty is that there is no practical way to test our code, to see if it is correct. All we can do is run it on as much hardware as possible, and see if it breaks. But how do we know what hardware is the most unique, the most likely to break? When there are few Vulkan games, the GPU manufacturers will test for but \u0026ndash; but for the second wave, it\u0026rsquo;s going to take some time to figure those things out.\nVulkan needs high quality layers on top of it Vulkan is too difficult for many game engine teams to approach. Some early games can maybe get through it with help from the working group and driver developers; and experienced console developers will be fine. But for the second wave, many engine teams risk just falling down an endless rabbit hole.\nFurthermore, the benefit in many cases will be limited. I\u0026rsquo;ve seen many game engines that have turned to technologies like Vulkan or Apple Metal for performance improvements. But they\u0026rsquo;ve not found what they sought because they didn\u0026rsquo;t understand their profile well enough to understand that their existing API overhead was already low relative to the overhead in the rest of their engine code. I\u0026rsquo;ve seen that over and over again; it\u0026rsquo;s just common for game engine code to be much less optimized than GPU driver code. In some cases this may mean that replacing driver code with engine code results in a net decrease in performance.\nFor the aspirational industry leaders, the opposite is going to be true (even that the generality of a driver will make it more difficult to optimized). But there are few that fall into that category.\nThe solution is to build reusable mini-engine layers on top of Vulkan. Layers that provide the same kinds of guarantees and ease of use that DirectX does. Ideally these layers should open-source, and they should be written to shield less experienced engine developers from the major pitfalls and vulgarities of low level coding.\nCross-platform gaps One of Vulkan\u0026rsquo;s greatest attributes is it\u0026rsquo;s cross-platform nature. Windows, Mac, Linux, Android, etc, etc \u0026ndash; all in one very thin package. It\u0026rsquo;s fantastic.\nBut there\u0026rsquo;s some gaps here. Lack of support on Apple is a big one. There\u0026rsquo;s another odd one that also came up in the conference \u0026ndash; and that is WebGL. Obviously Vulkan doesn\u0026rsquo;t quite seem right for the web; but this is an odd case because I was thinking about exactly the same thing when the questioner asked about WebGL.\nI actually have a really good reason for wanting a single solution that can work across IOS, Android, Windows, Mac \u0026amp; the web. But it still seems like an odd combination of platforms!\nIt turns out that OpenGLES actually works on all of these platforms (and it\u0026rsquo;s the only solution that does). But OpenGLES is near end-of-life, and all of those platforms have much better solutions except for the web (and point-in-question Android).\nThe other possibility is to use Vulkan for Windows, Mac, Android, Apple Metal for IOS and OpenGLES for web and Android fallback. In some ways that\u0026rsquo;s a ridiculous combination of very different APIs (but what else can I do??!)\nMy take-aways So, I mostly represented myself in my professional context as a mobile games developer (but perhaps I should have put XLE first in this context?). My studio has both the technical and art capabilities to use Vulkan very effectively on mobile, and the willingness to invest in best-in-class solutions. We\u0026rsquo;re also facing some significant internal engine refactoring in the near future, which requires that we consider future trends.\nSo I guess in part I wanted to find the answer to the question of how high should Vulkan be on our list of priorities? That list is deep with important stuff; so there\u0026rsquo;s plenty of competition?\nI very much want Vulkan to succeed; but I\u0026rsquo;m also aware that I\u0026rsquo;m a little biased in that way. It feels like we haven to plan for Vulkan (because we don\u0026rsquo;t want to refactor towards a direction that will be incompatible with Vulkan in the long run). At the moment, I think that Vulkan can bring improvements to our internal day-to-day development; but it\u0026rsquo;s unclear if there will be any great benefit to the final product (ie, we have better options for performance improvement currently, and we still need an OpenGLES fallback for low end hardware, anyway).\nIt feels like Vulkan still has a path to tread before it can establish itself fully. Vulkan must convince game engine programmers to change they way they work; to take on more responsibilities, to understand more of the pipeline, and to publish more code as open-source. In some ways, Vulkan feels like a reaction to Sony console hardware of the past. Right at the moment, my feeling is that Vulkan can only succeed if there are a sufficient number of low-level serious graphics coders spread throughout the industry (such those trained on the PS2 \u0026amp; PS3) that can help address the risks and demonstrate the advantages of the explicit model. Otherwise, the temptation is to just fall back on the likes of Microsoft, Nvidia \u0026amp; AMD, and let them shoulder the load for awhile longer.\n","id":4,"section":"posts","summary":"More on spherical harmonics is coming, but this is a slight intermission about Vulkan. Yesterday, I got a chance to attend a Vulkan workshop (called Vulkan DevU) in Vancouver, Canada. It was a short conference with talks by some of the Vulkan working group, with a mixture of both advanced and beginner sessions. You can get the slides here.\nUnfortunately, isn\u0026rsquo;t been awhile since I\u0026rsquo;ve touched the Vulkan implementation in XLE, and it wasn\u0026rsquo;t fresh in my mind \u0026ndash; but I got a chance to meet some of the working group members and ask a bunch of random questions.","tags":["Vulkan","rendering","API","cross platform"],"title":"Vulkan Workshop / DevU 2017","uri":"https://djewsbury.github.io/2017/01/vulkanworkshop2017/","year":"2017"},{"content":"This is a continuation of the previous page; a tutorial for using spherical harmonic methods for real time graphics. On this page we start to dig into slightly more complex math concepts \u0026ndash; but I\u0026rsquo;ll try to keep it approachable, while still sticking to the correct concepts and terms.\nIntegrating the diffuse BRDF On the previous page, we reconstructed a value from a panorama map that was compressed as a spherical harmonic. The result is a blurry version of the original panorama.\nOur ultimate goal is much more exciting, though \u0026ndash; what if we could calculate the diffuse reflections at a material should exhibit, if it was placed within that environment?\nThis is the fundamental goal of \u0026ldquo;image based\u0026rdquo; real time lighting methods. The easiest way to think about it is this \u0026ndash; we want to treat every texel of the input texture as a separate light. Each light is a small cone light at an infinite distance. The color texture of texel tells us the color and brightness of the light (and since we\u0026rsquo;re probably using a HDR input texture, we can have a broad range of brightnesses).\nEffectively, the input texture represents the \u0026ldquo;incident\u0026rdquo; light on an object. We want to calculate the \u0026ldquo;excident\u0026rdquo; light \u0026ndash; or the light that reflects off in the direction of the eye. We\u0026rsquo;re making an assumption that the lighting is coming from far away, and the object is small.\nSince our input texture has a finite number of texels, we could achieve this with a large linear sum:\n Where t is a texel in the input texture, I is the color in the input texture, d is the direction defined by the mapping (ie, equirectangular or cubemap), a is the solid angle of the texel, and L() is our (somewhat less than bi-directional) BRDF. In spherical harmonic form, we would have to express this equation as a integral (because the spherical harmonic form is a non-discrete function).\nBut we can do better than this. It might be tempting to consider the blurred spherical harmonic reconstruction of the environment as close enough \u0026ndash; but, again, we can do better!\nZonal harmonics We\u0026rsquo;ve talked about using spherical harmonics to represent data across a sphere. But certain type of data can be expressed in a even more compressed form. For example, if the data has rotational symmetry, we can take advantage of that.\nLet\u0026rsquo;s consider a vector dot product:  Where u is a constant 3d vector. If the parameter, x, is a unit length 3d vector, then we have an equation that can be represented as a spherical harmonic. However, this equation is rotationally symmetrical around the u axis (meaning we can rotate x around u without changing the value of f(x)).\nFor convenience, we\u0026rsquo;ll set u to be +Z. Now, only the z component of x has any bearing. As a result, when we build the spherical harmonics for this equation, the harmonics that use the x and y components can be discarded. It turns out that only the harmonics where m = 0 will remain \u0026ndash; that is, we end up with one harmonic per band, and it\u0026rsquo;s the middle one.\nSo, this convention of preferring +Z as the central axis for equations with rotational symmetry is part of the reason we express the harmonics as they are, in the order that they typically appear (but, as we\u0026rsquo;ll find out, the rotational invariance property means that other conventions would work just as well).\nThis new set of harmonics (with rotational symmetry around the +Z axis) are called \u0026ldquo;zonal harmonics.\u0026rdquo; They can be useful for simplifying some operations down to more manageable levels.\nBRDF as a zonal harmonic Obviously, when we say \u0026ldquo;diffuse BRDF\u0026rdquo;, typically we mean the lambert diffuse equation:  This is one of those cases where the \u0026ldquo;normalization factor\u0026rdquo; of 1/pi is important. Sometimes we can ignore that; but here it\u0026rsquo;s critical! (Particularly since we\u0026rsquo;re probably going to combine our diffuse IBL simulation with a specular IBL simulation from the same input texture and we want them to be properly balanced against each other). So, if you\u0026rsquo;re not familiar with it \u0026ndash; better do some research!!\nThe first we need to do is create a spherical harmonic that represents the BRDF itself (notice that our simple BRDF, with one vector parameter, takes the same form as other equations we\u0026rsquo;ve expressed with spherical harmonics \u0026ndash; it, also, is just a function expressed over the surface of a sphere).\nFor the lambert diffuse, this is straight-forward. We\u0026rsquo;ve already seen that we can represent a dot product as a zonal harmonic. All we need to do is modify that for our normalized equation. We can do this by projecting the equation into spherical harmonics (using the formulae in the previous post).\nDiffuse SH methods seem to universally use this simple BRDF\u0026hellip; But it might be an interesting exercise to extend these ideas for more complex BRDFs. 2 candidates come to mind:\n Oren-Nayar, which introduces a view dependent microfacet simulations the Disney diffuse model, which simulates the fresnel effect on both incidence and excidence  But for now, let\u0026rsquo;s just stick with lambert.\nZonal harmonics for lambert diffuse Here are the resulting coefficients for the first 3 bands (these are derived from Peter-Pike Sloan\u0026rsquo;s \u0026ldquo;Stupid SH Tricks.\u0026quot;):\n This is a zonal harmonic \u0026ndash; so we only have one coefficient per band. It\u0026rsquo;s also a polynomial approximation of a non-polynomial function, so it can\u0026rsquo;t be perfectly accurate. We should take a look at the consequence of these inaccuracies before we go forward.\n Here, the orange line is cos(theta)/pi and the purple line is our approximation using SH (the x axis is theta). You can find some similar graphs for higher orders in the above reference. As you can see, our approximation seems to drift off a bit, particularly at theta=0 and theta=pi/2.\nOn the previous page, I showed some images from our final algorithm and another showing the absolute difference from a reference solution multiplied by 30. Here it is again:  The banding suggests that most of the error is coming from high frequency details that are lost to spherical harmonic reconstruction. However, we\u0026rsquo;re also seeing places where bright light appears to be bleeding into dark areas. I wonder if this may be due to the cosine lobe inaccuracies near pi/2.\nConvolving our environment by the cosine lobe Recall the equation we started with:  We can\u0026rsquo;t afford to actually do that sum in real time \u0026ndash; it would mean sampling every pixel in the input texture and multiplying it by the BRDF. We just want some equation that can find the sum in a single step.\nOur L() part of this equation represents the zonal harmonic approximation of the BRDF. That\u0026rsquo;s the quantity of reflection for a particular direction. And I represents the spherical harmonic approximation of our environment. That\u0026rsquo;s the amount of incident light from a direction.\nWe want to find a way to combine these two parts of the equation and evaluate the sum all at once. It turns out, there\u0026rsquo;s a way to do this \u0026ndash; it\u0026rsquo;s called convolution.\nThat might sound difficult enough, but there\u0026rsquo;s even more to it than that. Our cosine lobe zonal harmonic coefficients are defined only for a normal along +Z. We need a way to rotate the zonal harmonic coefficients relative to the spherical harmonic coefficients (which would be the same as rotating the normal from +Z to some arbitrary direction). We would rotate spherical harmonic coefficients (which we can do arbitrarily and without loss) but actually that formula is fairly expensive. We want something more efficient.\nIt turns out that there is a way to do all of this at once \u0026ndash; find the convolution and also rotate the zonal harmonic. The equation is oddly simple, and (fortunately for us!) the result is just another set of coefficients, which when added together will give us the final reconstruction.\nSee Local, Deformable Precomputed Radiance Transfer by Sloan, Luna \u0026amp; Snyder:  Where h are zonal harmonic coefficients, c are spherical harmonic coefficients, y are the basic SH constants (from the previous page), \u0026lt;x,y,z\u0026gt; is the direction that +Z will be rotated onto, and [c * h] are the convolved output coefficients.\nThe result (finally) are coefficients that represent the diffuse response, for a given environment, for a given direction. If we summed these coefficients, we would end up with a value that takes into account all incident light, from all directions. And \u0026ndash; to an extraordinarily high accuracy. This works for any environment, and all of this comes from just 9 color coefficients.\nThis is the equation that makes all of this crazy stuff possible. We\u0026rsquo;ve effectively reduced a very complex integral down to a really basic equation. If we didn\u0026rsquo;t have such as simple formula for this convolution and rotation then math might have become unwieldy enough to be unmanageable.\nOddly, though, the role of this equation within the whole SH diffuse mess is frequently not well described in many papers and sources. I\u0026rsquo;ve come across a bunch of spherical harmonic implementations that have stumbled in this area, perhaps because it\u0026rsquo;s not very clear. Peter-Pike Sloan talks about it in the above paper, and he references Driscoll and Healy for the derivation\u0026hellip; But at that point, that math is getting pretty dense. Other papers (like Ramamoorthi \u0026amp; Hanrahan\u0026rsquo;s seminal paper) tend to deal with it in a somewhat abstract fashion. And in the final shader implemention, the equations will evolve into a completely new form that doesn\u0026rsquo;t look like a convolve at all. So, this step can be easily overlooked.\nThis is a large part of why I wanted to write this tutorial \u0026ndash; to try to make it a little clearer that there is a critical convolve operation in the middle of this application, and show how that evolves into the constants we\u0026rsquo;ll see later.\nStill to come So; we\u0026rsquo;ve figured out how to use spherical harmonics to distill our complex lighting integral down to something manageable and highly compressed.\nBut there\u0026rsquo;s still a bunch to cover!\n implementing efficient shader code rotating arbitrary spherical harmonic coefficients cool applications and tricks!  ","id":5,"section":"posts","summary":"This is a continuation of the previous page; a tutorial for using spherical harmonic methods for real time graphics. On this page we start to dig into slightly more complex math concepts \u0026ndash; but I\u0026rsquo;ll try to keep it approachable, while still sticking to the correct concepts and terms.\nIntegrating the diffuse BRDF On the previous page, we reconstructed a value from a panorama map that was compressed as a spherical harmonic.","tags":["SphericalHarmonics"],"title":"Spherical Harmonics and applications in real time graphics (part 2)","uri":"https://djewsbury.github.io/2017/01/sphericalharmonics1/","year":"2017"},{"content":"Yikes; it\u0026rsquo;s been awhile since my last update!\nI wanted to share a little bit of information from a technique I\u0026rsquo;ve recently been working on for an unrelated project. The technique uses a series of equations called \u0026ldquo;spherical harmonics\u0026rdquo; for extremely efficient high quality diffuse environment illumination.\nThis is a technique that started to become popular maybe around 15 years ago \u0026ndash; possibly because of its usefulness on low power hardware. It fell out of favour for awhile, I was never entirely clear why. I got some good value from it back then, and I hope to get more good value from the technique now; so perhaps it\u0026rsquo;s time for spherical harmonic\u0026rsquo;s star to come around again?\nThere\u0026rsquo;s a fair amount of information about spherical harmonics on the internet, but some of can be a little dense. There seems to be lack of information on how to take the first few steps in applying this math to the graphics domain (for example, for diffuse environment lighting). So I\u0026rsquo;ll try to keep this page approachable for graphics programmers, while also linking off to some of the more dense and abstract stuff later on. And I\u0026rsquo;ll focus specifically on how I\u0026rsquo;m using this math for graphics, how I\u0026rsquo;ve used it in the past, and how I\u0026rsquo;d like that to grow in the future.\nWhat are spherical harmonics The \u0026ldquo;spherical harmonics\u0026rdquo; are a series of equations that we\u0026rsquo;ll use to compress lighting information greatly. The easiest way to understand them is to start with something simpler and analogous \u0026ndash; and that is cubic splines.\nSplines are a method of describing a curve through space with a finite number of points (or points and tangents). Even though the curve is defined by a finite number of parameters, the result is effectively an infinite number of points. To define the curve, we need equations of the form:\n Where t is the distance along the spline (usually between 0 and 1) and x, y \u0026amp; z are cartesian coordinates. In the case of cubic splines, the functions f, h and g are cubic polynomials. If you\u0026rsquo;ve used Bezier splines before, you may be familiar with a way to express these polynomials using a form called a \u0026ldquo;Bernstein polynomial\u0026rdquo;.\nBernstein basis Imagine the folowing 4 equations: By Ben FrantzDale (Graphed in Matplotlib.) [GFDL (http://www.gnu.org/copyleft/fdl.html) or CC BY-SA 4.0-3.0-2.5-2.0-1.0 (http://creativecommons.org/licenses/by-sa/4.0-3.0-2.5-2.0-1.0)], via Wikimedia Commons\nThese are the Bernstein basis functions (B0(), B1(), B2(), B3()). Using these functions, we can express a cubic polynomial in this form:  Where w0, w1, w2 \u0026amp; w3 are constant weights. The four weight values will determine the shape of the curve we get. This is the important thing \u0026ndash; the Bernstein basis functions are defined so that we get any cubic polynomial function, just by changing the 4 weight values. And the four weight values provide a convenient way to express a spline (alternatively we can do similar things with other basis functions \u0026ndash; like the Hermite basis, or for different types of equations, the Fourier basis).\nWhen dealing with cubic polynormals, we use 4 Bernstein basis equations; but the equations are defined for any degree polynomials. So, for example, if we were working with 5th degree polynomials, then we could calculate 6 associated Bernstein basis functions.\nSH basis But we want to know how this relates to spherical harmonics! (But the interested might want to diverge off to Wolfram \u0026amp; Wikipedia: http://mathworld.wolfram.com/BernsteinPolynomial.html, https://en.wikipedia.org/wiki/B%C3%A9zier_curve).\nIt turns out that the \u0026ldquo;spherical harmonics\u0026rdquo; are actually a series of equations which are a little bit similar to the Bernstein basis functions (\u0026ldquo;harmonic\u0026rdquo; being just a name for a certain kind of equation). But the spherical harmonics are special because they are defined for a very specific type of data. And that is values that are defined across the surface of a sphere\u0026hellip;\nAbove, we used Bernstein basis functions to express polynomials. But the type of function that can be expressed using spherical harmonics looks a little like this:  Here, theta and phi are spherical coordinates. Also note that another way to express the spherical coordinates theta and phi is by using a unit length vector!\nSimplest application in graphics So why is this type of equation useful? Well, there\u0026rsquo;s another common construct in graphics that represents values stored over the surface of sphere \u0026ndash; that is a cubemap.\nWhen we use a cubemap for the background sky in a shader, we probably have a line of shader code a little like this:\nfloat3 skyColor = MySkyCubeMap.Sample(MySampler, unitLengthVector.xyz).rgb;\r For any given direction, the cubemap will give us a colour value. Even though it\u0026rsquo;s a cubemap, it\u0026rsquo;s defining colour values across the surface of a sphere (before cubemaps we used other mappings, like spheremaps and paraboloid maps \u0026ndash; but it turns out that, oddly, a cube is just the cheapest approximation to a sphere for shaders). We could use spherical harmonics to do something very similar.\nIn the spline example, the analogous operation would be fitting a curve to texture values. So imagine we had an RGB single dimensional texture. We would need 1 spline for each component, but let\u0026rsquo;s take red as an example. Since this is a 1D texture, we can graph the red values against the (1D) texture coordinate as so:\n In theory, we can find a best fit cubic spline for this data. If we do that, we can use the spline as a (non-discrete) replacement for the texture (if we were doing the reverse \u0026ndash; using a texture as a replacement for a function \u0026ndash; we would called it a lookup table).\nThe degree of the spline will determine how accurately we can approach the source data. Most textures would need very high degree polynomials to even get close. In general, high frequency information is hard to replicate, but low frequency information can sometimes come through.\nIn practice, this usually isn\u0026rsquo;t a very useful thing to do \u0026ndash; though there are applications along this line of thought (such as cubic interpolation between texels, particularly with terrain heightmaps). We\u0026rsquo;re also starting to tread on \u0026ldquo;signal processing\u0026rdquo; ground (which we might use for deep ocean wave simulations, like the one in XLE).\nSo why would we ever want to replace a cubemap with a function using spherical harmonics? Isn\u0026rsquo;t that just as useless? The answer is no! There are some important cases where spherical harmonics are both accurate enough and useful enough to be worth our attention!\nCartesian spherical harmonics The spherical harmonics can be expressed in spherical coordinates or cartesian coordinates. For graphics, the cartesian form is much more useful.\nOur notation for the harmonics themselves will be this:  Where l (lower case L) is the \u0026ldquo;band index\u0026rdquo; and m is a value with the constraint -l ≤ m ≤ l. The number of bands is the analogue of the degree of a polynomial \u0026ndash; more bands means greater accuracy and better reproduction of higher frequency data. For realtime stuff, we\u0026rsquo;ll focus on the first 3 \u0026ldquo;bands\u0026rdquo;. If you look around Wolfram or Wikipedia, you\u0026rsquo;ll see more complex forms of the equations. But this form is good for our applications.\nThere are the first 3 bands, in the form we\u0026rsquo;ll use in XLE:  To get a sense of how these equations look in 3D space, here\u0026rsquo;s a picture of the first 4 bands from Wikipedia (note that the blue represents positive numbers, the yellow is negative): By Inigo.quilez (Own work) [CC BY-SA 3.0 (http://creativecommons.org/licenses/by-sa/3.0)], via Wikimedia Commons\nIt\u0026rsquo;s important to be aware that you\u0026rsquo;ll see slightly different variations of these equations in different publications. This is why it\u0026rsquo;s not always safe to combine equations from different sources (such as the rotation equations and optimization equations we\u0026rsquo;ll get to later).\n this form employs the \u0026ldquo;Condon-Shortley phase\u0026rdquo;. This basically just adds the negative sign in front of some of the equations. This reduces the complexity of some of derived equations (such as the equations for rotation). Not all graphics literature use this form (for example, Robin Green doesn\u0026rsquo;t, but Peter-Pike Sloan does) the order in which y, z \u0026amp; x appear in band 1 can be different in different forms (this ordering seems to be most common, but it does make the rotation code a little more confusing) this form assumes that the vector [x y z] is unit length (ie: x^2+y^2+z^2=1). For our applications, that\u0026rsquo;s always true \u0026ndash; but many sources don\u0026rsquo;t mention that as an assumption the equation for m=0 \u0026amp; l=2 is often expressed with 2z^2 - x^2 - y^2 in the brackets. But, given the unit length assumption, this equivalent finally, there seems to be some uncertainty about the constant in the equation when m=2 \u0026amp; l=2 (maybe?). While working on this page, I noticed that some sources have 15/16, others have 15/32. I\u0026rsquo;m not sure what' going on there \u0026ndash; I might have to re-derive the equation to figure that out  In short, we can\u0026rsquo;t just copy/paste our why through spherical harmonics. We have to know what\u0026rsquo;s going on. (It\u0026rsquo;s also a good example of why you should never trust the correctness of any code you find on the internet!)\nProjecting into spherical harmonics and reconstruction As mentioned above, we want to combine the spherical harmonics with weight values, and use this combination to express some complex spherical function. Converting a function into spherical harmonics is called \u0026ldquo;projection.\u0026rdquo; Using the spherical harmonics to get back an approximation of the original function is called \u0026ldquo;reconstruction.\u0026rdquo;\nThe projected form of a function is just the set of weights \u0026ndash; one weight for each spherical harmonic (up to some limit on the number of bands). The weights are called coefficients. Let\u0026rsquo;s say our input function is the red channel of a cubemap. We\u0026rsquo;ll use 3 bands to approximate that \u0026ndash; that\u0026rsquo;s 9 equations, and 9 coefficients. Since we also want the green and blue channels, that\u0026rsquo;s another 9 coefficients per channel.\nIn effect, we end up with an approximation of the input cubemap in just 9 color values. Obviously we can\u0026rsquo;t represent much high frequency data with so few colours. We could get higher quality versions with more bands (4 bands would be 16 colour coefficients, 5 would be 25, 6 would be 36). This might be useful for spherical reflections, but for diffuse (as we\u0026rsquo;ll see later) there\u0026rsquo;s a good reason to stop at 3 bands.\nProjection and Reconstruction Projection is surprising simple. We can calculate each coefficient separately, and we do that by just finding the integral of the input function multiplied by the corresponding spherical harmonic.\n Since our input function is discreet (ie, it\u0026rsquo;s just pixels in a cubemap), the integral becomes simple. We just need to multiply the pixel values by the spherical harmonic for the cubemap direction \u0026ndash; the only difficulty is to weight for the solid angle of the cubemap pixel (see the XLE shader code for this equation).\nThe reconstruction formula is also very simple; and should now look oddly familiar:\n Here, we just take the weighted sum of the spherical harmonics for a given direction (where b is the band number limit). It\u0026rsquo;s pretty straight forward; but it gets more complex from here.\nExample So what does it look like? Here\u0026rsquo;s an example environment texture and the equivalent constructed environment. I\u0026rsquo;m going to skip ahead just a bit and show how it looks when we construct diffuse reflectance from an environment texture. These are equirectangular panorama maps; trivially compressed to LDR jpgs.\nSource texture  (from sIBL Archive: http://www.hdrlabs.com/sibl/archive.html)\nReference diffuse response ie, for each direction in the panorama map, this shows the correct diffuse response of the environment, from a plain lambert diffuse BRDF.  Reconstructed response with spherical harmonics compression  Absolute difference multiplied by 20  The reconstructed response is using 3 bands; so it\u0026rsquo;s stored using just 9 colour values. As you can see, there is some information loss \u0026ndash; but we can actually prove mathematically that the maximum error is pretty small.\nTo be continued So, we\u0026rsquo;ve got the very basics down. We know enough to be able to compress a cubemap down to spherical harmonic form, and reconstruct an approximation from that.\nAlone, it may sound like an interesting thing to do; but it\u0026rsquo;s not very useful yet. I\u0026rsquo;ll follow this up with some details on how we can use spherical harmonics for some really interesting stuff. Still to cover:\n combining our compressed environment information with the lambert diffuse equation building optimized reconstruction code for shaders rotating and manipulating spherical harmonics at runtime (and optimization thereof) applications in global illumination schemes some examples from around the web, and a list of the right literature to follow up with some cool examples of SH used in games (eg; The Last of Us, The Order: 1886)!  ","id":6,"section":"posts","summary":"Yikes; it\u0026rsquo;s been awhile since my last update!\nI wanted to share a little bit of information from a technique I\u0026rsquo;ve recently been working on for an unrelated project. The technique uses a series of equations called \u0026ldquo;spherical harmonics\u0026rdquo; for extremely efficient high quality diffuse environment illumination.\nThis is a technique that started to become popular maybe around 15 years ago \u0026ndash; possibly because of its usefulness on low power hardware.","tags":["SphericalHarmonics"],"title":"Spherical Harmonics and applications in real time graphics","uri":"https://djewsbury.github.io/2016/12/sphericalharmonics0/","year":"2016"},{"content":"Work is still continuing on XLE! Over the last few weeks, I\u0026rsquo;ve have been very distracted by other priorities. I\u0026rsquo;m likely to be somewhat busy and distracted during for a few more weeks until things return to a more normal situation. However, I\u0026rsquo;m still finding time to work on XLE, and make some improvements and fixes!\nLately, my focus has been on Vulkan support (in the experimental branch). Vulkan is looking more and more stable and reliable, and the HLSL -\u0026gt; SPIR-V path is working really well now! However, there are still some big things I want to improve on:\n Improved interface for BoundUniforms, that can allow for precreation of descriptor sets during loading phases Improved interface for Metal::ConstantBuffer and temporary vertex/index buffers that would rely on a single large circular device buffer Pre-create graphics pipelines when using SharedStateSet (eg, when rendering RenderCore::Assets::ModelRenderer) (this might also involve a better solution for the render state resolver objects used by materials) Fix triangle hit-test supported used by the tools (which is complicated by the render pass concept and requires stream output support) Tessellation support for terrain rendering Integration of MSAA resolve for Vulkan  With these changes, we should start to see much more efficient results with the Vulkan pipeline. Some SceneEngine features won\u0026rsquo;t work immediately, but otherwise we will be getting very good Vulkan results, while also retaining the DirectX compatibility!\nLately, there have been many cool improvements related to Vulkan, including:\n Lots of improvements to the HLSLcc cross compiler that coverts HLSL -\u0026gt; GLSL (which becomes SPIR-V) this is now working for almost all shaders, and adds support for many HLSL features that previously weren\u0026rsquo;t well supported by the HLSLcc project Support for using Vulkan within the GUI tools! Properly tracking GPU progress for object destruction, allowing full descynronisation of CPU and GPU Support for event pools, and the GPUProfiler Reduction of dependencies on the RenderCore::Metal project by projects that should be gfx-api agnostic Core lighting support including IBL and shadows  Please keep checking back regularly for the latest updates!\n","id":7,"section":"posts","summary":"Work is still continuing on XLE! Over the last few weeks, I\u0026rsquo;ve have been very distracted by other priorities. I\u0026rsquo;m likely to be somewhat busy and distracted during for a few more weeks until things return to a more normal situation. However, I\u0026rsquo;m still finding time to work on XLE, and make some improvements and fixes!\nLately, my focus has been on Vulkan support (in the experimental branch). Vulkan is looking more and more stable and reliable, and the HLSL -\u0026gt; SPIR-V path is working really well now!","tags":["Vulkan"],"title":"Latest Update","uri":"https://djewsbury.github.io/2016/06/juneupdate/","year":"2016"},{"content":"XLE has a thin layer over the underlying graphics API called \u0026ldquo;Metal\u0026rdquo;. This was originally built for DirectX11 and OpenGLES. But over time it became more DirectX-focused. Part of the goal of building in Vulkan support was to provide a basis for refactoring and improving the metal layer.\nThe goals for this layer are simple:\n compile time polymorphism between underlying graphics APIs not link time or run time. We know the target during compilation of client code \u0026ldquo;leaky\u0026rdquo; abstraction layer meaning that most client code is independent of the underlying graphics API but the underlying objects are still accessible, so client code can write API-specific code when needed very thin, minimal overhead for example, many DeviceContext methods get inlined into client code, meaning that performance is similar to using the underlying API directly  To make this kind of layer work, we need to find abstractions that work well for all target APIs. Usually this means finding abstractions that are great for one API, and pretty good for other APIs. That can be tricky, particularly as APIs are changing and evolving over time.\nDescriptor Set concept Ideally we want the concept of \u0026ldquo;descriptor sets\u0026rdquo; to exist in the metal API somewhere. There are two reasons for this:\n Clean up the DeviceContext interface so there are fewer BindXX(\u0026hellip;) methods pre-cook permanent descriptor sets using BoundUniforms (or otherwise), so that they can be reused frame to frame  DirectX11 has no concept of descriptor sets, though, so this could be a bit awkward. There are a couple of tricky problems\nSharing descriptors across shader stages In modern APIs, we can bind a descriptor set to multiple shader stages; meaning that binding a resource one can make it accessible to multiple shaders. However, DirectX11 strictly separates the stages so that we explicitly bind only to a single stage.\nSo how to we create a single solution that handles both these cases?\nOne option is to expand the \u0026ldquo;root signature\u0026rdquo; concept. This defines a set of virtual binding points, which can be redirected to the true underlying binding points. The virtual binding points can know which shader stages they apply to \u0026ndash; and so can calls the appropriate underlying binding functions.\nThis might have some long-term advantages because it also allows us to abstract the C++ code completely from the true binding points expressed in the shader files. There\u0026rsquo;s a little extra overhead, but maybe it\u0026rsquo;s not a huge issue.\nThat would be a great solution for \u0026ldquo;frame-global\u0026rdquo; or long-term bindings. It would also be great for our few global SamplerState objects (which can easily become immutable in Vulkan, and handled automatically in DirectX).\nBut for short-term dynamic bindings, it may not be great. Often we\u0026rsquo;re just binding one or two resources or constant buffers to the first few binding points, and using them for a single draw call. For this, we might need something different.\nIf the \u0026ldquo;dynamic\u0026rdquo; descriptor set in Vulkan was very large, we could more easily have separate binding points for resources that are shared between stages, and those that are used only in one. However, currently we need to keep this descriptor set small because we need to write to all binding points, regardless of whether they are used or not. If a shader accesses an unused descriptor, we will get a device-lost error.\nThis could be made better by calculating a mask of the used slots in the dynamic descriptor set for each shader. This would allow us to make better decisions about when to write to this dynamic descriptor set. Most draw calls don\u0026rsquo;t need this dynamic set (it\u0026rsquo;s mostly required by procedurally driven effects, and less important for data driven geometry like models), and this approach should reduce the overhead in the most common cases.\n","id":8,"section":"posts","summary":"XLE has a thin layer over the underlying graphics API called \u0026ldquo;Metal\u0026rdquo;. This was originally built for DirectX11 and OpenGLES. But over time it became more DirectX-focused. Part of the goal of building in Vulkan support was to provide a basis for refactoring and improving the metal layer.\nThe goals for this layer are simple:\n compile time polymorphism between underlying graphics APIs not link time or run time. We know the target during compilation of client code \u0026ldquo;leaky\u0026rdquo; abstraction layer meaning that most client code is independent of the underlying graphics API but the underlying objects are still accessible, so client code can write API-specific code when needed very thin, minimal overhead for example, many DeviceContext methods get inlined into client code, meaning that performance is similar to using the underlying API directly  To make this kind of layer work, we need to find abstractions that work well for all target APIs.","tags":["Vulkan","cross platform","Metal"],"title":"Vulkan prototype - metal layer refactoring","uri":"https://djewsbury.github.io/2016/05/metallayerrefactoring/","year":"2016"},{"content":"The Vulkan build is steadily getting more and more functionality. Now the core rendering pipeline in SceneEngine is working \u0026ndash; which means we can have deferred lighting, shadows, IBL, tonemapping, etc. Simple scene should render correctly now. But there are some inefficiencies and issues (see below).\nUnfortunately the DirectX11 version isn\u0026rsquo;t working at the moment. This is all in the \u0026ldquo;experimental\u0026rdquo; branch.\nDeclarative render passes Tone-mapping now works, and it was a good prototype for the \u0026ldquo;declarative\u0026rdquo; render pass model. This allows us to specify render passes and render targets required using a \u0026ldquo;description\u0026rdquo; structure. The system will do some caching and correlate request to resources, creating and binding as necessary.\nThere is some overhead with this design because it involves doing some per-frame hash and lookups as we go along. It\u0026rsquo;s not as efficient as (for example) just pre-creating all of the frame buffer / render pass objects in a configure step. However, this design is maybe a little more flexible and easier to tie into existing scene engine code. In effect, we\u0026rsquo;re building a new layer that is just one step more abstract from the underlying Vulkan objects.\nI\u0026rsquo;ve pushed some of the \u0026ldquo;busy-work\u0026rdquo; (like declaring subpass dependencies) down into the RenderCore::Metal layer. This makes the interface easier to use\u0026hellip; But the downside is that my abstraction is not expressive enough for some unusual cases. For example, I came across a cases where we want to bind the \u0026ldquo;depth \u0026amp; stencil\u0026rdquo; aspects of a depth texture in one subpass; and in the second subpass only the \u0026ldquo;stencil\u0026rdquo; aspect is bound. This apparently needs a dependency\u0026hellip; But it\u0026rsquo;s just really inconvenient with this interface.\nI\u0026rsquo;ve also build a concept called \u0026ldquo;named resources\u0026rdquo; into the Metal::DeviceContext. This allows us to get TextureViews for attachments from the device context. It feels out of place because it\u0026rsquo;s an operation that doesn\u0026rsquo;t involve the hardware, but there doesn\u0026rsquo;t seem to be any better way to handle this case.\nFundamentally we want to define attachments FrameBufferDesc objects, so that we can later refer to them again by binding id. It would be better if some of this functionality was in the RenderCore::Techniques library\u0026hellip; But it would be just too much hassle to split it better Techniques and Metal.\nAnyway, it\u0026rsquo;s working now in Vulkan. However, I still haven\u0026rsquo;t got to the caching and reuse part. And it also needs to be implemented for DirectX11, also!\nCompute shader work! I added in support for the compute pipeline. It actually was pretty easy. I decided to switch some of the tonemapping code from pixel shaders to compute shaders \u0026ndash; because this seems to be more natural in Vulkan. Working with viewports and render targets is much more complex in Vulkan than DirectX11.\nRender state objects working \u0026amp; dynamic pipeline objects Now all of the render state objects (like BlendState, SamplerState, etc) work. However all of the \u0026ldquo;pipeline objects\u0026rdquo; are dynamically created as needed. Vulkan allows these to be pre-calculated and optimised and load time. This is pretty smart, because a lot of that render state information can just become shader instructions and combined into the shader code.\nI think the SharedStateSet techniques might be able to precalculate pipeline objects. But a lot of pipeline objects will have to be dynamically created like this. But perhaps I can use the \u0026ldquo;inheritance\u0026rdquo; stuff in pipeline objects to calculate some stuff earlier (for example, in ShaderPrograms).\nFor the moment, pipeline objects are just created and recreated like crazy!\nSeparated Samplers and Textures in HLSLCrossCompiler I found that GL_KHR_vulkan_glsl added support for separate Sampler and Texture objects to GLSL. So I added support for this to the HLSLCrossCompiler! This is a huge help, because otherwise it would be a hassle to work around the standard HLSL model of separate objects.\nMany fixes in HLSLCrossCompiler As I go through, I\u0026rsquo;m finding issues in the HLSLCrossCompiler. It\u0026rsquo;s quite strange because the compiler supports so many instructions and modes\u0026hellip; But they there are certain things that are just incorrect. In particular, I\u0026rsquo;ve had some fixes to certain swizzle operations, and made some fixes dealing with constant buffers.\nIt\u0026rsquo;s really quite fascinating because this cross compiler path works so well for so many shaders\u0026hellip; I\u0026rsquo;m really using a lot of HLSL features in XLE, and I\u0026rsquo;m often happily surprised by the cross compiler just \u0026ldquo;figuring it out.\u0026rdquo; But then suddenly some shader will use an instruction in a slightly different way, and everything falls apart.\nAnyway, it\u0026rsquo;s getting more reliable as I go along.\nBetter model for descriptor sets I finally settled on a much better model for descriptor sets. I borrowed some ideas from the DirectX12 \u0026ldquo;root signatures\u0026rdquo; \u0026ndash; these allow us to creating a mapping between the linear register mapping in HLSL (eg, register(t2), register(b3), etc) and a system of descriptor sets and binding points.\nSo I\u0026rsquo;ve re-purposed that idea for XLE. The root signatures actually also defines the \u0026ldquo;descriptor set layouts.\u0026rdquo; This allows us to create a few global layouts, and reuse them for all shaders. This seems to be the best way.\nWe have 4 descriptor sets available; I\u0026rsquo;m using one for BoundUniforms, one for \u0026ldquo;dynamic\u0026rdquo; bindings (not-BoundUniforms) and one for global resources that remain bound the entire frame. And so there\u0026rsquo;s one left over; maybe it will be used for input attachments.\nThere\u0026rsquo;s still some thrashing that can occur. But this design seems reasonable. I may expand the interface for BoundUniform that will allow for every efficient use.\nMore to come\u0026hellip; So it\u0026rsquo;s working! But there\u0026rsquo;s still a lot to come. Some things got broke while changing things, and there\u0026rsquo;s still a lot of perform improvements to make. But it\u0026rsquo;s looking ok so far.\n","id":9,"section":"posts","summary":"The Vulkan build is steadily getting more and more functionality. Now the core rendering pipeline in SceneEngine is working \u0026ndash; which means we can have deferred lighting, shadows, IBL, tonemapping, etc. Simple scene should render correctly now. But there are some inefficiencies and issues (see below).\nUnfortunately the DirectX11 version isn\u0026rsquo;t working at the moment. This is all in the \u0026ldquo;experimental\u0026rdquo; branch.\nDeclarative render passes Tone-mapping now works, and it was a good prototype for the \u0026ldquo;declarative\u0026rdquo; render pass model.","tags":["Vulkan","lighting","cross platform","shaders","HLSL","SPIR-V"],"title":"Vulkan latest progress -- core lighting working","uri":"https://djewsbury.github.io/2016/05/vulkanmoreprogress/","year":"2016"},{"content":"The lighting parser is a set of steps (including geometry rendering and full screen passes) that occur in a similar order and configuration every frame. In some ways it is like a higher level version of a \u0026ldquo;renderpass\u0026rdquo; (or frame buffer layout). Each \u0026ldquo;subpass\u0026rdquo; in the renderpass is like a step in the lighting parser process for evaluating the frame.\nBut how do we map the lighting parser steps onto render pass subpasses? Do we want to use one single huge render pass for everything? Or a number of small passes?\nDeclarative lighting parser Since we\u0026rsquo;re using a \u0026ldquo;declarative\u0026rdquo; approach for \u0026ldquo;frame buffer layouts\u0026rdquo;, we could do the same for the lighting parser. Imagine a LightingParserConfiguration object that takes a few configuration variables (related to the current platform and quality mode, and also scene features) and produces the set of steps that the lighting parser will perform.\nWe could use the VariantFunctions interface for this, along with some structures to define the \u0026ldquo;attachment\u0026rdquo; inputs and outputs for each step. If we have a step (such as tonemapping) we just define the input attachments, output attachments, and temporary buffers required, and add the tone map function.\nThen we would just need a small system that could collect all this information, and generate a RenderCore::FrameBufferDesc object from it.\nThis would allow us to have a very configurable pipeline, because we could just build up the list of steps as we need them, and from there generate everything we need. In effect, we would declare everything that is going to happen in the lighting parser before hand; and then the system would automatically calculate everything that needs to happen when we call Run().\nThis could also allow us to have a single render pass for the entire lighting parser process. But is that what we really want?\nHow many render passes? There are a few things to consider:\n Compute shader usage frame-to-frame variations in the render pass parallization within the lighting parser  Compute shaders We can\u0026rsquo;t execute compute shaders during a render pass. vkCmdDispatch can only be used outside of a render pass. This is an issue because some of our lighting parser work current uses compute shaders (such as the tone mapping) \u0026ndash; effectively meaning that the render pass must be split there.\nThis could be converted to pixel shaders, I guess. But it would mean that we would have to either rewrite all of the compute shader stuff, or move it to a precalculation phase before the render pass.\nAlso, sometimes we might want to call vkCmdCopyImage to duplicate a buffer. Again, it can be emulated with pixel shaders, I guess.\nFrame-to-frame variations We don\u0026rsquo;t really want the render pass to change from frame to frame, because the render pass is a parameter to Vulkan graphics pipelines. There is some flexibility for \u0026ldquo;compatibility\u0026rdquo; between render passes. However, what we don\u0026rsquo;t want is a situation where a change related to one subpass effects the pipelines of another subpass. In that case, it would be better to use 2 separate render passes, so that changes in one stage of the pipeline don\u0026rsquo;t effect other stages.\nThis could be an issue with postprocessing effects that are sometimes enabled (such as a radial blur when damaged, or refractions in water). We could always just skip a subpass by calling NextSubpass immediately, I guess. But having many optional subpasses might blunt the benefit of having a huge render pass, anyway.\nParallization within the lighting parser Most of the steps in the lighting parser are not really very parallizable with respect to each other. For example, the steps before tonemapping must all be complete before tonemapping occurs, and the steps after tonemapping can\u0026rsquo;t begin until tonemapping ends.\nSo, in this situation, it\u0026rsquo;s not clear how much benefit there is in combining everything in a giant render pass.\nSimple lighting parser The lighting parser was originally intended to have a very simple structure involving just conditions and function calls. It represents a fundamentally procedural operation, so it makes sense to implement it as basic procedures. That seems like the clearest way to give it a clear order and structure.\nHowever, moving to a more declarative pattern divorces us from that goal a little bit. That is, adding extra levels of abstraction within the lighting parser could also make it more difficult to understand and maintain.\nConclusion So, all in all, it\u0026rsquo;s not really clear what benefit there would be to have a single huge render pass. It seems more sensible to instead use a number of smaller render passes, one for each phase of the pipeline.\nThis would probably mean a structure such as this:\n initial opaque stuff   creating the gbuffer   opaque light resolve   perform deferred lighting ScreenSpaceReflections requires some compute shader work taking gbuffer as input and consumed during lighting resolve   multiple separate render passes for translucent rendering   in here we have things like ocean rendering, ordered transparency methods, particles, etc many of these methods require extra buffers and special requirements   possibly MSAA resolve, post processing steps and tonemapping could be combined into a single render pass  The configurable lighting parser structure from above might still be useful for the post processing steps at the end.\nThis means that the \u0026ldquo;gbuffer\u0026rdquo; will only be an \u0026ldquo;attachment\u0026rdquo; during the very first render pass. In other passes, it can be a normal image view / shader resource.\n","id":10,"section":"posts","summary":"The lighting parser is a set of steps (including geometry rendering and full screen passes) that occur in a similar order and configuration every frame. In some ways it is like a higher level version of a \u0026ldquo;renderpass\u0026rdquo; (or frame buffer layout). Each \u0026ldquo;subpass\u0026rdquo; in the renderpass is like a step in the lighting parser process for evaluating the frame.\nBut how do we map the lighting parser steps onto render pass subpasses?","tags":["Vulkan","cross platform"],"title":"Vulkan prototype - how many separate render passes in the lighting parser?","uri":"https://djewsbury.github.io/2016/04/vulkandeclarativelightingparser/","year":"2016"},{"content":"I\u0026rsquo;m making more progress on the Vulkan prototype. Now the SceneEngine compiles, BufferUploads works, and it\u0026rsquo;s now possible to render basic geometry. Many features aren\u0026rsquo;t supported. But the core rendering features (such as binding textures, compiling shaders, pushing geometry through the pipeline) have an early implementation.\nSo, while a working Vulkan version is now close, I have a much better idea of what major hurdles there would be to get a really streamlined and efficient Vulkan implementation!\nRenderPasses This is going to be the big issue. RenderPasses are a new concept in Vulkan (though with heritage from Apple Metal and OpenGL), and not very compatible with the DirectX way of handling render targets. This is going to require some deep thought!\nI haven\u0026rsquo;t researched this thoroughly \u0026ndash; but this is my basic understanding of what might be happening\u0026hellip;\nRenderPasses seem to be fundamentally designed around PowerVR-style hardware (such as that found in IPhones and some Android hardware). Now, if I understand the situation correctly, this type of hardware doesn\u0026rsquo;t need a traditional depth buffer. Instead it collates triangles as they are submitted, then splits them into buckets for tiles on the screen, and finally decomposes them into scanlines. The scanlines can be sorted using an old algorithm, so that for every pixel we know the front-most scanline.\nThe key issue here is that that we collate triangle information in the frame buffer, and do not produce a final pixel value until all triangles are finished. This requires the hardware to maintain buffers of triangle scanlines attached to the frame buffer.\nThe key problem here is this: what happens if we have render targets A and B, both of which are used in the same frame. We render to A, then to B, and then switch back to A and start rendering again?\nIn a traditional DirectX OMSetRenderTargets environment, this is not a big issue. We might do this (for example) to render a reflection into target B, which appears on geometry in target A. Maybe we cause a bit of a GPU pipeline stall; but it\u0026rsquo;s probably not going to be a major issue.\nWith the PowerVR-style hardware, however, it\u0026rsquo;s a bigger deal. We want to continue collating triangle information in A throughout the entire frame. We don\u0026rsquo;t want to separate that into 2. It would be better if A and B were actually 2 separate viewports onto the same frame buffer.\nIn other words, when we switch away from A, we kind of want to know if we will be returning to it. That\u0026rsquo;s what RenderPasses are useful for. We can express to the API that A is not finished yet, and that B just contains temporary data that will eventually be used on A.\nAMD claim on their website that this expressiveness is also useful for traditional forward rendering hardware. But it probably isn\u0026rsquo;t quite so significant.\nNevertheless, Vulkan appears to be built entirely around RenderPasses, and the interface is very different from OMSetRenderTargets. So we need to figure out what to do.\nRenderPass integration There are 2 approaches:\n Make RenderPasses a first-class feature of RenderCore::Metal, and deprecate binding render targets   this would require all client code to change to using RenderPasses. For DirectX, the RenderPass object would just generate OMSetRenderTarget calls  Dynamically build simple RenderPasses when binding render targets   the client code wouldn\u0026rsquo;t need to change. However, the render passes generated would not be ideal. It would also require calling VkCreateRenderPass and VkCreateFramebuffer during the frame (which is probably not ideal)  Support both RenderPasses and binding render targets   clients can choose to use either  I\u0026rsquo;m encouraged to choose method one, even though it will require the most work.\nWhen there are these kinds of incompatibilities between APIs, it\u0026rsquo;s normally best to follow the design of one of the APIs fairly closely. So, we can write Vulkan code in a DirectX-like way, or we can write DirectX code in a Vulkan like way. In this case, the Vulkan method is more expressive and is a clear superset of the DirectX behaviour. Ignoring RenderPasses would make targeting PowerVR-style hardware (at a late date) much more difficult.\nBut it seems like it will be a little difficult to make the transition to RenderPasses. It\u0026rsquo;s probably going to require significant changes in the SceneEngine. But it\u0026rsquo;s also a good opportunity to think about other improvements to render target management within the SceneEngine.\nDescriptors Vulkan has a slightly different way to look at binding resources and constant buffers to shaders. We have a lot of new concepts:\n Descriptor sets Descriptor set layouts  These relate most closely to the Metal::BoundUniforms object in XLE.\nThe descriptor set layout contains a list of all of the bindings of a shader (or set of shaders) and the descriptor sets contain the list of objects to actually bind.\nCurrently, the BoundUniforms objects own both descriptor sets and descriptor set layouts. But I\u0026rsquo;m not sure this is a great idea, mostly because we end up with just a 1:1 relationship (which seems redundant).\nBoth of these relate to the \u0026ldquo;binding\u0026rdquo; number set in the shader. My understanding at the moment is that the binding number should be a low number, and numbers should be used \u0026ldquo;mostly\u0026rdquo; sequentially \u0026ndash; like the register binding numbers in DirectX.\nSince XLE binds things via string names, we still need some way to associate the string name to a particular binding number. One idea is to use the hash of some string value for the binding numbers\u0026hellip; I haven\u0026rsquo;t seen any documentation to say this is a bad idea \u0026ndash; but, Nevertheless, I still suspect that this is a bad idea. Near-sequential is going to be safer.\nWe can choose 2 different designs for descriptor set layouts:\n keep different descriptor set layouts bound tightly to specific shaders ie, each shader could have it\u0026rsquo;s own layout, containing only the bindings relevant to that particular shader share descriptor set layouts broadly so layouts may contain a superset of bindings for any given shader  If we\u0026rsquo;re sharing descriptor sets, we could choose to do that on a per-technique level. That would kind of make sense. Or we could do it on a global level. That is, we could just have the single huge layout for each descriptor set binding index.\nThe descriptor set binding indices are similar to the \u0026ldquo;bound uniforms stream index\u0026rdquo; in XLE. In particular, it makes sense to have a single shared layout for the global binding stream index. This would also move us closer to a \u0026ldquo;bindless\u0026rdquo; approach to these things.\nHowever, that introduces the problem of mapping between our current string binding names at the binding indices used by the shader. Since no single shader contains all bindings, the normal method of using reflection doesn\u0026rsquo;t work.\nIn addition to the layout management issues, there are also issues managing the descriptor sets. Some descriptor sets have temporary data (ie, we expect it might change in a future frame). But some descriptor sets contain only static data. For example, the texture bindings of a model are set at load time and remain constant.\nFor static descriptor set data, we would ideally write this once, and just reuse it every frame. That seems possible by adapting the SharedStateSet interfaces. But it would require a few changes.\nFor temporary descriptor set data, I suspect that the simple approach of just writing every frame and then throwing it away might be fine, and it would require fewer changes.\nAlso, we want to be able to attach the \u0026ldquo;set\u0026rdquo; layout qualifier to the GLSL code. However, there is no equivalent to this in HLSL, so no way to get it through the HLSL -\u0026gt; GLSL approach. Maybe we can do some hacks via register bindings, but it might be awkward.\nWe might possibly need to build a global table of bindings \u0026ndash; this table could assign the string name to binding ids, and also assign a \u0026ldquo;set\u0026rdquo; qualifier.\nAt the moment it\u0026rsquo;s not perfectly clear what\u0026rsquo;s the best approach to dealing with descriptor sets. It might take some experimentation.\nPipelines Vulkan has some support for precalculating and reusing what it calls \u0026ldquo;pipelines.\u0026rdquo; These are a combination of all shader stages, the descriptor layouts and the input layouts. But they also have some render state information mixed in.\nIt\u0026rsquo;s another big departure from DirectX, because it pre-binds these objects together. Again, it\u0026rsquo;s a superset of DirectX behaviour.\nIt feels like we may have to dynamically create some pipelines during the frame with XLE. That said, for models (which should be the majority of geometry) we should be able to leverage the SharedStateSet code, so that these can be reused over multiple frames.\nThis would mean that some draw calls would use precreated pipelines, while others would use dynamically created pipelines. I think this might be the best balance between practicality and efficiency\u0026hellip;\nShader constant push buffers In Vulkan, we have a few new approaches for uploading shader constants. We still have uniform buffers (but we have a lower level interface for them). We also have \u0026ldquo;push buffers\u0026rdquo; which presumably use the command buffer to pass frame-by-frame data to the constant registers.\nPush buffers will be great for data that changes every frame. Uniform buffers are best for data that is retained for multiple frames, or reused in many draw calls.\nFortunately, we also have the ConstantBufferPacket concept in Metal::BoundUniforms. This could be extended to be used with push constants. But push constants seem like they also require shader changes. That is, the maybe the shader must know if it\u0026rsquo;s going to receive those constants via push constants or a uniform buffer.\nIt might be a good time to rethink how to handle the LocalTransform constants for model rendering. Often, the local transform will be constant over the model\u0026rsquo;s lifetime. In these cases, we might want to store the transform in a retained uniform buffer.\nBut another alternative is to just use push constants. This will be required in the case of animated transforms. And maybe it could be reasonable efficient? Anyway, it might be interesting to think about.\nImage layouts Vulkan has a new \u0026ldquo;image layout\u0026rdquo; concept. Each image object has a specific layout and we set these asynchronously using a command on the command buffer.\nNormally, we only care about layout while we\u0026rsquo;re creating the image or initializing it with data. But it\u0026rsquo;s sort of architecturally awkward, because we create the object synchronously using a VkDevice call, but we don\u0026rsquo;t set the layout until we first use it in a command buffer. That\u0026rsquo;s a problem, because we usually don\u0026rsquo;t know when a use is the first time.\nAll our images should be initialized via buffer uploads. This might make easier to solve this efficiently, because we can add in a \u0026ldquo;layout initialization\u0026rdquo; command list that always get executed as part of the buffer uploads update. In this way, buffer uploads will manage layouts, and other code can just ignore it. But it is going to create some difficulties for Transaction_Immediate resources.\nResource deletion XLE is often lazy about retaining references to objects. In general, we\u0026rsquo;re taking advantage of the way DirectX11 keeps objects alive while they\u0026rsquo;re required by the device.\nOften we allocate some object, use it for one frame, and then destroy (expecting the memory to be freed up when the GPU is finished with that frame). While might not be perfectly efficient, it is very convenient.\nVulkan doesn\u0026rsquo;t automatically track the GPUs progress when we free an object. So we need to know when it is currently referenced by a command buffer, and when it might be used in the future (or even if it\u0026rsquo;s currently being used by the GPU). This is complicated by cases with secondary command buffers (such as those used by the buffer uploads).\nSo we need some new concepts for tracking GPU progress, and destroying objects when they are no longer needed.\nLikewise, some objects don\u0026rsquo;t need to be destroyed, but they might need to be overwritten. This happens in systems that are built for streaming. These cases are very similar \u0026ndash; \u0026ldquo;safe to delete\u0026rdquo; is often the same as \u0026ldquo;safe to overwrite.\u0026rdquo;\nThe majority of objects can follow a simple path that just tracks a GPU frame index. After the GPU finishes a frame, then resources associated with that frame can be destroyed.\nIn some cases, the destroy might actually be a \u0026ldquo;reset.\u0026rdquo; For example, if we have a number of pools of temporary descriptor sets, we must track the GPU progress to know when it\u0026rsquo;s safe to reset and reuse a pool.\nSome cases might be more complicated \u0026ndash; such as if we have resources tied to secondary command buffers that are retained over several frames. In this case, the resources must be maintained until after the command buffer is no longer used used and destroyed.\nConclusion Many of the Vulkan concepts are familiar to me from previous projects\u0026hellip; So to some extent, many ideas from Vulkan are already built into XLE. But at the same time it\u0026rsquo;s going to require some refactoring and improvements of the current code before we can use Vulkan very efficiently.\nWhile I\u0026rsquo;ve been working on Vulkan, I\u0026rsquo;ve also been improving the RenderCore::Metal interface. There are aspects of this interface that I\u0026rsquo;ve always meant to improve \u0026ndash; but I\u0026rsquo;ve never had the chance before. When working just with DirectX, it didn\u0026rsquo;t really need to be very polished; but adding Vulkan to the mix changes that.\nAnyway, it feels like Vulkan is going to be around for awhile, the prototype so far has shown where the most important difficulties are going to be!\n","id":11,"section":"posts","summary":"I\u0026rsquo;m making more progress on the Vulkan prototype. Now the SceneEngine compiles, BufferUploads works, and it\u0026rsquo;s now possible to render basic geometry. Many features aren\u0026rsquo;t supported. But the core rendering features (such as binding textures, compiling shaders, pushing geometry through the pipeline) have an early implementation.\nSo, while a working Vulkan version is now close, I have a much better idea of what major hurdles there would be to get a really streamlined and efficient Vulkan implementation!","tags":["Vulkan","cross platform"],"title":"Vulkan prototype - the big issues","uri":"https://djewsbury.github.io/2016/04/vulkanbigissues/","year":"2016"},{"content":"After 1 week of playing around with Vulkan, here are 4 tips I\u0026rsquo;ve learned. These are maybe not very obvious (some aren\u0026rsquo;t clearly documented), but I think they\u0026rsquo;re important to know.\n1. Use buffers as staging areas to initialize images The crux of this is we need to use a function, vkCmdCopyBufferToImage, to implement staging images with Vulkan. It\u0026rsquo;s almost impossible to do any graphics work with Vulkan without doing this \u0026ndash; but a first glance it might seem a bit counter-intuitive. First a bit of a background.\nVulkan images have a \u0026ldquo;tiling\u0026rdquo; property, which can be either \u0026ldquo;linear\u0026rdquo; or \u0026ldquo;optimal.\u0026rdquo; This is property relates to how pixels are arranged in memory within a single mip level.\nIn linear tiling, texels are arranged in row by row, column by column. So a texel\u0026rsquo;s linear address follows this pattern:\naddress = (y*rowPitch) + x*bitsPerPixel/8\r When we have images on disk, we usually expect them to have this tiling.\nHowever, this has a big problem. Texels in the same row will be near each other in memory. But texels in the same column will be quite separate in memory.\nGiven that images can be mapped arbitrarily on the final 2D triangles, we can\u0026rsquo;t control the order in which the GPU will access the texels. So this is big issue for the memory cache.\nSo all GPUs are able to rearrange the texels into a \u0026ldquo;swizzled\u0026rdquo; order that tries to guarantee that nearby texels in 2D (or 3D) image space will be nearby in linear memory. This is called \u0026ldquo;optimal\u0026rdquo; tiling in Vulkan.\nOptimal tiling really is going to be much faster in almost all cases. Some GPUs have limited support for reading linear iamges in shaders \u0026ndash; but this will rarely be a good idea. The existential advantage of linear tiling is that it is easy to program the CPU code that reads and writes image memory. Also, optimal tiling isn\u0026rsquo;t defined as any particular pattern \u0026ndash; it\u0026rsquo;s vendor specific. I have a feeling that it might actually the same pattern for all hardware (that is, hierarchically packing into 2x2 or 2x2x2 blocks), but I don\u0026rsquo;t know that for sure.\nBecause optimal tiling isn\u0026rsquo;t defined, we can\u0026rsquo;t pass the initial image data in that tiling. We must initialize the image data in linear tiling.\nThis is why staging images are required. We initialize a linear staging image in \u0026ldquo;host visible\u0026rdquo; memory first. Then we issue a command, and the GPU will copy from the staging image into the final image, swizzling into optimal layout as it goes. It\u0026rsquo;s been like this for many, many years \u0026ndash; but to PC programmers, it may seem new because DirectX hides this behaviour.\nSo, I said \u0026ldquo;staging image\u0026rdquo; above. But what I really meant to say was \u0026ldquo;staging buffer.\u0026rdquo; Here\u0026rsquo;s the problem \u0026ndash; even though there are \u0026ldquo;linear\u0026rdquo; tiling images that can be positioned in \u0026ldquo;host visible\u0026rdquo; memory, these can only have a single mip level and a single array layer. So how do we initialize textures with multiple mip levels and multiple array layers?\nIt seems that we\u0026rsquo;re intended to use a VkBuffer for this. That\u0026rsquo;s the trick \u0026ndash; and every Vulkan programmer needs to know it :).\nIt may seem strange to do this, but it does make sense\u0026hellip; In Vulkan, a VkImage contains functionality for driving the \u0026ldquo;sample\u0026rdquo; shader operations. All these concepts of tiling, mip levels, pixel formats, etc, are all related to shader sampling. But a staging texture will never be sampled by a shader. So, in effect, the VkImage concepts are redundant. We just want an area of video memory with no special restrictions (and VkBuffer fits that requirement better).\nIt does mean we have to implement our own functions for arranging mip levels and array layers within the buffer space. In one sense, this means writing a custom implementation of vkGetImageSubresourceLayout.\nSo, first we create the staging buffer, and initialize the device memory. Then we can issue the copy command with vkCmdCopyBufferToImage. This allows us to copy into the image subresources from arbitrary memory within the buffer. I find the interface for this function a little awkward (and there are some limitations) but it\u0026rsquo;s not to bad.\nThere\u0026rsquo;s a thread on nvidia\u0026rsquo;s site that seems to verify that this was intended: https://devtalk.nvidia.com/default/topic/926085/texture-memory-management/.\nAnyway, it\u0026rsquo;s an important trick. Because (as far as I can tell) this is the only way to create textures with multiple mip levels or multiple array layers in Vulkan!\n2. Write custom reflection for SPIR-V bytecode The SPIR-V bytecode is a very simple format, and it\u0026rsquo;s also an open standard. The bytecode also contains many of the variable and type names from the original source code (which are called decorations). These aren\u0026rsquo;t used during execution \u0026ndash; but they are useful for debugging and reflection.\nI\u0026rsquo;ve implemented a little bit of code to read SPIR-V bytecode and extract layout bindings and other useful information. This is similar to the ID3DShaderReflection interface in DirectX. But since it\u0026rsquo;s custom coded, it\u0026rsquo;s crazy efficient.\nI recommend checking out the following files in the Vulkan SDK for a starting point for working with SPIR-V byte code:\n glslang/SPIRV/disassemble.cpp glslang/SPIRV/doc.cpp  3. Set the VK_LAYER_PATH variable! The Vulkan SDK has a bunch of debugging \u0026ldquo;layers\u0026rdquo; built in. These are really useful!\nBut to get them working, you need to set the VK_LAYER_PATH variable to your sdk \u0026ldquo;bin\u0026rdquo; directory (eg C:/VulkanSDK/1.0.5.0/Bin), and maybe do a bunch of other things. This may not be documented anywhere\u0026hellip;?!\nWhen things go wrong with Vulkan, usually you\u0026rsquo;ll just get a program crash (that is, if the video card driver doesn\u0026rsquo;t crash and bluescreen). You won\u0026rsquo;t get much debugging information normally. To get error and warning messages, you\u0026rsquo;ll need the layers installed.\nIf you read the Vulkan specs document, you\u0026rsquo;ll notice that all functions have a set of rules about input parameters \u0026ndash; written in contract style. These are the kinds of things the layers check for. But they also check for threading access and other frequent usage problems.\nIt\u0026rsquo;s really helpful, believe me! Get it working early on. Play with the \u0026ldquo;enable_validation_with_callback\u0026rdquo; sample until it\u0026rsquo;s working.\n4. Download the source code for RenderDoc Compile a debug build of RenderDoc for yourself, and run it in the debugger. You\u0026rsquo;ll get asserts and debugging information in those cases where your code is so screwy that even RenderDoc can\u0026rsquo;t handle it.\nJust go to renderdoc.org \u0026ndash; it\u0026rsquo;ll redirect to github!\n","id":12,"section":"posts","summary":"After 1 week of playing around with Vulkan, here are 4 tips I\u0026rsquo;ve learned. These are maybe not very obvious (some aren\u0026rsquo;t clearly documented), but I think they\u0026rsquo;re important to know.\n1. Use buffers as staging areas to initialize images The crux of this is we need to use a function, vkCmdCopyBufferToImage, to implement staging images with Vulkan. It\u0026rsquo;s almost impossible to do any graphics work with Vulkan without doing this \u0026ndash; but a first glance it might seem a bit counter-intuitive.","tags":["Vulkan","cross platform"],"title":"Important Vulkan tips","uri":"https://djewsbury.github.io/2016/04/vulkantips/","year":"2016"},{"content":"So, the Vulkan prototype is progress\u0026hellip; But I\u0026rsquo;m running into many problems working with the drivers and the associated tools. Here\u0026rsquo;s some examples of the problems I\u0026rsquo;m finding.\nRenderDoc crashing RenderDoc is the best tool for debugging Vulkan at the moment\u0026hellip; But every time I tried to capture a log, it just crashed! The crash report didn\u0026rsquo;t contain any useful information. All I could do was guess at the problem.\nFortunately, RenderDoc has one really great feature\u0026hellip; It\u0026rsquo;s open-source! So, I just downloaded the code and ran from Visual Studio (it compiled first time).\nRenderDoc is still very unstable for Vulkan. But now that I have my own compiled version, that\u0026rsquo;s not really a big issue. I can just debug any crashes and make changes as required. All of the other GPU debugging tools I\u0026rsquo;ve ever used (PIX, console tools, GPA, nsight, etc) have been unstable, as well. But they were all closed source. So whenever I got an error, my only choices were to either use another debugging, or guess at the problem. With this in mind, (open-source + very unstable) is probably better than (closed-source + mostly stable).\nMy issue was related to \u0026ldquo;binding\u0026rdquo; decorations in SPIR-V. RenderDoc requires that all resources have binding decorations. I found this was also an issue for run-time XLE code. Even though the GLSL compiler is capable of generating SPIR-V code without binding decorations, it seems like all practical uses require them.\nMy shaders weren\u0026rsquo;t getting \u0026ldquo;bindings\u0026rdquo; for any resources, and this was the cause of RenderDoc\u0026rsquo;s crashes!\nHLSL cross compiler and \u0026ldquo;bindings\u0026rdquo; Part of the issue is related to the HLSL cross compiler. In some cases, the cross compiler can attach \u0026ldquo;location\u0026rdquo; values, but it never attaches \u0026ldquo;binding\u0026rdquo; values for textures or constant buffers.\nFortunately, the HLSL cross compiler is also open-source\u0026hellip; So I can create a fork with the modifications I need. That seems to be required in this case. I could try to attach the binding information later in the pipeline (eg, by modifying the output GLSL, or by inserting instructions into the SPIR-V bytecode)\u0026hellip; But changing and improving the cross compiler seems like the best option.\nWe ideally also want to specify the \u0026ldquo;descriptor set\u0026rdquo; in the GLSL code. Unfortunately, HLSL 5 doesn\u0026rsquo;t have an equivalent concept. That is going to require some more effort.\nCross compiler incorrect translation The next problem was some HLSL instructions were generating incorrect GLSL code. In particular, I was using an expression like \u0026ldquo;float4(localPosition.xy, 0, 1)\u0026rdquo; to generate a 4D vector. But this was being treated as \u0026ldquo;float4(localPosition.xy, 0, 0)\u0026rdquo;.\nThere are a number of unknown instructions that are incorrectly translated by the HLSL cross compiler\u0026hellip; So it looks like I\u0026rsquo;ll need to do some work improving the code.\nVulkan validation layer It took me awhile to figure out how to enable the Vulkan validation layer. This is really important for finding usage errors! But it\u0026rsquo;s not really documented and it\u0026rsquo;s a very unclear how to get it working.\nEventually, I found out I needed to set the VK_LAYER_PATH environment variable. It seems like this should be set by the SDK installer, but maybe it was an oversight.\nAnyway, I also needed to use the VK_EXT_DEBUG_REPORT_EXTENSION_NAME extension to install a message handler. It looks like there are other ways to use the validation layers. But I still don\u0026rsquo;t know how to get them working. For now, I\u0026rsquo;m just catching errors and warnings and pushing them into the XLE logging system.\nBinding samplers and images together HLSL separates samplers and textures, but Vulkan seems to prefer to combine them together into one. This is going to cause a bit of any issue with the way XLE shaders use samplers\u0026hellip; Probably to start with, I just use a single point filtering sampler for all textures.\nvkCmdBindDescriptorSets ending command buffers For some reason, vkCmdBindDescriptorSets is silently \u0026ldquo;ending\u0026rdquo; the command buffer. It not clear why \u0026ndash; there must be some error. But I haven\u0026rsquo;t found it yet. Even the validation layers aren\u0026rsquo;t much help in this case.\n\u0026ldquo;Image layouts\u0026rdquo; concept confusing There seems to be some confusion in the API over a concept called \u0026ldquo;image layouts.\u0026rdquo; This appears to be related to how images are stored in memory. The exact detail are very implementation specific and opaque. We need to instruct the GPU (rather than the CPU side of the API) to change the layout of an image. So changing the layout involves appending commands to the command buffer.\nBut there are multiple different ways to do this\u0026hellip; It\u0026rsquo;s not really clear how to best handle this currently. The samples have their own way of dealing with image layouts. But that doesn\u0026rsquo;t look like the most optimal approach \u0026ndash; and anyway, it\u0026rsquo;s architecturally awkward (because it mixes unsynchronised initialisation functions with synchronised command buffer functions).\nSo, I\u0026rsquo;ll need to do some experimentation to find the best way!\nNow rendering geometry! But, I\u0026rsquo;ve finally got some basic geometry rendering! It\u0026rsquo;s just a few 2D triangles, but it\u0026rsquo;s something!\nIn short, there are a lot of problems and difficulties with using Vulkan currently.\nI think I\u0026rsquo;ve found problems with every step in the chain so far. But many of the tools and library are open-source, and that is helping a lot. If (for example) RenderDoc had been closed source, I would just be making guesses now, and probably not getting very far.\nIt would be nice if everything was stable and polished\u0026hellip; But for now, as long as I can identify the particular cause of each problem, I think I can make educated decisions about how to navigate through the minefield.\n","id":13,"section":"posts","summary":"So, the Vulkan prototype is progress\u0026hellip; But I\u0026rsquo;m running into many problems working with the drivers and the associated tools. Here\u0026rsquo;s some examples of the problems I\u0026rsquo;m finding.\nRenderDoc crashing RenderDoc is the best tool for debugging Vulkan at the moment\u0026hellip; But every time I tried to capture a log, it just crashed! The crash report didn\u0026rsquo;t contain any useful information. All I could do was guess at the problem.","tags":["Vulkan","cross platform","SPIR-V"],"title":"Vulkan prototype slowly progressing","uri":"https://djewsbury.github.io/2016/04/vulkanprototypeslowprogress/","year":"2016"},{"content":"As part of the Vulkan prototype, I\u0026rsquo;m experimenting with compiling HLSL shaders to SPIR-V.\nThe Vulkan API doesn\u0026rsquo;t have a high level shader language attached. Instead, it works with a new intermediate bytecode format, called SPIR-V. This works with the LLVM compiler system, so in theory we plug in different front ends to allow various high level languages to compile to SPIR-V.\nThat sounds great for the future\u0026hellip; But right now, there doesn\u0026rsquo;t seem to be a great path for generating the SPIR-V bytecode.\nAll of the XLE shaders are HLSL\u0026hellip; So how do we use them with Vulkan? Let\u0026rsquo;s consider the options.\nPreprocessor framework One option is to convert the shader code to GLSL using a preprocessor framework. HLSL and GLSL are closely related\u0026hellip; But they vary significantly in the way the shader interface is declared (eg, shader resources, vertex elements, built in values, etc).\nWe can get around this by using a system of preprocessor macros and complex #includes. We would end up with native HLSL and GLSL code that does the same thing. The core shader code that does the math and lighting (etc) should be fairly compatible between languages\u0026hellip; It\u0026rsquo;s just the interface that is an issue.\nHowever this kind of approach it\u0026rsquo;s a bit awkward, and difficult maintain in the long term. And it might mean dropping support for some of the more unusual D3D features I\u0026rsquo;m using (such as for dynamic linking).\nAlso, it looks like GLSL might not be around a very long time. It could possibly go the way of OpenGL in the medium term. So it doesn\u0026rsquo;t make sense to invest a lot of time into GLSL, just to have to be replaced with something else later.\nCross compile Another option is to try to convert the HLSL output to SPIR-V by way of a cross compiler.\nThere is an interesting project here https://github.com/James-Jones/HLSLCrossCompiler. This will take as input HLSL bytecode, and output GLSL high level shader code.\nHLSL bytecode is a much simpler than HLSL source (given that it\u0026rsquo;s mostly assembler like instructions). So this approach should be more maintainable.\nThe issue here is there is no standard equivalent bytecode form of GLSL. The output from the cross compile is just high level GLSL code.\nOnce we have the GLSL code, we can generate SPIR-V using the GLSL pipeline in the Vulkan SDK.\nCross compile problems The process for compiling a shader becomes quite long:\n compile HLSL to HLSL bytecode using D3DCompiler_47.dll (ie, just like D3D11) translate HLSL bytecode to GLSL parse GLSL to AST translate GLSL AST to SPIR-V  The big problem here is that each step adds it\u0026rsquo;s own restrictions. So in the end we up with the sum of all those restrictions.\nAnother issue is that a few of these steps have know bugs and version support issues. The HLSLCrossCompiler seems to translate several instruction incorrectly. And the GLSL AST to SPIR-V seems to currently only support a subset of GLSL.\nFor example, the [earlydepthstencil] annotations in HLSL get correctly converted into GLSL \u0026ndash; but then the SPIR-V translator doesn\u0026rsquo;t support them!\nAlso, GLSL to SPIR-V doesn\u0026rsquo;t fully support GLSL version 4 shaders yet\u0026hellip; So I\u0026rsquo;m using GLSL 3.3. But that may not be able to support all of the features I\u0026rsquo;m using in HLSL.\nFortunately, both the HLSL cross compile and the GLSL to SPIR-V compiler are open-source\u0026hellip; So there is room to make fixes and improvements if necessary. And it means those projects will be more open about what is working, and what isn\u0026rsquo;t.\nPrototype results It seems that the cross-compile approach is actually going to be best, despite the problems. So, I\u0026rsquo;ve got an early version of this process working in the \u0026ldquo;experimental\u0026rdquo; branch.\nIn this branch, there is a new implementation of RenderCore::ShaderService::ILowLevelCompiler that does HLSL -\u0026gt; SPIR-V translation.\nIt requires some libraries from the Vulkan SDK currently\u0026hellip; I\u0026rsquo;m not sure how I will incorporate those into linking process (perhaps it will mean a new VULKAN_SDK environment variable).\nI\u0026rsquo;ve only tested a few shaders so far\u0026hellip;. But it works! It actually does compile, and we end up with SPIR-V at the end. Wow, pretty incredible.\nI\u0026rsquo;ve added a new \u0026ldquo;VULKAN=1\u0026rdquo; shader define. This will be probably be required for enabling and disabling certain shader features for this pipeline.\nThe next step will be working on how to best extract reflection information from the shaders. We\u0026rsquo;ve actually got many ways to do that now\u0026hellip; But I want to check to make sure we can still get the information we need out.\n","id":14,"section":"posts","summary":"As part of the Vulkan prototype, I\u0026rsquo;m experimenting with compiling HLSL shaders to SPIR-V.\nThe Vulkan API doesn\u0026rsquo;t have a high level shader language attached. Instead, it works with a new intermediate bytecode format, called SPIR-V. This works with the LLVM compiler system, so in theory we plug in different front ends to allow various high level languages to compile to SPIR-V.\nThat sounds great for the future\u0026hellip; But right now, there doesn\u0026rsquo;t seem to be a great path for generating the SPIR-V bytecode.","tags":["Vulkan","cross platform","shaders","HLSL","SPIR-V"],"title":"HLSL shader prototype with Vulkan","uri":"https://djewsbury.github.io/2016/04/vulkanshaderprototype/","year":"2016"},{"content":"So, it\u0026rsquo;s a simple story \u0026ndash; boy meets API, yada, yada, yada\u0026hellip;\nI\u0026rsquo;ve started to build some initial experiments with the Vulkan API. Version 1.0 of the API was just released \u0026ndash; and there\u0026rsquo;s an SDK from Valve (called LunarG) around.\nInitial impressions My first impressions are very positive! Many of the design ideals and structures of the API are familiar from my days working with consoles (particularly the Sony consoles). This type of API has not been available on open platforms (like Windows) before \u0026ndash; so for people who don\u0026rsquo;t have experience with consoles, it might give an idea of what it\u0026rsquo;s like.\nI\u0026rsquo;m also really happy how much of the surrounding resources have been made open-source\u0026hellip; The samples, shader compilation tool-chain, etc\u0026hellip; It\u0026rsquo;s all very effective use of github.\nKhronos have drawn attention to how much input they\u0026rsquo;ve gotten from the game engine development community. And it shows in the results. This is the kind of API that an experienced engine developer wants to see.\nI think it\u0026rsquo;s also a model that is much more viable for cross platform development than anything we\u0026rsquo;ve seen before. OpenGL had a lot of problems, and DirectX was so tied to the Windows platform. But this feels like something that is truly viable across many platforms (including low end and high end).\nI\u0026rsquo;m really impressed with how a third party group has managed to build an API that balances the needs of engine developers with the needs of hardware designers. I think Khronos has really shown how this kind of thing should be done \u0026ndash; and it seems like a good model for other APIs (sound, physics hardware, etc).\nLong term viability Vulkan feels like an API that could stick around for awhile. OpenGL has been on it\u0026rsquo;s last legs for a long time\u0026hellip; And DirectX always needs constant refreshes to survive. But Vulkan feels like it will be here for awhile. Due to it\u0026rsquo;s cross-platform and long term viability, it\u0026rsquo;s really undermined DirectX and Apple\u0026rsquo;s Metal.\nI felt the same about C++11, when I started using it. The new C++14 is a huge step forward from the old C++98 days. Many of the design patterns we C++ programmers always wanted to use are not much more viable in C++14. Stroustrup said that C++14 \u0026ldquo;completes\u0026rdquo; C++11 \u0026ndash; but really, I think it \u0026ldquo;completes\u0026rdquo; C++ as a whole.\nC++17, C++20 \u0026ndash; these are going to be even better, also. And Vulkan will get better over time. But my gut feeling is that code written in C++14 and Vulkan 1.0 is likely to be viable for a very long time.\nBy comparison, code written in OpenGL, Apple\u0026rsquo;s Metal (or, help us, DirectX9) is going to feel stale pretty quickly.\nThe unfinished part of the equation is the shader language. Vulkan uses SPIR-V as an intermediate language\u0026hellip; And that\u0026rsquo;s great. But it\u0026rsquo;s not clear yet what high level language should be used. The current SDK uses GLSL \u0026ndash; but maybe we could do better? Perhaps C++ is an option here\u0026hellip; But maybe we would be better off with some next iteration forward from HLSL and GLSL.\nBut it\u0026rsquo;s not easy! Clearly, Vulkan is not intended as a beginner\u0026rsquo;s API. DirectX always tried to find a balance between power and accessibility for it\u0026rsquo;s API. But Vulkan is designed by and for experienced developers.\nThis is clearest in the threading and reference counting approaches. Vulkan is mostly leaving it up to the engine developer to handle these things. That\u0026rsquo;s great when you want to write a really efficient engine\u0026hellip; But it\u0026rsquo;s going to be a major hassle for first timers.\nXLE code You can see the Vulkan code in the \u0026ldquo;experimental\u0026rdquo; branch.\nI will be adapting and improving platform abstraction layer in XLE as I plug in Vulkan features. Fortunately, it seems that the XLE architecture should mostly work well with Vulkan.\nI will also try to use HLSL with Vulkan by cross-compiling first to GLSL and then to SPIR-V.\n","id":15,"section":"posts","summary":"So, it\u0026rsquo;s a simple story \u0026ndash; boy meets API, yada, yada, yada\u0026hellip;\nI\u0026rsquo;ve started to build some initial experiments with the Vulkan API. Version 1.0 of the API was just released \u0026ndash; and there\u0026rsquo;s an SDK from Valve (called LunarG) around.\nInitial impressions My first impressions are very positive! Many of the design ideals and structures of the API are familiar from my days working with consoles (particularly the Sony consoles).","tags":["Vulkan","rendering","API","cross platform"],"title":"Starting to experiment with Vulkan","uri":"https://djewsbury.github.io/2016/04/vulkan/","year":"2016"},{"content":"There are has been a lot research on order independent transparency recently. There are a few screenshots comparing the following methods:\n Sorted \u0026ndash; this mode sorts back-to-front per fragment. It\u0026rsquo;s very expensive, but serves as a reference. Stochastic \u0026ndash; as per nVidia\u0026rsquo;s Stochastic Transparency research. This uses MSAA hardware to estimate the optical depth covering a given sample. Depth weighted \u0026ndash; as per nVidia\u0026rsquo;s other white paper, \u0026ldquo;A Phenomenological Scattering Model for Order-Independent Transparency.\u0026rdquo; This is very cheap, and uses a fixed equation based on fragment depth to weight samples.  Sorted  Stochastic  Depth Weighted  Unordered  You can see the artifacts we get with stochastic modes clearly here. Also, you can see that the depth weighted mode is holding up well in some areas of the image, but produces weird results in other areas.\nHere\u0026rsquo;s a scene more similar to what might appear in a game:\nSorted  Stochastic  Depth Weighted  Here, the artifacts in the stochastic method are less obvious. Also, the volume and shape of the geometry is preserved well.\nThe depth weighted version looks ok when there are few layers of transparency. But as soon as we pile on a few layers, it quickly turns to mush. The shape of the trees has become lost, and we end up with a trees merging into other trees.\nWhen rendering trees like this, we need to calculate the lighting on every layer (whereas, with alpha tested geometry we just calculate it once per pixel). This can become extremely expensive. One of the advantages of the stochastic method is we can estimate the optical depth in front of a layer, and fall back to cheaper lighting for layers that are heavily occluded.\nLikewise, in all methods there some room to move some aspects of the lighting from per-layer to per-pixel (for example, the Phenomenological Scattering Model paper does the refraction lookup once per pixel regardless of the number of layers).\n","id":16,"section":"posts","summary":"There are has been a lot research on order independent transparency recently. There are a few screenshots comparing the following methods:\n Sorted \u0026ndash; this mode sorts back-to-front per fragment. It\u0026rsquo;s very expensive, but serves as a reference. Stochastic \u0026ndash; as per nVidia\u0026rsquo;s Stochastic Transparency research. This uses MSAA hardware to estimate the optical depth covering a given sample. Depth weighted \u0026ndash; as per nVidia\u0026rsquo;s other white paper, \u0026ldquo;A Phenomenological Scattering Model for Order-Independent Transparency.","tags":["OITrans"],"title":"Comparing different methods for order independent transparency","uri":"https://djewsbury.github.io/2016/03/compareoitrans/","year":"2016"},{"content":"Here are a few screenshots of environment rendering in XLE. I don\u0026rsquo;t have a lot of art I can take screenshots of, so I can\u0026rsquo;t show a very polished scene\u0026hellip; But you can see some of the rendering features.\nLook for:\n shallow water surface animation order independent blending for foliage volumetric fog (\u0026amp; related effects) dynamic imposters for distant trees high res terrain geometry terrain decoration spawn \u0026ldquo;contact hardening\u0026rdquo; shadows infinite terrain shadows  (see older screenshots here)\n       Here\u0026rsquo;s a shot of a turbulent ocean (though the indoor environment reflections are weird!). The ocean animation is dynamically calculated on the GPU, and can simulate both small waves on a calm ocean surface and turbulant large waves.  (some of the sky textures in these screenshots are from sIBL Archive)\n","id":17,"section":"posts","summary":"Here are a few screenshots of environment rendering in XLE. I don\u0026rsquo;t have a lot of art I can take screenshots of, so I can\u0026rsquo;t show a very polished scene\u0026hellip; But you can see some of the rendering features.\nLook for:\n shallow water surface animation order independent blending for foliage volumetric fog (\u0026amp; related effects) dynamic imposters for distant trees high res terrain geometry terrain decoration spawn \u0026ldquo;contact hardening\u0026rdquo; shadows infinite terrain shadows  (see older screenshots here)","tags":["Screenshots"],"title":"Environment Rendering Screenshots","uri":"https://djewsbury.github.io/2016/03/envshots/","year":"2016"},{"content":"I\u0026rsquo;ve attached a compiled version of the XLE tools to the Github repo.\nThis is still an early build \u0026ndash; and hasn\u0026rsquo;t be extensively tested on different hardware. So some features may not work on configurations. If you run in problems, or if you find it interesting, I recommended downloading the source and compiling for yourself.\nDownload XLE v0.04.1: XLE v0.04.1 Windows x64You will need Visual C++ Redistributable Packages for Visual Studio 2013.\nFor older versions of Windows, you will also need to install the .Net runtime.\nEdit \u0026ndash; updated to link to v0.04.1 (which fixes a problem locating the shader compiler dll on some versions of windows).\nDocumentation I\u0026rsquo;ve included a couple of \u0026ldquo;Getting Started\u0026rdquo; docs:\n Getting Started Exporting Getting Started with Terrain  These will help get you through the first few steps.\n","id":18,"section":"posts","summary":"I\u0026rsquo;ve attached a compiled version of the XLE tools to the Github repo.\nThis is still an early build \u0026ndash; and hasn\u0026rsquo;t be extensively tested on different hardware. So some features may not work on configurations. If you run in problems, or if you find it interesting, I recommended downloading the source and compiling for yourself.\nDownload XLE v0.04.1: XLE v0.04.1 Windows x64You will need Visual C++ Redistributable Packages for Visual Studio 2013.","tags":["Releases"],"title":"Release v0.04.0","uri":"https://djewsbury.github.io/2016/03/release0040/","year":"2016"},{"content":"Along with the tools improvements, I\u0026rsquo;ve added a few features to make the \u0026ldquo;Environment\u0026rdquo; sample a little easier to use.\nExporting to the Environment sample  Start a new world Find the \u0026ldquo;palette\u0026rdquo; window and drag these objects into the main viewport:   CharacterSpawn AmbientSettings DirLight    You may want to position the character spawn and directional light using the move manipulator Save your world to some location (with \u0026ldquo;File/Save As\u0026rdquo;). The next step will create an export relative to this location. Do an \u0026ldquo;Export to game\u0026rdquo;:   in the level editor, the world is stored as large XML tree. But this isn\u0026rsquo;t loaded into the environment sample. Instead we have a number of small configuration files.    Now execute the environment sample:    This should pop up a separate process with the environment sample, and you will have a simple character running around.\nEnvironment sample The Environment sample is only the most basic of client apps for XLE. But it shows a few important concepts:\n use of ISceneParser to construct a scene of objects rendering models with animated data  High level scene management and an game objects framework are not features of XLE. XLE is intended to be integrated with other solutions that provide this functionality. So, if you are familiar with OpenSceneGraph or some other favourite game framework \u0026ndash; then this can be used together with the core XLE rendering and asset management functionality.\nReloading changes Note that that Environment sample will reload assets as they change. So if you keep the window open, you can just do another \u0026ldquo;Export To Game,\u0026rdquo; and the sample will load any changes. This is great for rapid development, because you can keep the Level Editor and the sample open at the same time and they work well together.\n","id":19,"section":"posts","summary":"Along with the tools improvements, I\u0026rsquo;ve added a few features to make the \u0026ldquo;Environment\u0026rdquo; sample a little easier to use.\nExporting to the Environment sample  Start a new world Find the \u0026ldquo;palette\u0026rdquo; window and drag these objects into the main viewport:   CharacterSpawn AmbientSettings DirLight    You may want to position the character spawn and directional light using the move manipulator Save your world to some location (with \u0026ldquo;File/Save As\u0026rdquo;).","tags":["Tools","Samples"],"title":"Environment Sample Streamlining","uri":"https://djewsbury.github.io/2016/03/environmentsample/","year":"2016"},{"content":"Here\u0026rsquo;s a rundown of some of the latest improvements to the tools. These are just a few additions and improvements made over about a week\u0026rsquo;s time.\nSearch and Replace for placements Find placements using complex queries:  Once found, they can be selected from the results menu (or run a replace operation).\nSometimes it\u0026rsquo;s useful to search for placements that use a specific model:  Grouping support for placements  Group together objects to make them easier to use. Support for hierarchical groups. Use the “Any Object (ignore groups)” filter to ignore groups when selecting.\nRandomize transforms   Give objects random scale and rotation values. Can be applied to many objects at the same time.\nPrefab support    Select a group of objects Save them to a separate “.prefab” file That prefab file can now be dragged into the world in multiple places  For example, a house + a tree + a fence can be combined to make home.prefab. We can then add home.prefab in multiple places in the world.\nThis is pretty cool because we can edit each instance of home.prefab separately\u0026hellip; The system will load changes to the prefab file, but also remembers what has changed in each instance!\nTerrain editing optimization and fixes In the terrain code, there’s a system called “short-circuit”. This applies to terrain changes to the streaming assets. This has been optimised. I also fixed an old problem where new streamed assets weren\u0026rsquo;t being “short-circuited” with the latest changes (which fixes issues when some terrain tiles show the wrong height values).\nTerrain fine tune  Click to place a circle on the terrain. Now you can drag up and down, and the terrain will follow the cursor.\nThis can be used to fine-tune small areas of the terrain (such as moving a single height value up and down).\nQuick access to manipulator properties  There is now control that floats over the bottom right corner of the screen. This is manipulator specific, and can be used to change frequently used settings. For the Paint Coverage manipulator, it can be used to select the material to paint with.\nAlso, the base texture materials can be given arbitrary names. This is just for convenience, so you can label a grass texture \u0026ldquo;grass\u0026rdquo; (for example).\nHidden placements  Hidden placements now appear like a transparent shadow.\n","id":20,"section":"posts","summary":"Here\u0026rsquo;s a rundown of some of the latest improvements to the tools. These are just a few additions and improvements made over about a week\u0026rsquo;s time.\nSearch and Replace for placements Find placements using complex queries:  Once found, they can be selected from the results menu (or run a replace operation).\nSometimes it\u0026rsquo;s useful to search for placements that use a specific model:  Grouping support for placements  Group together objects to make them easier to use.","tags":["Tools"],"title":"Latest Tool Features","uri":"https://djewsbury.github.io/2016/03/newtoolfeatures/","year":"2016"},{"content":"I\u0026rsquo;ve been playing with the transmitted specular implementation for IBL, and working on getting the right balance and visual impression for glass.\n Background textures from http://www.hdrlabs.com/sibl/archive.html\nIt\u0026rsquo;s curious to think about how terms have changed over the last few years. \u0026ldquo;Specular transmission\u0026rdquo; is the term I\u0026rsquo;ve been using to talk about what we might have previously called \u0026ldquo;refraction mapping\u0026rdquo;, and \u0026ldquo;specular IBL\u0026rdquo; is the new term for \u0026ldquo;reflection mapping.\u0026rdquo; The new terms show how real-time methods are now encompassing ideas previously only used in ray tracers.\nBasic method I\u0026rsquo;ve using the basic method from the GGX paper, \u0026ldquo;Microfacet Models for Refraction through Rough Surfaces\u0026rdquo; from Walter, et al. This matches our reflection implementation, of course, because the math for our reflection method is also based on that paper.\nI\u0026rsquo;ve been able to adapt the math to the split-term concept used for reflected specular IBL. It works just the same way; we have one lookup-table that tell us the brightness of the refraction, and a cubemap that is pre-blurred according to the shape of our lighting equation.\nSee the page on Improved IBL for more information that that split-term stuff.\nThe basic principles are simple. But followed blindly, the results do not quite feel right. It turns out that specular transmission is more complicated than specular reflections. I\u0026rsquo;ll talk about some of the complications involved in the implementation\u0026hellip;\nRelationship between \u0026ldquo;specular\u0026rdquo; and index of refraction It\u0026rsquo;s important to remember that there is a relationship between the \u0026ldquo;specular\u0026rdquo; material parameter and the index of refraction.\nRemember that the specular parameter is used to determine \u0026ldquo;F0\u0026rdquo; \u0026ndash; which is the brightness of specular reflections in the center of a sphere. This phenomenon of reflections being brighter around the edges of a sphere is driven by the \u0026ldquo;fresnel\u0026rdquo; effect.\nThe fresnel effect determines how light is reflected off a material, and how much gets absorbed into (or transmitted through) the material. Remember that is the ratios of the indices of refractions on either side of the boundary that determine the quantity of reflection and absorption.\nSo, there is a 1:1 relationship between F0, \u0026ldquo;specular\u0026rdquo; and the index of refraction. These 3 values all represent the same physical property.\nThe index of refraction important to us because it also determines the direction of refraction. And it\u0026rsquo;s important that this agrees with the fresnel effect calculated for specular reflection, because the refraction looks strange if it doesn\u0026rsquo;t merge into the reflection correctly.\nSo, this means that the \u0026ldquo;specular\u0026rdquo; material parameter effects both the apparent brightness of the refracted image, and the quantity of distortion applied to it.\n3 parameter lookup-table Our \u0026ldquo;brightness\u0026rdquo; lookup table now requires 3 parameters (one more than the reflection case):\n N dot V \u0026ldquo;roughness\u0026rdquo; index of refraction ratio  Also, the lookup-table can contain values greater than 1 (because refraction can focus light).\nGenerally it should be ok if specular transmission is less accurate than specular reflections, so I\u0026rsquo;ve settled on using a 64x64x32 Float16 texture. This is actually the same total size as the reflection lookup-table.\nIt should be noted, however, that subtle effects in the brightness of the refraction seem to have a big impact on visual believability. It\u0026rsquo;s important to get exactly the right\nFixed index of refraction? We can potentially optimize the solution by using a fixed index of refraction for all transmitting surfaces. An ideal number might be around 1.5 (which is glass). The fresnel effect for the transmission should match the reflection case, but apart from that, the quantity of distortion applied to the image doesn\u0026rsquo;t seem very important to visual believability. So it might be sensible to use a single fixed index of refraction for all materials, just to simplify the shader math.\nGGX transmission focusing term Most of the equations in the GGX paper can be brought across as is. However, there is one term that causes many problems.\n This term is responsible for dealing with the focusing of light. Where the refracted image appears shrunk, more light has been focused together, and so the result is brighter. Likewise a magnified image is darker.\nWalter describes the derivation of this term, and explains that is an approximation of the following equation:\n Where the straight brackets represent the determinant of a Jacobian partial derivatives matrix. The omega values are angles of the outgoing ray and the half vector.\nThe core equation is the derivative of the angle of half-vector with respect to the angle of outgoing vector. Remember that the half-vector is a direction somewhere between the incident ray and the outgoing ray. In Walter' equations, the incident ray is constant. So, we can think about this equation as measuring how much the outgoing ray moves as we shift the half-vector. If it moves a lot, we know the incident light is being scattered in a wide angle due to microfacet detail.\nThere\u0026rsquo;s another interesting approach to this same equation in Jos Stam\u0026rsquo;s \u0026ldquo;An Illumination Model for a Skin Layer Bounded by Rough Surfaces\u0026rdquo; from 2001. He mentions that he initially overlooked the importance of a term like this and found that the results looked like they were missing something. So developed his own approximation to this term.\nThere is a problem with this term, however, because the sampling that Walter is doing is not the same as what we want to do for real-time. In our sampling of this equation, we want the outgoing ray to be constant, and the incident ray will move \u0026ndash; this is the opposite of Walter\u0026rsquo;s sampling.\nSo, we need to rebuild this focusing term for our needs. We could use the same approach as Walter, and just flip the terms around\u0026hellip; But I\u0026rsquo;ve been playing with some other approaches.\nNew focusing term equation I\u0026rsquo;ve been experimenting with an algebraic solution to the derivative equation. Our algebraic solution isn\u0026rsquo;t necessarily going to be perfectly accurate, but the end result appears visually acceptable.\nA simple algebraic solution involves comparing \u0026ldquo;O dot H\u0026rdquo; to \u0026ldquo;I dot H\u0026rdquo; (ie, these are the cosines of the angles between these directions and the half-vector). These 2 dot products are related, and we can come up with a simple equation for their relationship. We can take the derivative of the angles use this as an approximation.\nSince O is constant, a change to \u0026ldquo;O dot H\u0026rdquo; represents a movement in the half-vector. So this equation tells us how much I changes relative to H whenever H changes.\nHowever, that\u0026rsquo;s not perfect. We really want to know the change in the angle between I and some fixed direction. The simplest solution is just to use O as that fixed direction.\nSo, we can adjust our equations to find the relationship between \u0026ldquo;O dot H\u0026rdquo; to \u0026ldquo;I dot O\u0026rdquo;. We can then find the derivative of that equation (with respect to the angles involved). This should give us an idea of how I changes when H changes.\nDerivative equation Unfortunately our derivative equation is very complicated. It contains many terms, and many trigonometric operations.\nIn plain text form, it\u0026rsquo;s:\n(a sin(2 x)-(sin(2 x) (a^2 cos(2 x)+1))/(sqrt(2) sqrt(cos^2(x) (a^2 cos(2 x)-a^2+2))))/sqrt(1-(sqrt(cos^2(x) (a^2 cos^2(x)-a^2+1))-a cos^2(x)+a)^2)\n\u0026hellip; Ok, it\u0026rsquo;s a crazy-looking equation.\nHowever, we\u0026rsquo;re going to be mostly just using it for pre-calculating tables, so that shouldn\u0026rsquo;t be a problem.\nThis equation is showing the right kinds of effects we expect it to \u0026ndash; we get a brightening around focused areas, and a diminishing around magnified areas.\nI\u0026rsquo;ve found that I\u0026rsquo;ve needed to multiply the results by \u0026ldquo;4\u0026rdquo; to get correctly balance. That part isn\u0026rsquo;t clear to me yet.\nAlso, Walter explains how he is using the determinant of a Jacobian matrix of partial derivatives. However, our derivative equation has just the one parameter. I haven\u0026rsquo;t quite wrapped my head around how that fits in yet.\nThat said, the results are visually satisfying. Even those there are these uncertainties, I would say that the results are good enough for our needs.\nSimplified equation We can simplify the monster of a derivative equation. By hand, I found that the following equation seems like a good approximation:\n Where a the ratio of the indices of refraction. This shows all of the same behaviours as the complex equation. For our purposes, the differences are negligible.\nTransmission blurriness For blurring the refracted image, we could simply build a new cubemap with new filtering intended for the transmission case. Indeed, this is possible. However, given the similarities between the filtering for transmission and the filtering for reflection (ie, they are both dominated by the \u0026ldquo;D\u0026rdquo; Trowbridge-Reitz distribution factor), it would be better if we could share the same texture.\nAn ideal solution would compare the blurring kernel for a given sample to some generically blurred texture, and pick around the right brightness from there. It\u0026rsquo;s almost like we need some metric that can tell us how much of this GGX-filtered blurring to apply.\nI\u0026rsquo;ve been thinking of this metric as \u0026ldquo;GGX-iness\u0026rdquo;. Well, maybe it needs a better name. But the principle should be clear \u0026ndash; how broad should the filtering be.\nIn the reflection case, we\u0026rsquo;ve been assuming that the quantity of blurring only varies with roughness. This might be a reasonable approximation in that case (the biggest issue is that the blurring can\u0026rsquo;t be directionally biased).\nBut in the refraction case, it\u0026rsquo;s not as accurate. The quantity of distortion to the image varies with the viewing angle. As a result, the quantity of blurring should also vary.\nSo, our GGX-iness = roughness equation needs to get more complex.\nFocusing term! As it turns out, the focusing term is the answer here, as well. Remember that the focusing term tells us the quantity of angular distortion. That\u0026rsquo;s exactly what we need to modify the GGX-iness equation.\nI\u0026rsquo;ve implemented a simple equation below for this calculation:  This is just based on my visual impression of what looks right (not any physical basis). It may seem strange to come this far using so much math with strong physical bases, and then finish it of with a empirical hack\u0026hellip; But I guess sometimes the simple solutions are the best\u0026hellip;!\n Importantly, on the very edges the image become very blurred. This is more obvious on cubes. Its helps the refraction blend into the reflection better, and generally just gives a more believable result.\nNew Years Day in Korea It\u0026rsquo;s the start of the Lunar New Year holiday in Korea. So, 여러분, 새해 복 많이 받으세요! Have a good weekend!\n","id":21,"section":"posts","summary":"I\u0026rsquo;ve been playing with the transmitted specular implementation for IBL, and working on getting the right balance and visual impression for glass. Background textures from http://www.hdrlabs.com/sibl/archive.html It\u0026rsquo;s curious to think about how terms have changed over the last few years. \u0026ldquo;Specular transmission\u0026rdquo; is the term I\u0026rsquo;ve been using to talk about what we might have previously called \u0026ldquo;refraction mapping\u0026rdquo;, and \u0026ldquo;specular IBL\u0026rdquo; is the new term for \u0026ldquo;reflection mapping.\u0026rdquo; The","tags":["Specular","GGX","IBL"],"title":"Transmitted specular","uri":"https://djewsbury.github.io/2016/02/transmittedspecular3/","year":"2016"},{"content":"I\u0026rsquo;ve been working on improving the accuracy of the Imaged Based Lighting (IBL) solution for XLE. This is the technology that allows us to load in a background panorama map and use it for both diffuse and specular lighting.\nThe best way to do this is by comparing our real-time result to other renderers. So, for example, I\u0026rsquo;ve been experimenting with Substance Designer (more on that later). It has nVidia\u0026rsquo;s \u0026ldquo;IRay\u0026rdquo; raytracer built-in \u0026ndash; so we can compare the non-real-time results from IRay with real-time XLE. I have other ways to do this, also \u0026ndash; for example, the shader in ToolsHelper/BRDFSlice.sh can generate textures directly comparable with Disney\u0026rsquo;s BRDFExplorer tool.\nWhile doing this (and working on the specular transmission stuff), I\u0026rsquo;ve found some interesting improvements to the IBL solution!\nConclusion This is a really long post; so I\u0026rsquo;ll start with the conclusion. I\u0026rsquo;ve made a number of improvements that make the IBL solution appear more realistic, and more similar to ray tracers.\n   \u0026ldquo;Jamie Gnome\u0026rdquo; model from here: http://www.turbosquid.com/3d-models/free-obj-mode-jamie-hyneman-gnome/789391\nI found some errors in some source material I referenced and fixed some incorrect assumptions I made. Each change is small. But when added up, they make a big difference. The IBL specular is has a lot much punch, things are jumping off the screen better.\nNow, I\u0026rsquo;ll go into detail about what\u0026rsquo;s changed.\nIBL basis I\u0026rsquo;ve been using a split-term style IBL solution, as per Brian Karis' Siggraph 2013 course (see http://blog.selfshadow.com/publications/s2013-shading-course/). As far as I know, this is the same method used in Unreal (though I haven\u0026rsquo;t checked that, there may have been some changes since 2013).\nThis splits the IBL equation into two separate equations. In one equation, we calculate the reflected light from a uniform white environment. In a sense, this is calculating our reflective a given pixel is, with equal weighting to each direction. Our solution has to make some simplifications to the BRDF (to reduce the number of variables), but we use most of it.\nMicrofacets! We can think about this on a microfacet level. Remember our specular equation is an approximate simulation of the microfacet detail of a surface.\n This is a microscopic photo by Gang Xiong, Durham University. It shows a smooth plastic surface.\nEach microscopic surface has a normal \u0026ndash; this is the microfacet normal. Remember that \u0026ldquo;D\u0026rdquo; represents how the microfacet normal are distributed, relative to the surface normal. For example, in very smooth surfaces, microfacet normals are more likely to be close to the surface normal. This is the most important part of the equation in terms of the actual shape of highlights.\nThe \u0026ldquo;G\u0026rdquo; term is for microscopic scale shadowing between different microfacets. This is why G is close to 1 for most of the range and drops off to zero around the edges. It \u0026ldquo;fights\u0026rdquo; the F term, and prevents excessive halos around the edges of things.\nAnd \u0026ldquo;F\u0026rdquo; is for the fresnel effect, which determines how much incident light is actually reflected at a given angle. In the specular equation, we\u0026rsquo;re calculating the fresnel effect on a microfacet level, not on the level of the surface normal.\nA little bit of history Most of this stuff actually dates back to 1967 with Torrance \u0026amp; Sparrow. It evolved up to around 1981 with Cook \u0026amp; Torrance (yes, same Torrance). But then nothing much happened for awhile. Then Walter, et al., rediscovered some old math in 2007 (and introduced the term, GGX). And we kind of discovered everything we were doing between 1981 and 2007 was a little bit wrong.\nMore recently, Brent Burley and Disney found a novel way to visualize multi-parameter BRDFs in a 2D image. They used this to compare our equations to real world surfaces. And so now we have a clearer idea of what\u0026rsquo;s really working and not working.\nAnyway, that\u0026rsquo;s why specular highlights in PS4 games feel much \u0026ldquo;brighter\u0026rdquo; than previous generations. The shape of the highlight greatly affects the impression of brightness.\nThere\u0026rsquo;s a great picture here:  The \u0026ldquo;GGX\u0026rdquo; dot is not any brighter\u0026hellip; But it appears to glow because of the shape of the falloff.\nSecond split term part Anyway, the second part of the split term equation is the environment texture itself. In XLE, this is a cubemap.\nEach microfacet will reflect a slightly different part of the image. And our specular equation (D, G \u0026amp; F) effect the brightness of that particular reflection.\nHowever, in this approximation, we only know the average effect of all microfacets, and we can only afford to take a single sample of this cubemap. So, we want to pre-blur the cubemap to most closely approximate the results if we had sampled every microfacet separately.\nIn XLE, we can use the cvar \u0026ldquo;cv.IBLRef = true\u0026rdquo; to enable a \u0026ldquo;reference\u0026rdquo; specular equation that samples many evenly distributed microfacets. We want to try to match that.\nSampling microfacets Since our reflection simulation is based on the microfacet normal, in order to calculate the specular for a pixel, we want to know what microfacets there are in that pixel. But that\u0026rsquo;s far too difficult. We can simplify it down a little bit. What we do know is the probability for the angle between the microfacet normal and the surface normal. This is the \u0026ldquo;D\u0026rdquo; term.\nFor our pre-calculation steps, we sometimes need an accurate sample for a pixel. To do this, we can generate random microfacet normals, and use the D term to weight the effect of each microfacet normal. A little more on this later\u0026hellip;\nBlurring convolution What is the ideal convolution to use? Brian Karis suggests a simple convolution involving N dot L. Previously, I was using a method suggested by another project that weights the textures by the entire specular equation. So what is the best method to use?\nThe blurring method is actually a little similar to the reference specular equation. For every direction in the cubemap, we imagine a parallel flat surface that is reflecting that point. For roughness=0, we should get a perfectly clear reflection. As the roughness value increases, our surface should become more and more blurry. The result is actually only current for surfaces that are parallel to what they are reflecting. On the edges, the reflection should stretch out \u0026ndash; but this phenomenon doesn\u0026rsquo;t occur in our approximation. In practice, this is not a major issue.\nThe roughness value affects the microfacet distribution and this is what leads to the blurriness. So, we can generate a random set of microfacets, weight them by \u0026ldquo;D\u0026rdquo; and then find the average of those normals. That should cause the blurriness to vary against roughness correctly.\nBut is there better filtering than that? What is the ideal \u0026ldquo;weight\u0026rdquo; value for each microfacet normal?\nOur goals for filtering should be twofold:\n Try to replicate the \u0026ldquo;GGX\u0026rdquo; falloff shape. A bright pixel in the reflection should appear the same as a dynamically calculated specular highlight. The GGX falloff is also just nice, visually. So we want the falloff to match closely. Try to prevent sampling errors. This can occur when low probability samples are weighted too highly.  I played around with this for awhile and eventually decided that there is a simple, intuitive answer to this equation.\nIn our simple case where the surface is parallel to the reflection, when we sample the final blurred cubemap, we will be sampling in the direction of the normal. We\u0026rsquo;re actually trying to calculate the effect of the microfacets on this reflection. So, why don\u0026rsquo;t we just weight the microfacet normal by the ratio of the specular for the microfacet normal to the specular for the surface normal?\nSince the specular for the normal is constant over all samples, that factors out. And the final weighting is just the specular equation for that microfacet normal (what I mean is, we use the sampled microfacet normal as the \u0026ldquo;half-vector\u0026rdquo;, or M, in the full specular equation).\nThis weighting gives us the GGX shape (because it is GGX). It also makes sense intuitively. So, in the end the best method was actually what I originally had. But now the code \u0026amp; the reasons are a little clearer.\nExample Here is an example of a filtered specular map:      Original image from sIBL archive: http://www.hdrlabs.com/sibl/archive.html\nNotice that bright point still appears bright as they are filtered down. They are retaining the GGX shape. Also, when there are bright points, the light energy from those points will get spread out in lower mipmaps. So the bright points become successively wider but darker.\nCurrently, we have a linear mapping from roughness -\u0026gt; mipmap. But as you can see, often we have more resolution that we actually need. So we could change this to a non-linear mapping.\nSampling errors At high roughness values, we can get some sampling errors. In HDR environment textures, some pixels can be hundreds or thousands of times brighter than the average pixel. As a result, these pixels overwhelm hundreds or thousands of other pixels. Our sampling pattern attempts to cluster more samples in more areas. If a pixel like this is picked up in an \u0026ldquo;unimportant\u0026rdquo; area, where are are few samples, they can lead to artifacts.\nThe following mip chain was generated from an image where the point in a middle of the sun is so bright that it can\u0026rsquo;t be represented as a 16-bit float. This causes errors, and you can see how the errors radiate outwards according to the sampling pattern.\n    Original image CC-Zero from http://giantcowfilms.com/.\nThis particular case can only be solved by a better float32 -\u0026gt; float16 converter than clamps values against the valid range. But it\u0026rsquo;s a good way to visualize how a few pixels can affect many other pixels.\nIt may be possible to get improvements by sampling every input pixel for every output pixel. This might not be such a bad idea, so if I get a chance, I\u0026rsquo;ll try it.\nThere\u0026rsquo;s also another possible sampling improvement related to the \u0026ldquo;G\u0026rdquo; term. Using the microfacet normal rather than the surface normal could help reduce the effect of samples around the extreme edge of the sampling pattern. See \u0026ldquo;M vs N in G\u0026rdquo; below for more details on that.\nImproved microfacet sampling When we build the set of random microfacets, we don\u0026rsquo;t necessarily have to distribute those microfacets evenly. We want some \u0026ldquo;important\u0026rdquo; microfacets to be more likely, and \u0026ldquo;unimportant\u0026rdquo; microfacets to be less likely.\nPreviously, I was using the sampling pattern suggested by Karis in his course notes. However, it\u0026rsquo;s possible that there is an error in that equation.\nI\u0026rsquo;ve gone back to the original microfacet distribution function by Walter in \u0026ldquo;Microfacet Models for Refraction through Rough Surfaces\u0026rdquo; (the GGX paper). He built this pattern specifically for GGX, and he also designed it for both reflection and transmission. This means the view direction isn\u0026rsquo;t considered in the distribution (which is great for us).\nKaris' function for generating the microfacet direction actually agrees with this paper (though it is in an optimized form). However, there is a difference in the \u0026ldquo;Probability Density Functions\u0026rdquo; (or PDFs) for both:\n Karis' PDF is (D * NdotH) / (4.f * VdotH) Walter\u0026rsquo;s PDF is D * NdotH  Here, Karis' PDF seems immediately suspicious because the view direction is a factor in the PDF, but not a factor in the microfacet function itself. That seems odd. The factor of \u0026ldquo;4\u0026rdquo; is also and area of confusion, because it appears in some versions of the Cook-Torrance microfacet equations, but not others. It\u0026rsquo;s not clear why Karis included it, but it may be because he is using a different form of the Cook-Torrance equation to XLE?\nWhen I switched to Walter\u0026rsquo;s sampling equations, I found 2 immediate improvements:\nFloating point errors in microfacet function Karis' microfacet distribution function is not working perfectly, possibly because of floating point errors (or weird edge cases). I\u0026rsquo;ve replaced his equation with another equation that is mathematically identically, but I get much better results.\nSometimes the shader compiler\u0026rsquo;s optimizations can change the mathematics to a point where is it visibly wrong. Could this be the case here?\nOld equation:  New equation  They should be producing the same results, but the old version appears to be producing incorrect results for inputs within a certain range. The inputs are \u0026ldquo;dithered\u0026rdquo; according to a 4x4 pattern in this example, which is why is appears regular.\nEdges are too dark The differences in the PDF equation are actually making the edges too dark. This is related to the VdotH term. It makes sense to remove VdotH from the PDF for two reasons:\n The view direction has no impact on the microfacets generated We want to use this sampling in view independent equations  Old version:  New version:  In the new version, the extreme edges are much brighter. The dark halo we get with the old version seems unnatural \u0026amp; incorrect.\nNdotL I\u0026rsquo;ve added the NdotL term into IBL, as well. This was previously excluded but is required to match our dynamic specular equation.\nThis is visually important \u0026ndash; without it, the edges become excessively bright, brighter than the thing they are reflecting. With NdotL, the brightness correctly matches the reflected object.\nIn effect, we\u0026rsquo;ve removed a VdotH term, and added a NdotL. Note that VdotH is the same as LdotH when the \u0026ldquo;H\u0026rdquo; is the half-vector between L and V\u0026hellip; So this might be source for the mysterious VdotH term in Karis' PDF! He may have assumed that this was part of the PDF, when in fact he was just compensating for a part of his specular equation. If that\u0026rsquo;s the case, though, why isn\u0026rsquo;t it explained in his course notes?\nRebuilding the lookup table Our split-term solution involves a texture lookup table \u0026ldquo;glosslut.dds.\u0026rdquo; This is built from all of the math on this page. Our previous version was very similar to Karis' version. But let\u0026rsquo;s rebuild it, and see what happens!\nOld version:   New version:   So, the changes are subtle, but important. The specular is general a little bit darker, except at the edges of objects where it has become significantly brighter.\nComparisons Old look up table:  New look up table:  Reference:  This is just changing the lookup-table. You can see how the reflection is slightly darker in the new version, except that the edges are much brighter. Also, new version matches our reference image much better.\nAt high F0 values (and along the edges) the reflection appears to be the correct brightness to match the environment.\nAlpha remapping One thing to note is that in XLE, the same alpha remapping is used for IBL that is used for dynamic specular maps. Karis suggests using different remapping for IBL, explaining that the normal remapping produces edges that are too dark. It seems like we\u0026rsquo;ve discovered the true cause of the dark edges and the results seem to look fine for us with our normal remapping.\nAlso, since we\u0026rsquo;re using the exact same specular equation as we use for dynamic lighting, the two will also match well.\nDiffuse normalization Previously, I had been assuming our texture pipeline had was premultiplying the \u0026ldquo;1/pi\u0026rdquo; diffuse normalization factor into the diffuse IBL texture. However, that turned out to be incorrect. I\u0026rsquo;ve compensated for now by adding this term into the shader. However, I would be ideal if we could take care of this during the texture processing step.\nWith this change (and the changes to specular brightness), the diffuse and specular are better matched. This is particularly important for surfaces that are partially metal and partially dielectric \u0026ndash; because the diffuse on the dielectric part must feel matched when compared to the specular on the metal part.\nFurther research Here are some more ideal for further improvements.\nH vs N in G calculation There is an interesting problem related to the use of the normal in the \u0026ldquo;G\u0026rdquo; part of the specular equation. Different sources actually feed different dot products into this equation:\n Some use N dot L and N dot V Others use H dot L and H dot V (where H is the half-vector or microfacet normal)  For our dynamic specular lights, we are using a single value for N and H, so the difference should be very subtle. But when sampling multiple microfacets, we are using many different values for H, with a constant N.\nFor example, when using the specular equation as a weight while filtering the cubemap, N dot L and N dot V are constant. So they will have no effect on the filtering.\nRemember that \u0026ldquo;G\u0026rdquo; is the shadowing term. It drops off to zero around the extreme edges. So by using H dot L and H dot V, we could help reduce the effect of extreme samples.\nAlso, when building the split-term lookup table, N dot V is constant. So it has very little effect. It would be interesting experimenting with the other form in these cases.\nOn the other hand, in the dynamic specular equation H dot L and H dot V should be equal and very rarely low numbers. So, in that case, it may not make sense to use the half-vector.\nProblems when NdotV is \u0026lt; .15f Rough objects current tend to have a few pixels around the edges that have no reflection. It\u0026rsquo;s isn\u0026rsquo;t really visible in the dielectric case. But with metals (where there is no diffuse) it shows up as black and looks very strange.\nThe problem is related to how we\u0026rsquo;re sampling the microfacet normals. As you can imagine, when normals are near perpendicular to view direction, sometimes the microfacet normal will end up pointing away from the view direction. When this happens, we can reject the sample (or calculate some value close to black). When the number of samples rejected is too high, we will end up with a result that is not accurate.\nThis tends to only affect a few pixels, but it\u0026rsquo;s enough to pop out. We need some better solution around these edge parts.\n","id":22,"section":"posts","summary":"I\u0026rsquo;ve been working on improving the accuracy of the Imaged Based Lighting (IBL) solution for XLE. This is the technology that allows us to load in a background panorama map and use it for both diffuse and specular lighting.\nThe best way to do this is by comparing our real-time result to other renderers. So, for example, I\u0026rsquo;ve been experimenting with Substance Designer (more on that later). It has nVidia\u0026rsquo;s \u0026ldquo;IRay\u0026rdquo; raytracer built-in \u0026ndash; so we can compare the non-real-time results from IRay with real-time XLE.","tags":["IBL","Lights","Specular","GGX"],"title":"Improved IBL","uri":"https://djewsbury.github.io/2016/02/improvedibl/","year":"2016"},{"content":"Just a quick update\u0026hellip; I\u0026rsquo;ve been making some great progress with transmitted specular for IBL!\n These screenshots will look a little strange (I mean that black borders \u0026amp; grainyness), because it\u0026rsquo;s a debugging rendering mode.\n I\u0026rsquo;ve got the split-term stuff going; and it looks like it should be practical. I\u0026rsquo;d prefer to avoid having yet another cubemap, so maybe there\u0026rsquo;s some way to just reuse the reflection filtered cubemap. Its seems reasonable to say that the filtering should be similar. We just need some way to calculate the amount of blurriness that is correct for transmissions.\n Speaking of that, I\u0026rsquo;ve been thinking the type of filtering that is applied to the specular map. More on that later.\nNext week I\u0026rsquo;ll post some proper screenshots \u0026amp; a whole lot of details!\nI made some interesting observations while doing this, I\u0026rsquo;ve got a bunch of new changes and improvements for IBL. Check out the \u0026ldquo;experimental\u0026rdquo; branch for now.\nBTW, the node diagram for the GGX BSDF equation looks a little nicer now \u0026ndash;\n (See the older version here:Transmission Node Diagram)\nAs you can see in the diagram, the equations are causing a lot of light to get focused in around the edges. This seems to be exaggerated when roughness is very high. That might require a little more investigation next week.\n","id":23,"section":"posts","summary":"Just a quick update\u0026hellip; I\u0026rsquo;ve been making some great progress with transmitted specular for IBL!\n These screenshots will look a little strange (I mean that black borders \u0026amp; grainyness), because it\u0026rsquo;s a debugging rendering mode.\n I\u0026rsquo;ve got the split-term stuff going; and it looks like it should be practical. I\u0026rsquo;d prefer to avoid having yet another cubemap, so maybe there\u0026rsquo;s some way to just reuse the reflection filtered cubemap.","tags":["AreaLights","Lights","Specular","GGX"],"title":"Transmitted specular progress","uri":"https://djewsbury.github.io/2016/01/transmittedspecular2/","year":"2016"},{"content":"I\u0026rsquo;ve included a few Gradle scripts in the XLE distribution. This is mostly just a simple set of tools I use for my own testing. But you may find it useful for your own needs.\nOf course this system isn\u0026rsquo;t designed to be 100% robust and fool proof. Actually, it\u0026rsquo;s just a few simple scripts. But it is scalable and flexible.\nGradle At heart, an \u0026ldquo;asset path\u0026rdquo; is some system that can identify changed assets, recognize processing working that needs to be performed with those assets, and schedule that work.\nFor example, we might have an asset for a sky background texture. Various processing tools need to pre-filter this texture so it can be used for image based lighting. Whenever the texture changes (or when the processing tools change) we want to execute the processing steps and produce intermediate assets.\nXLE does some processing at runtime. But that is only practical for short processing steps. Expensive processing steps need some other solution.\nSo we need some build system to manage assets and dependences. Are requirements are similar to build systems we use for code. But most code-oriented build systems don\u0026rsquo;t work well for assets.\nI picked Gradle because of it\u0026rsquo;s procedural nature. It allows us to specify input assets \u0026ndash; but also to give instructions to the build path on how to handle that object. For example, we need to tell the build path if a texture is a sky texture, a normals texture, or some other type\u0026hellip; That kind of thing just falls out of Gradle very easily.\nExecuting Gradle First, you need to install Gradle, from: http://gradle.org/gradle-download/\nIn Tools/AssetPath, we have the \u0026ldquo;settings.gradle\u0026rdquo; root project file. Also, this folder contains some \u0026ldquo;groovy\u0026rdquo; source files that contain gradle task type implementations. So, there is a task type for processing sky textures \u0026ndash; which basically just involves executing a number of command line operations.\nI\u0026rsquo;ve also defined a root task called \u0026ldquo;tex\u0026rdquo;. This is where gradle starts to shine. The \u0026ldquo;tex\u0026rdquo; task searches through the entire working folder for build.gradle files that contain tasks whose names being with \u0026ldquo;tex\u0026hellip;\u0026quot;. All of the tasks found become subtasks of the root \u0026ldquo;tex\u0026rdquo; task.\nSo, we can create a \u0026ldquo;build.gradle\u0026rdquo; in any folder in the working directory. And if we add tasks to that file that being with \u0026ldquo;tex\u0026rdquo; (typically \u0026ldquo;tex0\u0026rdquo;, \u0026ldquo;tex1\u0026rdquo;, etc), they will be automatically added as subtasks of the root \u0026ldquo;tex\u0026rdquo; task.\nThis is important because when we execute gradle (using the \u0026ldquo;Tools/AssetPath/execute.bat\u0026rdquo; batch file) and pass the root task \u0026ldquo;tex\u0026rdquo; on the command line, this has the effect of execute all \u0026ldquo;tex\u0026hellip;\u0026rdquo; tasks in the entire working directory.\nPretty cool, right? We can also execute individual tasks using the normal gradle command line.\nThere is an example in git in \u0026ldquo;Working/Game/xleres/DefaultResources/build.gradle.\u0026quot; This project will generate the standard lookup tables.\nProcessing Textures Executing Gradle is just the first step. Gradle is just going to chain together command line operations. Those operations require a bunch of tools.\nOne of the most useful task types is \u0026ldquo;xle.EquiRectEnv.\u0026rdquo; This takes in a equirectangular (ie, paranoramic) environment map and produces 3 important textures: Cubemap background, Specular IBL texture and Diffuse IBL texture.\nThis get a little complicated. This is just the pipeline I use for processing these textures myself. So it\u0026rsquo;s a little complicated and involved right now.\nFirst, install these:\n AMD CubemapGen (http://developer.amd.com/tools-and-sdks/archive/legacy-cpu-gpu-tools/cubemapgen/) Sébastien Lagarde\u0026rsquo;s modified CubemapGen: https://seblagarde.wordpress.com/2012/06/10/amd-cubemapgen-for-physically-based-rendering/ nvcompress ** from nvidia-texture-tools (see below) TextureProcess sample from XLE  All of these must in the system environment variable path. In the case of TextureProcess, don\u0026rsquo;t move the executable from the Finals_** folders (because it needs to find the working directory for shaders).\nI use a slightly slightly modified version of nvidia-texture-tools that works better with HDR textures. The standard nvidia-texture-tools always tonemaps HDR textures on load-in (frustratingly). You can find my modified version here: https://github.com/djewsbury/nvidia-texture-tools. This modified version allows us to read HDR files (from .hdr and other formats) and write unprocessed floating point .dds file.\nSébastien Lagarde\u0026rsquo;s ModifiedCubeMapGen is used to generate the diffuse IBL texture. This goes via a spherical harmonic representation before arriving at a small cubemap.\nXLE\u0026rsquo;s TextureProcess.exe is used to generate the specular IBL texture, as well as compressing to BC6. The specular IBL process reads from a equirectangular HDR map and writes a cubemap.\nAll of these textures need to be compress to BC6. Awkwardly, nvidia-texture-tools doesn\u0026rsquo;t support compressing HDR data to BC6 properly, but TextureProcess.exe can do that via the DirectXTex library.\nSo, once you\u0026rsquo;ve got all of that, compiled it all, put in all in the path\u0026hellip; Then you\u0026rsquo;re ready to process textures.\nIntermediate outputs from this process get written into the \u0026ldquo;int/u\u0026rdquo; (or intermediate/universal) directory. From there, you can copy it out, or do what you like. You should get a (file).dds, (file)_diffuse.dds \u0026amp; (file)_specular.dds.\nMake it your own! It\u0026rsquo;s difficult to get this working the first time. There\u0026rsquo;s a lot of work. But once it\u0026rsquo;s going, all of the parts are extensible and exchangeable. It\u0026rsquo;s a simple, but very flexible system. Handy for managing small, everyday processing tasks.\n","id":24,"section":"posts","summary":"I\u0026rsquo;ve included a few Gradle scripts in the XLE distribution. This is mostly just a simple set of tools I use for my own testing. But you may find it useful for your own needs.\nOf course this system isn\u0026rsquo;t designed to be 100% robust and fool proof. Actually, it\u0026rsquo;s just a few simple scripts. But it is scalable and flexible.\nGradle At heart, an \u0026ldquo;asset path\u0026rdquo; is some system that can identify changed assets, recognize processing working that needs to be performed with those assets, and schedule that work.","tags":["AssetPath","Gradle","IBL"],"title":"Processing textures with the XLE scriptable asset path","uri":"https://djewsbury.github.io/2016/01/assetpathscripts/","year":"2016"},{"content":"I\u0026rsquo;ve modified the lighting equations to allow for transmitted (as well as reflected) specular for dynamic lightings. Transmitted specular means the light is on the opposite side of the object, but light is coming through the object towards the viewer.\nThis is important for thin materials (such as leaves)\nNo transmitted specular:  With transmitted specular:  Here the amount of transmission is maybe slightly too high \u0026ndash; but it shows the effect well. This works particularly well with shadowed directional lights and geometry using order independent transparency. The above screenshot is exactly that situation. The tree leaves are rendered with stochastic transparency (which works particularly well for this model). So we get a nice halo effect around the edges of the tree.\nAnd, if you look closely, you can see that the tree shadowing is blocking the transmitted specular, giving a volumetric look.\nThis model is built in a way to create a lot of noise in the normals. As you can see here:  Normals point in every direction, and there is a lot of local variation.\nThe transmitted specular really helps highlight this noise. It helps to make the tree canopy feel more 3D, and it helps to hide the \u0026ldquo;true\u0026rdquo; triangular geometry. The preview reflected-only specular just wasn\u0026rsquo;t able to do this as well.\nIt\u0026rsquo;s expensive! The transmitted specular calculation is actually a second, additional, specular calculation \u0026ndash; and it\u0026rsquo;s a little more expensive than the reflected specular. It\u0026rsquo;s using the node diagram from the previous post, Transmission Node Diagram. So it\u0026rsquo;s not really cheap. It\u0026rsquo;s particularly expensive in this case, because stochastic transparency can result in many lighting calculations per pixel (for the different depth layers).\nIBL implementation This is not currently working with image based lighting. Part of the IBL principle is that every effect applied to \u0026ldquo;dynamic\u0026rdquo; lighting should also be applied to IBL \u0026ndash; (so material appear the same under both types of lights, and so we can fade distant dynamic lights into precalculated IBL).\nIt might be interesting to try to get it working under IBL, so I\u0026rsquo;ll give it a shot.\nBetter doubled sided GGX specular equation When the shader flag \u0026ldquo;MAT_DOUBLE_SIDED_LIGHTING\u0026rdquo; is set, the specular equation is now double sided. The results will be symmetrical for a flipped normal (ie, Specular(normal)==Specular(-normal), for all normals). It can be handy in cases like this, where the normals are pointing in all directions.\n","id":25,"section":"posts","summary":"I\u0026rsquo;ve modified the lighting equations to allow for transmitted (as well as reflected) specular for dynamic lightings. Transmitted specular means the light is on the opposite side of the object, but light is coming through the object towards the viewer.\nThis is important for thin materials (such as leaves)\nNo transmitted specular:  With transmitted specular:  Here the amount of transmission is maybe slightly too high \u0026ndash; but it shows the effect well.","tags":["AreaLights","Lights","Specular","GGX"],"title":"Transmitted specular","uri":"https://djewsbury.github.io/2016/01/transmittedspecular/","year":"2016"},{"content":"Here is a node diagram for the specular transmission function (BTDF) from Walter, et al, from \u0026ldquo;Microfacet Models for Refraction through Rough Surfaces.\u0026quot; Also known as the \u0026ldquo;GGX\u0026rdquo; model (or Trowbridge-Reitz).\n You can see how the node graph comes in handy for debugging a shader function like this. It\u0026rsquo;s useful to visualize each term and see it\u0026rsquo;s effects. For example, the \u0026ldquo;D\u0026rdquo; term of the BTDF is a node on the diagram. And we can visualize the effects of that term independent of everything else.\nIn this case, the diagram saves a HLSL function, and that function is called by text-based HLSL code. This will become part of some new functionality for specular transmission through thin surfaces.\nI\u0026rsquo;ll pop this in a new \u0026ldquo;experimental\u0026rdquo; branch.\nSee the post from yesterday, Material and Node Diagram Tool, for more information.\n","id":26,"section":"posts","summary":"Here is a node diagram for the specular transmission function (BTDF) from Walter, et al, from \u0026ldquo;Microfacet Models for Refraction through Rough Surfaces.\u0026quot; Also known as the \u0026ldquo;GGX\u0026rdquo; model (or Trowbridge-Reitz).\n You can see how the node graph comes in handy for debugging a shader function like this. It\u0026rsquo;s useful to visualize each term and see it\u0026rsquo;s effects. For example, the \u0026ldquo;D\u0026rdquo; term of the BTDF is a node on the diagram.","tags":["MaterialTool","Tools","Shaders"],"title":"Transmission Node Diagram","uri":"https://djewsbury.github.io/2016/01/transmissionnodegraph/","year":"2016"},{"content":"The master branch has just been updated! It now includes a major new tool, called the MaterialTool. This is a dedicated app for building materials and applying them to objects, and includes some cool new features\u0026hellip;\nIn XLE, most material information can be authored in standard graphics packages (like 3DS Max, Maya, Substance Painter, etc). In particular, the most critical material values (color, roughness, specular \u0026amp; metal) can come directly from standard packages.\nBut in cases we want to add custom information to models, or even develop custom shaders for complex materials. This is were the MaterialTool comes in. There is some functionality in common with the ModelViewer and LevelEditor tools \u0026ndash; but the MaterialTool provides a convenient focused tool for this kind of work.\n Basic functionality  Our core functionality allows us to preview a model (much like the ModelViewer, with various rendering modes), click on materials and then change their properties (such as opacity, translucency modes, and various shader flags).\nThis works within the Sony ATF framework, and so we have all of the handy features from the LevelEditor, such as:\n detachable, arrangeable windows IronPython scripting and cvar access interface skinning, keyboard rebinding, etc\u0026hellip; and, of course, it\u0026rsquo;s all very extensible C# code, convenient for adding custom features   Node Diagram editor  Also integrated is a new version of the node diagram editor. This is used for building custom shaders for special cases. Make it possible to visually create shader logic. It\u0026rsquo;s designed for use by both programmers and technical artists.\nEach diagram becomes a expression in HLSL shader code (and this can be used, just like any other shader). But these expressions can become very complex, and can (indirectly) include loops and conditions.\n Each node in the diagram has a real time preview. For mathematical nodes, this might be a chart.\n But we can also have 3D previews (using a sphere, box or a full model).\n Texture nodes can also be previewed using a flat 2D preview.\nPreviewing the diagram at every node makes the process of creating shaders much more visual. The effect of each function on the final shader becomes immediately apparent.\nFor example, XLE has some shader code for converting a \u0026ldquo;specular color\u0026rdquo; texture into the new \u0026ldquo;roughness, specular, metal\u0026rdquo; scheme (for convenience reasons). Building this logic as a node diagram is infinitely easier than just working in raw HLSL directly, because we can see the results immediately, and in detail.\n Nodes are HLSL functions! So far, the node diagram tool sounds fairly standard. But there\u0026rsquo;s an important twist. A node diagram is just a collection of \u0026ldquo;nodes\u0026rdquo; that have been connected together. But where do those nodes come from, and what do they do?\nXLE contains a permissive HLSL parser (written in Antlr3). This parser can parse almost all valid HLSL code and can build an abstract syntax tree of it\u0026rsquo;s contents. In particular, we can use this parser to extract the function signatures from a shader file.\nSo, for example the shader file Surface.h contains the functions VSIn_GetLocalPosition, VSIn_GetLocalTangent, etc\u0026hellip; Our parser can read Surface.h and find those functions, plus their parameters, output type, semantics, etc.\nSo, there\u0026rsquo;s our answer! Our nodes are actually HLSL functions. And since our parser works will every shader source file in XLE, that means that any shader function can be used as a node.\n Nodes are dragged into the diagram from something called the shader fragment palette.\nThere are no hard coded nodes, and the shader fragment palette is reloaded on the fly. So, if you\u0026rsquo;re building a diagram and suddenly realize you want a new node type\u0026hellip; Just open a text editor, add a new function into a shader file, and it can immediately be dragged into your diagram.\nThe xleres/Nodes directory is set aside to contain functions that are specifically intended to be used as nodes. In some cases, functions in this folder are just thin wrappers over other functions. But it\u0026rsquo;s recommended to mostly use functions from this directory in node diagrams, so as to isolate diagrams from shader changes.\n Each diagram is both a node type and a shader function Use of HLSL as nodes creates some interesting advantages. Each diagram itself is a shader function. And so, when you save a diagram to disk, you can then use that diagram as a node in another diagram. In this way, we can have embedded diagrams very easily.\nIt also means that we have full control over when to use a diagram, and when to use text-based HLSL. Some expressions are just awkward to do in diagram form.\nFor example, try implementing a Modulo function as a diagram using just divide, multiply, subtract and round nodes. It can be done, but it\u0026rsquo;s awkward. In cases like this, it\u0026rsquo;s better to just write a text based function (in this case, using built-in shader language functions) and then use that function within your diagram.\n Complex shaders  These methods can be used to create arbitrarily complex shaders. Above is an example of a node diagram that was duplicated from the (CC-Zero) Cycles Material Library: http://www.blendswap.com/blends/view/56470\nThis is a 100% procedural texture, with no texture inputs. The XLE implementation uses the same arrangements of nodes and the same constants to give the same final results as the Cycles render engine.\nSince the output is text HLSL code, the normal HLSL compiler and optimizers apply. There are certain cases in which hand written HLSL code will be more efficient the diagram based stuff \u0026ndash; but that might be an advanced topic. In many cases, the diagram based shaders should be as efficient as hand written code.\n Using a diagram as a material To use a diagram as an object material, follow these steps:\n Create an output node from Nodes/Outputs.sh:Output_PerPixel You may need some inputs from geometry, these are usually \u0026ldquo;Get\u0026rdquo; or \u0026ldquo;Sample\u0026rdquo; functions. For example, Nodes/Texture.sh:SampleTextureDiffuse, and Surface.h:GetNormal To create a material parameter, right click in empty space and select \u0026ldquo;Create Input\u0026rdquo; Go to Edit/Diagram Settings\u0026hellip; Select \u0026ldquo;Technique (material for any object)\u0026rdquo; as the diagram type Save your diagram! (you must save to see the results in the Model view window currently) Now you can go to the \u0026ldquo;Model view\u0026rdquo; window, right click an object and select \u0026ldquo;Assign Technique (\u0026hellip;)\u0026quot;  Check out Working/Game/xleres/Objects/Basic.tech as a starting example.\n Node graph dynamic linking You may notice some similarity between this technology and a previous post \u0026ndash; Dynamic Function Linking Graph for Shaders. They are similar because they both involve linking together the inputs and outputs of shader functions. But unfortunately they don\u0026rsquo;t work together yet\u0026hellip; Perhaps later\u0026hellip;?\n Other uses of the HLSL parser The HLSL parser has some other cool applications\u0026hellip; It can parse most valid HLSL code. And actually, it\u0026rsquo;s fairly permissive, so some invalid HLSL code will parse, as well.\nI\u0026rsquo;ve been using this parser as a linter for HLSL code in the Atom editor! There is a linter plugin for Atom, and so all it involved was creating a script that ran the \u0026ldquo;ShaderScan\u0026rdquo; sample. This reads HLSL code and spits out parsing errors.\nAnd so, those parsing errors now appear in real time while writing HLSL code in Atom. This has two uses for me, currently\u0026hellip; It catches certain errors in the HLSL (Intellisense-style). But it\u0026rsquo;s also serving as a way to test the parser itself!\nIn theory, this parser could also be extended to provide automatic conversion between HLSL and GLSL (or other languages). Some engines use this kind of approach for dealing with cross platform issues. Another possibility is just to use a complex series of #defines\u0026hellip; But either method would be awkward in it\u0026rsquo;s own unique way.\n Support for other languages Currently the node diagram tool is designed for use with HLSL. However, in theory it can also be used with other languages. All we need is a parser that can extract the function signatures. Since HLSL is a fairly generic c-like syntax, it shares a lot of similarity with many other languages. So the code that builds HLSL from the shader diagram could probably be easily adapted for languages (like Lua, Python, D, Swift, whatever).\nThis could be handy because HLSL is tied to GPU execution only. But another language would open the door for CPU side execution \u0026ndash; which could be used for game logic, physics or any other systems. This would be handy, because it would mean reusing the same core node diagram functionality for multiple separate tasks.\n","id":27,"section":"posts","summary":"The master branch has just been updated! It now includes a major new tool, called the MaterialTool. This is a dedicated app for building materials and applying them to objects, and includes some cool new features\u0026hellip;\nIn XLE, most material information can be authored in standard graphics packages (like 3DS Max, Maya, Substance Painter, etc). In particular, the most critical material values (color, roughness, specular \u0026amp; metal) can come directly from standard packages.","tags":["MaterialTool","Tools","Shaders"],"title":"Material and Node Diagram Tool","uri":"https://djewsbury.github.io/2016/01/materialtool/","year":"2016"},{"content":"For many modern engines, the shear quantity of different configuration options for shaders can start to be a major burden. Many compile-time options can end up increasing the number of compiled shaders exponentially. It can get to the point where the shaders data image can make up a large segment of download time, and compile time can be a major hassle during development.\nFor example, a pixel shader for forward lit scene elements will often need to be specialized to suit the number and types of lights nearby. If we have a few different types of lights, the number of combinations can become very quickly unmanageable.\nBut we really need a lot of compile time options! They are very useful.\nDynamic linking methods in D3D What we really need is a way to do dynamic linking of shaders \u0026ndash; so that we can construct the particular shader we need at runtime.\nD3D provides a few different methods for dynamic shader linking. The one simple method involves \u0026ldquo;classes\u0026rdquo; and \u0026ldquo;interfaces.\u0026rdquo;\nClasses and interfaces In the shader code, we can define an interface like this:\ninterface ILightResolver { float3 Resolve( GBufferValues sample, LightSampleExtra sampleExtra, LightDesc light, float3 worldPosition, float3 directionToEye, LightScreenDest screenDest); };  Then we can create an implementation of this interface:\nclass Directional : ILightResolver { float3 Resolve( GBufferValues sample, LightSampleExtra sampleExtra, LightDesc light, float3 worldPosition, float3 directionToEye, LightScreenDest screenDest) { ... } };  All of the methods in the interface function like virtual methods in C++. We can select which particular implementation to use from C++ code. It sounds very convenient \u0026ndash; and indeed it is really convenient!\nXLE has supported this method for some time; but it\u0026rsquo;s mostly just used for debugging shaders. While his method can work very well in simple situations, it has severe performance problems in complex situations.\nProblems with lighting resolve The biggest issue here is that each possible implementation class is included in the one single \u0026ldquo;uber\u0026rdquo; shader. There are \u0026ldquo;call\u0026rdquo; instructions inserted into the compiled code that will jump to the correct implementation. This jumping should be quick (actually nvidia cards have had quick branching on static bools for a very long time) but in this case the issue is the shader just becomes too large.\nIf we exceed the instruction cache for pixel shaders, it could cause some very serious performance problems.\nWith deferred rendering, the granularity seems wrong. We select a configuration, and then use it on a very large number of pixels. And then select the next configuration, etc\u0026hellip; It doesn\u0026rsquo;t seem right to have redundant code in the shader when we select the configuration infrequently.\nThere can also be problems passing large structures through the interface. In this sample, passing GBufferValues values is not ideal. The shader compiler seems to need to consume temporary registers to hold all of the parameters passed \u0026ndash; and in this case the number of parameters is too large.\nStatic polymorphism with interfaces One of the cool things about interfaces and classes is they can be resolved at compile time! If we know the true type of the class at compile time, then the shader compiler will treat it just as a normal function call.\nThis works even if we are interacting with an interface pointer. So for example, I can have the function:\nILightResolver GetLightResolver() { #if LIGHT_SHAPE == 1 Sphere result; #elif LIGHT_SHAPE == 2 Tube result; #elif LIGHT_SHAPE == 3 Rectangle result; #else Directional result; #endif return result; }  Now, if I call GetLightResolver().Resolve(\u0026hellip;) is it not a dynamic jump. It is treated just as a normal function call. So I can use GetLightResolver() anywhere, and never have to write the preprocessor switch stuff again.\nThis is a great hidden feature of the HLSL compiler! It\u0026rsquo;s static polymorphism, just like using function overloads or templates in C++. It can really make shader code cleaner and clearer (but unfortunately this doesn\u0026rsquo;t work with the feature described in the next section).\nPatching shaders together Our lighting shaders can actually be split into 3 logical parts:\n light shape shadow cascade resolve shadow resolve  Each part has a different configuration settings, but we can mix and match them together\nWhat we really want is to be able to choose a configuration for each part independently, and then just stick them all together as one. We could do this back on old consoles \u0026ndash; when we had a lot of control of low level stuff, we would just compile the parts of shaders, and patch them into one a runtime.\nEnter ID3D11Linker Actually, D3D11 has a new feature that can do something like this. It involves the interfaces ID3D11Linker and ID3D11FunctionLinkingGraph. These are new features, but they are features of the compiler \u0026ndash; so they can work with any D3D 11 hardware.\nThis allows us to create reusable shader \u0026ldquo;libraries.\u0026rdquo; The libraries can export functions using the \u0026ldquo;export\u0026rdquo; keyword:\nexport float3 DoResolve_Directional( float4 position, float3 viewFrustumVector, float3 worldPosition, float screenSpaceOcclusion MAYBE_SAMPLE_INDEX);  So, for example I can compile a library for light shapes, containing one exported function for each shape.\nNow, it might be nice if we could just say \u0026ldquo;import float3 DoResolve_Directional(\u0026hellip;)\u0026rdquo; in another shader, right? It seems logical, but it doesn\u0026rsquo;t seem to be supported. Anyway, we may want to be constructing our linking shader at runtime, and we don\u0026rsquo;t really want to be compiling HLSL source at that time.\nHowever, there is another way\u0026hellip;\nFunction Linking Graph With the Function Linking Graph we can represent a series of function calls, and the process for passing parameters between them. This is sort of like a simplified \u0026ldquo;abstract syntax tree\u0026rdquo; for HLSL. It doesn\u0026rsquo;t support any expressions or any statements other than function calls or return statements. But it\u0026rsquo;s enough to stitch together our shader from several parts.\nIn XLE, we want high level code to be able to select configuration options, but in an implementation independent way. And we want our solution to fit in well with our assets system (ie, supporting hot reloads and smart handling of errors, etc).\nThe best way do this is to introduce a simple scripting language. There are 2 ways to do this\n either a declarative oriented manner (ie, we declare the function nodes and the links between them, and let the system figure out what to do with them) or a more procedural method (ie, something that is just a thin layer over the underlying ID3D11FunctionLinkingGraph methods)  In this case, method 2 offers an efficient and more flexible solution. And the result is almost like a simplied HLSL:\nFunctionLinkingGraph:1 main = DeclareInput( float4 position : SV_Position, float2 texCoord : TEXCOORD0, float3 viewFrustumVector : VIEWFRUSTUMVECTOR {{#passSampleIndex}}, uint sampleIndex : SV_SampleIndex{{/passSampleIndex}}) // Link in our main module // We specify a filter for defines here. This is important because some defines // are intended for this file (eg, for Mustache symbols) while other defines // need to be passed down to this module // It's good to be strict about this list, because the fewer defines get passed // done to the modules, the fewer different versions of that module we have libLightShape = Module(lib_lightshape.sh, GBUFFER_TYPE;MSAA_SAMPLERS;MSAA_SAMPLES;DIFFUSE_METHOD) libShadow = Module(lib_shadow.sh, MSAA_SAMPLERS;MSAA_SAMPLES) libHelper = Module(lib_helper.sh, MSAA_SAMPLERS;MSAA_SAMPLES;HAS_SCREENSPACE_AO) // The basic structure is simple: // 1) Calculate some inputs to the resolve operations // 2) Perform each resolve step // 3) Generate the output value by combining the resolve outputs // // Steps 1 and 3 are fixes, but step 2 varies depending on the options // selected for the light (ie, this is where the dynamic linking occurs) setup = libHelper.Setup(position, viewFrustumVector) worldPosition = Alias(setup.2) worldSpaceDepth = Alias(setup.3) screenSpaceOcclusion = Alias(setup.4) {{#passSampleIndex}}PassValue(sampleIndex, setup.5){{/passSampleIndex}} light = libLightShape.DoResolve_{{shape}}( position, viewFrustumVector, worldPosition, screenSpaceOcclusion {{#passSampleIndex}}, sampleIndex{{/passSampleIndex}}) cascade = libShadow.DoResolve_{{cascade}}( position, texCoord, worldSpaceDepth) shadow = libShadow.DoResolve_{{shadows}}( cascade.3, cascade.4, cascade.5, position {{#passSampleIndex}}, sampleIndex{{/passSampleIndex}}) finalize = libHelper.FinalizeResolve(light.result, shadow.result) output = DeclareOutput(float4 outColour : SV_Target0) outColour = finalize.result  This script is read by a simple minimal parser. This parser should be quick enough to be run during runtime without major issues.\nAs a preprocessing step, I\u0026rsquo;m using a Mustache compliant library. This is important for performing the customization we need. The Mustache step might be a little slow. But the result should still be ok in Release builds. In debug the string handling stuff is probably a little slower than ideal.\nSo, here we\u0026rsquo;re basically just declaring the modules we want to link in. Then we specify the functions we want to call, and the sources for each parameter. We pass the outputs from each function down into where they are needed.\nWe can\u0026rsquo;t do any expressions or math or anything else here. All we can do is call functions and pass the results to output. Since this script will be compiled at runtime, perhaps a more restrictive language is actually more ideal.\nCompiled output The final compiled shader code form the stitched approach appears to be very similar to a statically linked result. There might be a little overhead in some situations (particularly since the optimizer can\u0026rsquo;t optimize across the function call points). But it seems minimal.\nThe stitched shader seems to use a few more temporary registers than the statically linked result (in one complex shader, the difference was 11 to 13). This may be an issue on some hardware, but maybe the benefits are worth some performance loss.\nUncommonly used? ID3D11FunctionLinkingGraph doesn\u0026rsquo;t seem to be frequently used currently. I\u0026rsquo;m not sure why, it\u0026rsquo;s been around for a few years, it works well and it\u0026rsquo;s pretty useful\u0026hellip;\nIf I do a Google search for ID3D11FunctionLinkingGraph, I only get around 3 pages right now. And there is only one clear sample (and it\u0026rsquo;s a bit hard to find). It\u0026rsquo;s strange to find features that are this useful, and yet still so uncommon!\nAnyway, it works in XLE fine now. So maybe now XLE can serve provide an example implementation for anyone that needs it.\nComplications There are some complications involving this method. In particular, functions can refer to global bindings (like constant buffers and resources). These must be compatible for all functions used together (ie, this often means preferring explicitly bound constant buffers and resources).\nAlso, there are some complex shader features that aren\u0026rsquo;t supported (like interpolation modes for input parameters). And matrices can\u0026rsquo;t be used for input or output parameters. But maybe that kind of missing functionality isn\u0026rsquo;t a high priority.\nAnd the scripting language is actually extremely primitive. But it seems featureful enough to support everything we want to do with it.\nAlso we can\u0026rsquo;t pass structures into and out of exported functions. Thats very annoying for me, because I\u0026rsquo;ve been using structures for lots of things. In particular, they are great for hiding optional parameters (like sampleIndex in the above example). Without structures, the solution for sampleIndex is a good deal more ugly.\n","id":28,"section":"posts","summary":"For many modern engines, the shear quantity of different configuration options for shaders can start to be a major burden. Many compile-time options can end up increasing the number of compiled shaders exponentially. It can get to the point where the shaders data image can make up a large segment of download time, and compile time can be a major hassle during development.\nFor example, a pixel shader for forward lit scene elements will often need to be specialized to suit the number and types of lights nearby.","tags":["D3D","ShaderCompile"],"title":"Dynamic Function Linking Graph for Shaders","uri":"https://djewsbury.github.io/2015/12/functionlinkedshaders/","year":"2015"},{"content":"Just a quick note on rectangle light diffuse. In XLE, rectangle lights are slightly different from the default rectangle lights in 3DS Max.\nXLE rectangle lights emit light mostly in the forward direction (and a reduced amount sideways). But in the 3DS Max Quicksilver renderer, rectangle lights emit light in all direction.\nEdge cases This is partially because the method we\u0026rsquo;re using for diffuse doesn\u0026rsquo;t work well on extreme angles. In the above example, the light plane and the top/bottom/left/right walls are actually perpendicular.\nThis causes weird problems right on the edges.\nIn the above example, the left and right walls look fine. But the top and bottom walls are missing their highlight. This is because the light is quite tall. A square light might be fine.\nBut you can see that, in general, the 3DS Max result and the XLE result is quite similar.\nPractical uses It might be possible to fix this with some research and a few tricks. But it\u0026rsquo;s not clear if that would really be useful. Rectangle lights might be most useful for things like windows, TV screens, photography reflectors, etc. In these cases we normally want the light to always go forward, anyway.\nPlus, if we block off the extreme edges, we have some easy options for shadowing.\n","id":29,"section":"posts","summary":"Just a quick note on rectangle light diffuse. In XLE, rectangle lights are slightly different from the default rectangle lights in 3DS Max.\nXLE rectangle lights emit light mostly in the forward direction (and a reduced amount sideways). But in the 3DS Max Quicksilver renderer, rectangle lights emit light in all direction.\nEdge cases This is partially because the method we\u0026rsquo;re using for diffuse doesn\u0026rsquo;t work well on extreme angles. In the above example, the light plane and the top/bottom/left/right walls are actually perpendicular.","tags":["AreaLights","RectangleLights"],"title":"Rectangle Light diffuse vs 3DS Max","uri":"https://djewsbury.github.io/2015/12/rectlightdiffuse/","year":"2015"},{"content":"Post updated on 2015-12-10\nThis is just some notes about some ideas I\u0026rsquo;ve been playing with lately. I\u0026rsquo;ve been thinking about some improvements to the specular highlights for rectangle lights!\nThe current implementation is based on the method by Michal Drobot in GPU Pro 5. I\u0026rsquo;m not going to repeat the description here (I\u0026rsquo;ll only give a few details) \u0026ndash; but I recommend buying the book and having a look! It\u0026rsquo;s quite a good method, and interesting read. Drobot describes a very practical method for diffuse and specular for rectangle and disc lights.\nHowever, at extreme angles, the specular reflection of rectangle lights can sometimes show some distortion.\nHere is a comparison:\nIt appears quite distorted here, but this is a contrived example. This is a material with high \u0026ldquo;roughness\u0026rdquo; but the geometry is completely flat. In a normal scene, flat geometry will probably have a less rough material. And in a less rough material, the distortion is much less visible.\nSpecular Cone Part of the problem is related to how the integration across the specular highlight is performed. We can calculate a cone that represents the part of the scene that contributes most greatly to the specular for a given point. When calculating the specular for a point, we assume that everything in that cone contributes very strongly to specular, and everything outside of it doesn\u0026rsquo;t contribute at all.\nThis is a simplification, because really we don\u0026rsquo;t want a binary on/off \u0026ndash; we want to weight all incoming light by the BRDF. But out simplification can be quite good for many cases.\nRectangle and cone intersection So, once we have our cone, we want to find how much of it is covered by the rectangle light. Ignoring shadows, we can do this by finding the intersection of the cone and the rectangle light.\nBut, of course, this is quite difficult and expensive. This is starting to go into conic sections, which are a particularly complication type of geometry.\nSo we need a simplification. Drobot describes a method that is very similar to his method for disc lights.\nHis method treats the intersection between a plane and the cone as a circle, and then further simplifies it into a square. This method works really well when the light direction is directly towards to the sample point. But on angles, the intersection should start to distort into an ellipse.\nSee, for example, this diagram from wikimedia commons: We don\u0026rsquo;t have to care about the parabola or hyberbola cases (because our cone is infinitely long).\nEllipse estimations Our goal is to find some way to estimate the area of an intersection of that ellipse and a rectangle. And we also need to find the geometric center of that intersection.\n Working with the true ellipse here is far too expensive. The method would be extremely complex. However, we can make some estimations. Also, the rectangle and the ellipse are not aligned in any easy way \u0026ndash; so that makes it more complex.\nOne idea is to use 2 squares, instead of one. We will place the squares near the vertices of the of the ellipse, and balance the area of the squares so that they roughly match the ellipse.\n With this method, as the ellipse widens, our simulation takes into account that widening.\nFlat on 2 squares seems to help in the flat-on case, also.\nAs you can see, improved method helps smooth out the result. The light retains it\u0026rsquo;s rectangular shape better.\nRepresentative point tweaks The goal of these methods is to find a quick estimate to integrating the BRDF across all angles. When we\u0026rsquo;re finding the intersection of the specular cone and the light, we\u0026rsquo;re really trying to estimate the intersection of the light geometry and the BRDF equation. If we could then find the mean of the BRDF in the intersection area, we would have a good estimate of the full integral.\n To estimate the mean, we adjust the reflection direction to find a new \u0026ldquo;representative point\u0026rdquo; for specular. As Drobot explains in his article, he was working with a phong based BRDF. However, with GGX, I\u0026rsquo;m finding the equation is very steep, and that small adjustments to the representative point have a huge effect on the result!\n This might be accentuated by the fact that I\u0026rsquo;m using a very wide angle for the specular cone. The wide angle helps give us really blurry highlights, but it makes some artifact worse.\nSo I\u0026rsquo;m finding that we have to be a little more conservative with the representative point. I\u0026rsquo;m experimenting with taking the average of the intersection center and the unmodified reflection direction.\nNew artifacts Unfortunately, the new method can add some artifacts. Sometimes the influence of each square can separate, which can end up giving the impression of 2 separate specular highlights. This can cause some real problems if one square clipped by one side of the light, and the other square is clipped by the other side.\nThere are also some cases where the single-square method looks better, even though it may be incorrect\u0026hellip; With a single square, the highlight will be incorrectly sharp on extreme angles. But it still looks right to the viewer.\nHere is an example of the artifacts that can occur in some cases\nHere the rectangle keeps it\u0026rsquo;s shape better, but the separate effects of the 2 squares starts to become visible.\nIn a real world situation, this is unlikely to be as visible. Since this is a surface with a high \u0026ldquo;roughness\u0026rdquo; we would normally expect there to be a normal map and more geometric detail. That will serve to hide the artifacts.\nMore screenshots Here are a few more comparison shots:\nIn-engine visualization You can use the cvar \u0026ldquo;LightResolveDebugging\u0026rdquo; to visualize the squares, intersection ellipse and representative points. Type \u0026ldquo;cv.LightResolveDebugging = True\u0026rdquo; in the IronPython window in the editor.\n ","id":30,"section":"posts","summary":"Post updated on 2015-12-10\nThis is just some notes about some ideas I\u0026rsquo;ve been playing with lately. I\u0026rsquo;ve been thinking about some improvements to the specular highlights for rectangle lights!\nThe current implementation is based on the method by Michal Drobot in GPU Pro 5. I\u0026rsquo;m not going to repeat the description here (I\u0026rsquo;ll only give a few details) \u0026ndash; but I recommend buying the book and having a look! It\u0026rsquo;s quite a good method, and interesting read.","tags":["AreaLights","RectangleLights"],"title":"Working some improvements to rectangle lights","uri":"https://djewsbury.github.io/2015/12/workingonrectlights/","year":"2015"},{"content":"So, you\u0026rsquo;ve just downloaded XLE, and you\u0026rsquo;re wondering what to do first? Here\u0026rsquo;s a suggestion for the first 10 minutes:\n Startup the level editor Select the Level Editor as the startup project:  You should use the Debug/x64 or Release/x64 configuration. Though XLE works in both 32 bit and 64 bits modes, normal usage for the level editor should be in 64 bits mode.\nIf you have trouble compiling, see the [Getting Started Compiling page] (https://github.com/xlgames-inc/XLE/wiki/CompilingFirstSteps).\nStart up this application, and you should see a large 3D window and various other windows about.\nThe level editor is based on a great project from Sony WWS SonyWWSLevelEditor. It has been modified to work with XLE, and some XLE specific behaviour has been added. I think this is a great example of the Open Source concept working for games developers.\n Creating an object For the first minute, the 3D window will appear black. This is because shaders are compiling in the background. They will get flushed to disk on application shutdown, so it only happens the first time.\nSelect \u0026ldquo;Window/Resources\u0026rdquo; to open the resources panel. You should see the directory structure under the \u0026ldquo;working\u0026rdquo; folder. Here, you can find .dae (Collada) files.\nFind a dae file, and drag it into the 3D viewport (or you can drag from Windows Explorer).\n Collada notes XLE should support all well formatted Collada files. But unfortunately some exporters produce poorly formatted files. For example, sometimes texture names are prefixed with \u0026ldquo;file://\u0026quot; \u0026ndash; this isn\u0026rsquo;t supported in XLE.\nBut you should be fine with the Blender Collada exporter, or the OpenCollada exporter for Max/Maya. You can also use Open Asset Import Library to convert files to Collada format.\nPlacement documents In the level editor, the main \u0026ldquo;game\u0026rdquo; file is like a solution file in Visual Studio. It contains links to project documents under it. One type of document is a \u0026ldquo;placements\u0026rdquo; document (called a placements cell). This contains a list of static objects with basic properties.\nNormal large world games should have many placement cells. These can be arranged in a grid structure. But cells can also be overlapping, or have irregular shapes.\n When you first startup the Level Editor, you get a default untitled placements document. However, you can reconfigure it by right-clicking \u0026ldquo;Placements\u0026rdquo; in the Project Lister and selecting \u0026ldquo;Configure placements\u0026hellip;\u0026rdquo;\n Adding lighting settings Probably, you will want to play with some lighting settings next.\nFirst, Find the \u0026ldquo;Palette\u0026rdquo; window (select \u0026ldquo;Window/Palette\u0026rdquo; to enable it if it isn\u0026rsquo;t visible).\nHere, open up the \u0026ldquo;Lights\u0026rdquo; category. You can drag items from the \u0026ldquo;Palette\u0026rdquo; window into the \u0026ldquo;Project Lister\u0026rdquo;.\n Drag these 4 items from the \u0026ldquo;Lights\u0026rdquo; category of the \u0026ldquo;Palette\u0026rdquo; into the \u0026ldquo;Project Lister\u0026rdquo;:\n DirLight AreaLight AmbientSettings ToneMapSettings  Try selecting the newly created \u0026ldquo;DirLight\u0026rdquo; in the Project Lister. You can press \u0026ldquo;M\u0026rdquo; to enable the \u0026ldquo;move\u0026rdquo; manipulator to move it around.\nNow, find the \u0026ldquo;Property Editor\u0026rdquo; window (select \u0026ldquo;Window/Property Editor\u0026rdquo; if it is hidden). Here you can see all of the properties for the item. You can customize the lighting environment by playing with the settings for the 4 items you created.\n  Console debugging modes Next, find the \u0026ldquo;IronPython\u0026rdquo; window (use Window/Iron Python to enable it if it is hidden).\nTry typing \u0026ldquo;cv.DeferredDebugging = 1\u0026rdquo;. This will enable a debugging mode. You can type \u0026ldquo;cv.DeferredDebugging = 0\u0026rdquo; to disable it again.\nHere, \u0026ldquo;cv\u0026rdquo; stands for \u0026ldquo;console variable\u0026rdquo;.\n What about the next 10 minutes? Well, that\u0026rsquo;s the end of the quick introduction. The rest is up to you. Have fun!\n","id":31,"section":"posts","summary":"So, you\u0026rsquo;ve just downloaded XLE, and you\u0026rsquo;re wondering what to do first? Here\u0026rsquo;s a suggestion for the first 10 minutes:\n Startup the level editor Select the Level Editor as the startup project:  You should use the Debug/x64 or Release/x64 configuration. Though XLE works in both 32 bit and 64 bits modes, normal usage for the level editor should be in 64 bits mode.\nIf you have trouble compiling, see the [Getting Started Compiling page] (https://github.","tags":null,"title":"What to do first","uri":"https://djewsbury.github.io/2015/12/whattodofirst/","year":"2015"},{"content":"XLE now support a few area light types: sphere, tube \u0026amp; rectangle (with disc and maybe some other shapes coming soon). Maybe in a future post I\u0026rsquo;ll go into some details about the implementation (actually, it\u0026rsquo;s quite interesting!). But this post is about something different: this post is about why they are important.\nI\u0026rsquo;m finding that it makes a huge difference. So much so that not only do we want to support area lights \u0026ndash; we also want to outlaw non-area lights. Point light sources are now the enemy!\nPBR Concept The biggest buzzword at the moment is Physically Based Renderering (or PBR, or sometimes Physically Based Shading). But what does that mean, really? Partially it means sampling values from the real world. But really it\u0026rsquo;s just a container under which we\u0026rsquo;re placing many related concepts.\nOne of these \u0026ldquo;sub-concepts\u0026rdquo; of PBR is the idea that a single material should work in many different lighting environments.\nIn the past, artists often tweaked the material settings for objects to suit the lighting environment they are in. This is a problem because it means that if the object moves into another lighting environment, the material settings must be tweaked to match.\nThis happen in both movies and games. In games, it\u0026rsquo;s particularly a problem for games with a day/night cycle. In games where time is changing, the lighting environment is also always changing. Previously this means that extreme lighting settings (sunsets, etc) had to be more subtle than real-life. What we want is materials that look correct in all possible lighting environments.\nSpecular highlight size There\u0026rsquo;s a simple way to think about this. Let\u0026rsquo;s consider the size of a specular highlight.\nHere are 5 spheres rendered in Blender with Cook Torrence lighting.  Each sphere is reflecting the same light, but the material settings are changed to change the size of highlight in each sphere.\nIn older games, this was the only way to change the size of the highlight. If a highlight appeared too small or too big, an artist would change the material settings.\nHowever there should be something else that should effect the size of the highlight: obviously, it\u0026rsquo;s the shape of the light! We need area lights to achieve that.\nNew model for specular highlights So, we have two important concepts to control the size of the specular highlights:\n material roughness light size  Notice that these ideas are simpler and clearer than concepts in older engines. Previously we might talk about \u0026ldquo;gloss\u0026rdquo; or \u0026ldquo;specular power\u0026rdquo; or \u0026ldquo;specular hardness.\u0026quot; But what do those really mean?\nRoughness is much clearer. It\u0026rsquo;s just a single value between 0 and 1. And it\u0026rsquo;s linear \u0026ndash; not exponential! It\u0026rsquo;s probably the most important variable of our material model (other than Color, I guess).\nRoughness Roughness controls the size of the highlight by spreading the light energy over a large area.\nNotice the image above. In one extreme the highlight is a dense, concentrated point of light. In the other extreme, the specular light energy is spread out over a very large area, so that it appear almost like diffuse lighting.\nThis is critical for us because in XLE, all materials are reflective. But when the roughness is very high, the reflections get spread out over a very large area, so that they appear almost diffuse.\nAlso, both specular highlights and IBL reflections get spread out in the same way. Sometimes it\u0026rsquo;s even difficult to tell what is a specular highlight, and what is a reflection.\nLight size With rectangle and sphere lights, the size of the highlight will change significantly with the size of the light (and also the distance to the light). But it changes in a very different way to roughness. Large lights give an appearance that just wasn\u0026rsquo;t possible previously.\nIf we want to make a very large specular highlight, we can do that by changing the size of the light. So there is no need to tweak the material any more!\nNo more point lights Now, the only problem with this is point lights no longer look correct. If the material is balanced correctly for use with area lights, then highlights from point lights will look unnaturally too small. This is logical, because point light sources don\u0026rsquo;t exist in reality.\nFortunately, sphere lights are only slightly more expensive than point lights. And we can use the same BRDFs with sphere lights. So it\u0026rsquo;s an easy transition.\nSo the solution is to always use area lights, and never use point lights.\nAuthoring material settings Even though we\u0026rsquo;ve created a separation between material settings and lighting environment settings, I\u0026rsquo;m still finding that we will probably need some kind of standard lighting environment for authoring the material settings.\nThat is, the values that artists will select for roughness may vary with the lighting environment they are testing with.\nProbably an Image Based Lighting environment would be best for this. This is a great reason why BRDF for IBL and dynamic lights should match as closely as possible. We should author the material settings in a 100% IBL environment, and then we should expect that dynamic lights should just work.\nSimpler\u0026hellip; and better So, I found that area lights are critical to the PBR concept. So much so that an engine that doesn\u0026rsquo;t support area lights shouldn\u0026rsquo;t properly be called PBR.\nAnd I think it meets two important concepts of PBR:\n materials should look correct in all lighting environments it should be simpler and better (at the same time)  3 cheers for area lights!\n","id":32,"section":"posts","summary":"XLE now support a few area light types: sphere, tube \u0026amp; rectangle (with disc and maybe some other shapes coming soon). Maybe in a future post I\u0026rsquo;ll go into some details about the implementation (actually, it\u0026rsquo;s quite interesting!). But this post is about something different: this post is about why they are important.\nI\u0026rsquo;m finding that it makes a huge difference. So much so that not only do we want to support area lights \u0026ndash; we also want to outlaw non-area lights.","tags":["PBR","AreaLights","Roughness","Materials"],"title":"Area Lights with Physically Based Rendering","uri":"https://djewsbury.github.io/2015/12/arealightsandpbr/","year":"2015"},{"content":"This blog will contain some day-to-day information about XLE.\nLet\u0026rsquo;s start with some screenshots, rendered with XLE:\n The Captain character from Archeage (US site).\n One of the (many) costumes from Archeage.\n A skeleton enemy from Archeage.\n A military spacecraft.\n The \u0026ldquo;Gweonid\u0026rdquo; character from Archeage.\n A metal golem character from Archeage.\n One of the costumes from Archeage, and \u0026ldquo;Nyra\u0026rdquo; character. Nyra model comes from Paul Tosca. Thanks for the great model, Paul!\n Detail of Nyra\n One more from Nyra.\nMany of the backgrounds in these images are from sIBL Archive.\n","id":33,"section":"posts","summary":"This blog will contain some day-to-day information about XLE.\nLet\u0026rsquo;s start with some screenshots, rendered with XLE:\n The Captain character from Archeage (US site).\n One of the (many) costumes from Archeage.\n A skeleton enemy from Archeage.\n A military spacecraft.\n The \u0026ldquo;Gweonid\u0026rdquo; character from Archeage.\n A metal golem character from Archeage.\n One of the costumes from Archeage, and \u0026ldquo;Nyra\u0026rdquo; character. Nyra model comes from Paul Tosca.","tags":["blog"],"title":"First post","uri":"https://djewsbury.github.io/2015/12/first/","year":"2015"}],"tags":[{"title":"API","uri":"https://djewsbury.github.io/tags/api/"},{"title":"AreaLights","uri":"https://djewsbury.github.io/tags/arealights/"},{"title":"AssetPath","uri":"https://djewsbury.github.io/tags/assetpath/"},{"title":"blog","uri":"https://djewsbury.github.io/tags/blog/"},{"title":"cross platform","uri":"https://djewsbury.github.io/tags/cross-platform/"},{"title":"D3D","uri":"https://djewsbury.github.io/tags/d3d/"},{"title":"GGX","uri":"https://djewsbury.github.io/tags/ggx/"},{"title":"Gradle","uri":"https://djewsbury.github.io/tags/gradle/"},{"title":"HLSL","uri":"https://djewsbury.github.io/tags/hlsl/"},{"title":"IBL","uri":"https://djewsbury.github.io/tags/ibl/"},{"title":"lighting","uri":"https://djewsbury.github.io/tags/lighting/"},{"title":"Lights","uri":"https://djewsbury.github.io/tags/lights/"},{"title":"Materials","uri":"https://djewsbury.github.io/tags/materials/"},{"title":"MaterialTool","uri":"https://djewsbury.github.io/tags/materialtool/"},{"title":"Metal","uri":"https://djewsbury.github.io/tags/metal/"},{"title":"OITrans","uri":"https://djewsbury.github.io/tags/oitrans/"},{"title":"PBR","uri":"https://djewsbury.github.io/tags/pbr/"},{"title":"RectangleLights","uri":"https://djewsbury.github.io/tags/rectanglelights/"},{"title":"Releases","uri":"https://djewsbury.github.io/tags/releases/"},{"title":"rendering","uri":"https://djewsbury.github.io/tags/rendering/"},{"title":"Roughness","uri":"https://djewsbury.github.io/tags/roughness/"},{"title":"Samples","uri":"https://djewsbury.github.io/tags/samples/"},{"title":"Screenshots","uri":"https://djewsbury.github.io/tags/screenshots/"},{"title":"ShaderCompile","uri":"https://djewsbury.github.io/tags/shadercompile/"},{"title":"Shaders","uri":"https://djewsbury.github.io/tags/shaders/"},{"title":"Specular","uri":"https://djewsbury.github.io/tags/specular/"},{"title":"SphericalHarmonics","uri":"https://djewsbury.github.io/tags/sphericalharmonics/"},{"title":"SPIR-V","uri":"https://djewsbury.github.io/tags/spir-v/"},{"title":"StraightSkeleton","uri":"https://djewsbury.github.io/tags/straightskeleton/"},{"title":"Tools","uri":"https://djewsbury.github.io/tags/tools/"},{"title":"Vulkan","uri":"https://djewsbury.github.io/tags/vulkan/"}]}